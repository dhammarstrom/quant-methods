# Categorical predictors and multiple regression

We have up to now used a single continuous predictor to predict a dependent variables. We will now show that the ordinary regression models can be the same as other statistical tests, they can be extended and modified. This will show that the ordinary regression model is very flexible.

## Linear models can be used instead of t-tests
Last week we performed t-tests. These are tests of differences from zero in a one-sample case or differences between groups with paired or unpaired observations. We calculated the difference between pre- and post-training in squat-jump to test against the null-hypothesis that there was no difference between these two time-points.

```{r, eval = FALSE}
cyc_select <- cyclingstudy %>%
        select(subject, timepoint, sj.max) %>%
        filter(timepoint %in% c("pre", "meso3")) %>%
        pivot_wider(names_from = timepoint, 
                    values_from = sj.max) %>%
        mutate(change = meso3 - pre) 
```

The data above may be used to perform the paired sample t-test and a one sample t-test

```{r, eval = FALSE}

paired <- t.test(cycling_data$meso3, cycling_data$pre, paired = TRUE)
one_sample <- t.test(cycling_data$change, mu = 0)

```

These tests are equal. Similarly, we can fit a linear model to the `change` variable to test against the same hypothesis

```{r, eval = FALSE}

lin_mod <- lm(change ~ 1, data = cycling_data)
summary(lin_mod)

```

Using the syntax `change ~ 1` in the formula we specify that we want to estimate the intercept of the model. The intercept is tested against the null-hypothesis that it is 0.

We also did two sample t-tests with unpaired observations. We can test if there is a difference in `VO2.max` percentage change between group `INCR`and `DECR` using a t-test.

```{r, eval = FALSE}
cyc_select <- cyclingstudy %>%
        select(subject,group, timepoint, VO2.max) %>%
        filter(timepoint %in% c("pre", "meso3"), 
               group != "MIX") %>%
        pivot_wider(names_from = timepoint, 
                    values_from = VO2.max) %>%
        mutate(change = 100 * (meso3-pre)/pre) %>%
        print()

unpaired <- t.test(change ~ group, data = cyc_select, var.equal = TRUE)
```

Above we use the formula-way to specify the t-test. Similarly we can use a linear model

```{r, eval = FALSE}

lin_mod <- lm(change ~ group, data = cyc_select)
summary(lin_mod)

```

Compare the two tests. Do they tell you the same?

Even the Welch two sample t-test can be replicated using a linear model. However, we have to specify it in a slightly different frame work using the `gls()`function from the `nlme` package. 

```{r, eval = FALSE}
library(nlme)

welch_twosample <- t.test(change ~ group, data = cyc_select, var.equal = FALSE)

lin_mod_var <- gls(change ~ group, data = cyc_select, weights = varIdent(form = ~1|group), na.action = na.exclude, method = "ML")


welch_twosample
summary(lin_mod_var)

```

You are not required to master `gls` at this time-point. It is an example that the linear model frame work is very flexible as it in this case also can be adjusted to take care of heteroscedasticity. 

### Dummy variables

The group variable in the code below introduces a new concept in our linear models, dummy variables.

```{r, eval = FALSE}

lin_mod <- lm(change ~ group, data = cyc_select)

```

When we put a categorical variable in the `lm` command, R will code it as a dummy variable. This variable will be zero if the group corresponds to the first level of the categorical (coded as a factor variable) and it will be 1 if it is the second level.

In the simplest case (as above) we will get a linear model looking like this:

$$Y = \beta_0 + \beta_1X$$

Where the $X$ is the grouping variable, remember, 0 if first (reference) group and 1 if the second level group. The coefficient $\beta_1$ only kicks in if the group is 1. Meaning that when group = 0 we have only the intercept. If group = 1 we have the intercept + the slope. The slope represents the difference between the intercept (group = 0) and group = 1.

If the grouping variable would have more groups more dummy-variables would have been added. 

Using all groups in the data set, fit a model and interpret the results.

 <a id="displayText" href="javascript:toggle(2);">Here is a possible solution</a>
  <div id="toggleText2" style="display: none">

```{r, eval = FALSE}
cyc_subset <- cyclingstudy %>%
        select(subject,group, timepoint, VO2.max) %>%
        filter(timepoint %in% c("pre", "meso3")) %>%
        pivot_wider(names_from = timepoint, 
                    values_from = VO2.max) %>%
        mutate(change = 100 * (meso3-pre)/pre) %>%
        print()

mod <- lm(change ~ group, data = cyc_subset)

summary(mod)

```

The `DECR` group is the reference group, the intercept shows the mean of this group. Each parameter shows the difference from the reference.



  </div>
  </br>  

The same assumptions are made with these kinds of models and they can be checked with the same methods as described above. 

## Multiple regression

Contrary to the t-tests used above, the linear model can be extended by adding predicting variables (independent variables). In a situation where multiple independent variables are included in the model, we control for their relationship to the dependent variable when we evaluate the other variables. Similarly with univariate regression we can examine each individual parameter from the summary.

In a previous example we used `height.T1` to predict `VO2.max`. We might want to add information to the model. We might wonder if the age (`age`) of participants have a relationship with VO~2max~. To fit this model, use the code below.

```{r, eval = FALSE}

cycling_data <-  cyclingstudy %>%
       # select(subject, timepoint, VO2.max, weight.T1, height.T1) %>%
        filter(timepoint == "pre") %>%
        print()  
          

mod1 <- lm(VO2.max ~  height.T1 + age, data = cycling_data)

summary(mod1)

```

From the output we can see that there is a negative relationship, when age increases VO~2max~ decrease. We can compare this model to the simpler model by looking at the $R^2$ value. We fit the simpler model.

```{r, eval = FALSE}

mod0 <- lm(VO2.max ~  height.T1, data = cycling_data)

summary(mod0)

```

We can interpret $R^2$ as the percentage of the variation explained by the model. 

The same assumptions apply to the multiple regression model as with the univariate regression model. 




---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Correlations

We are already familiar with the regression model. We will now take a step back, to the correlation. A correlation is a unit-less measure of the relationship between two variables. The strength of the relationship is expressed between -1 and 1 where values closer to 0 means weaker relationship.

Using the data set provided by Haun et al. 2019 we will see how the correlation works. First we will load the data and select the variables `SQUAT_VOLUME` and `DXA_LBM_T1`. A reasonable interpretation of these variables are that `SQUAT_VOLUME` is the pre-intervention training volume and `DXA_LBM_T1` is the percentage lean body mass before the intervention. To make the data more readable we will scale the squat volume from kg to tons

```{r, eval = FALSE}
library(tidyverse)
library(exscidata)
data("hypertrophy")


dat <- hypertrophy %>%
  select(PARTICIPANT, SQUAT_VOLUME, BODYMASS_T1, DXA_LBM_T1) %>%
  mutate(SQUAT_VOLUME = (SQUAT_VOLUME/1000)) %>%
  print()


```

A basic question given these kind of data is, do participants with more previous training volume have more  muscle mass (lean mass)? We can test this by the doing a correlation analysis. The `cor` function gives the correlation coefficient from two variables:

```{r, eval = FALSE}
cor(dat$SQUAT_VOLUME, dat$DXA_LBM_T1)
```

The value is quite high, around 0.5. Remember that a perfect correlation is either 1 or -1 and 0 indicates no correlation between variables. The correlation coefficient is not sensitive to the order of variables: 

```{r, eval = FALSE}
cor(dat$DXA_LBM_T1, dat$SQUAT_VOLUME)
```

We can use the correlation coefficients to draw inference. A test against the null hypothesis of no correlation, $H_0: r=0$, can be done in R using the `cor.test` function. 

```{r, eval = FALSE}
cor.test(dat$DXA_LBM_T1, dat$SQUAT_VOLUME)
```

From this function we get a confidence interval, does the confidence interval contain the $H_0$?

## Always plot the data!

When doing a correlation analysis you are at risk of drawing conclusions based on wonky data. A single data point can for example create a correlation in a small data set. Lets look at the data we are using now.

```{r, echo = FALSE, warning=FALSE, message=FALSE}

dat <- hypertrophy %>%
  mutate(SQUAT_VOLUME = SQUAT_VOLUME/1000) %>%
  select(PARTICIPANT, SQUAT_VOLUME, DXA_LBM_T1) 

```

```{r}
dat %>%
  ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal()
```

The plot displays no apparent relationship, there are no obvious outliers, both variables are evenly distributed (normally distributed). These are assumptions regarding the correlation analysis. Other assumptions are that the relationship is linear (a straight line). 

## Extending the correlation to the regression

All good! We have a test that tells us about the relationship between two variables. However, the test does not tell us more about the correlation. Moving to a regression analysis gives us more information. 

First some similarities. Notice that the p-value for the regression coefficient for squat volume is (almost) precisely the same as the p-value for the correlation analysis!

```{r, eval = FALSE}
# Store the correlation analysis in an object 
c <- cor.test(dat$DXA_LBM_T1, dat$SQUAT_VOLUME)
# store the regression model
m <- lm(DXA_LBM_T1  ~ SQUAT_VOLUME, data = dat)

# Display the p-value for the regression coefficient
coef(summary(rm))[2, 4] 
# Display the p-value for the correlation coefficient
c$p.value

```

Also notice that the $R^2$ value in the regression model is the same as the squared correlation coefficient. Remember that the $R^2$ in the regression model is the degree to which the model account for the data [@lsr], [also see here](https://en.wikipedia.org/wiki/Coefficient_of_determination). 

```{r, eval = FALSE}

summary(m)$r.squared

c$estimate^2

```

These similarities comes from the fact that they are the same analysis. The degree to which the two variables co-varies. 

```{r, echo = FALSE}

m <- lm(DXA_LBM_T1  ~ SQUAT_VOLUME, data = dat)
ci <- confint(m)

```


The additional benefit of using a regression analysis comes from the interpretation of the regression coefficient estimates. In our example we can see that the increasing the weekly volume with one ton increases percentage lean mass by `r round(coef(m)[2], 3)`%-points. The confidence interval is given on the same scale and can be retrieved by using the code below:

```{r, eval = FALSE}
confint(m)
```

This shows that the true value could be as low as  `r round(ci[2,1], 3)` and as high as `r round(ci[2,2], 3)`. Something that again indicates that the two variables do not vary together. 

## Correlation comes in many forms

If you look at the help pages for `cor` (`?cor`) you will see that you may specify the type of correlation used for analysis. Commonly used are Pearson's (default) and Spearman's correlation coefficient. The difference between these two is that the Spearman's correlation coefficient does not assume normally distributed data. This is basically a correlation of ranks. The highest number in a series of numbers will have the highest rank and the smallest will be given the lowest ( = 1).

We can prove this! The `rank` function gives a ranking to each number. We first plot the data as continuous values and then as ranks:

```{r, eval = FALSE}

dat %>%
  ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal()


dat %>%
  ggplot(aes(rank(SQUAT_VOLUME),
             rank(DXA_LBM_T1))) + geom_point() + theme_minimal()


```

We can see in the plot that the relationship persist after rank transformation. 

To use the Spearman's correlation coefficient we specify `"spearman"` in the `cor.test` function.

```{r, eval = FALSE}
cor.test(dat$SQUAT_VOLUME, dat$DXA_LBM_T1, method = "spearman")

```

To see that this is similar to using Pearson's correlation coefficient with ranked data, we do just that!

```{r, eval = FALSE}
cor.test(rank(dat$SQUAT_VOLUME), rank(dat$DXA_LBM_T1), method = "pearson")
```

Success! Another statistical mystery unlocked!

In this case the interpretation of tests using ranked data and un-transformed data are very similar. When do we use the rank based correlation? In cases when assumptions are not met, a rank based correlation will protect us from making bad decisions when for example a single data point "drives" a correlation. The the rank-based correlation (Spearman's) will be more conservative.

## Statistical and subject-matter interpretations

It is now very important to stop and think about the estimates that we arrived to above. We have concluded that lean body mass correlate quite well with squat volume calculated as the amount of weight lifted per week. Does this mean that individuals that exercise with higher volumes have more muscle? Perhaps, but could it also mean that individuals that are taller and heavier work out with greater resistance? Perhaps. In any case, the correlation (and regression) analysis of snap-shot observational data may always be have underlying patterns that better exoplain mathematical relationships other that the causation that first comes to mind. We have to thread carefully when interpreting association [@RN2902, Chapter 2, 4 and 5], this is were you subject-matter knowledge is handy.


## Summary

The correlation coefficient has many similarities with a univariate regression model. Correlations measures strength of association, but the regression model comes with benefits in terms of interpretation. The correlation only takes two variables but we can extend the regression model. When we think that data do not match our assumptions we can do correlation analysis using Spearman's rank correlation to avoid biased estimates of estimation. 

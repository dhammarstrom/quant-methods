[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "1 Introduction\nWelcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. These course notes covers almost the entire course through the combination of video lectures, tutorials and references to the course literature and external resources.\nThis book contains lecture notes for the course. Assignments, tutorials and other cours material has been moved to the course workshop site"
  },
  {
    "objectID": "01-intro-to-data.html#about-data-in-the-world-of-sport-and-exercise",
    "href": "01-intro-to-data.html#about-data-in-the-world-of-sport-and-exercise",
    "title": "2  Introduction to data science",
    "section": "2.1 About data in the world of sport and exercise",
    "text": "2.1 About data in the world of sport and exercise\nData are everywhere. Most of us always walk around with a data collection device in our pockets. This device (your mobile phone) records and store data about you throughout the day. Such data are the basis of the quantified self-movement1, which has grown in popularity as capabilities to record data from daily life have improved. People interested in quantifying their personal life does so for different reasons, but often with the intent to improve their health2.\nMuch of this kind of data is readily available to us because data privacy policies regard it as personal 3. With some effort, you can get your data out of your iPhone to explore, for example, your daily step count. I discovered that my phone(s) has been collecting data for me since 2016, and I tend to walk fewer steps on Sundays than Saturdays (see Figure Figure 2.1).\n\n\n\n\n\nFigure 2.1: Step count data from my iPhone displayed as all avalable data points (A, after data cleaning) and average step per weekday, per year and season (B).\n\n\n\n\nData are also collected and stored in publicly available databases. Such databases are created for the purpose of storing specific types of data, such as soccer4 or biathlon results5, or biological information, such as gene sequences6. Even data from scientific studies are often publicly available7, meaning we can perform scientific studies on unique data sets without collecting the data ourselves.\nThe above examples show an abundance of available data. The problem is that to understand a phenomenon better, we need techniques and methods to make sense of the data, and this is where data science and data literacy comes in. In the world of sports and exercise, regardless if you are interested in doing scientific investigations, coaching a soccer team or individual athletes, or helping patients recover from surgery using exercise therapy, you are faced with the problem of handling and making sense of data. Essential skills and a deeper understanding of data science are transferable between such areas of practice. One broader aim of this course is for you to develop skills to better understand data.\n\nThink about the literature! Spiegelhalter (The Art of Statistics, in the introduction chapter) talks about how statistics has evolved towards the broader field of data science. In data science, statistical theory and methods are just parts of the problem solving cycle. Try to think about how you would use the PPDAC cycle as a exercise coach and a scientist. What are the similarities and differences?"
  },
  {
    "objectID": "01-intro-to-data.html#replication-and-reproducibility",
    "href": "01-intro-to-data.html#replication-and-reproducibility",
    "title": "2  Introduction to data science",
    "section": "2.2 Replication and Reproducibility",
    "text": "2.2 Replication and Reproducibility\nIn scientific research, replication is a way to confirm scientific claims. When an independent group of researchers can confirm a result, the claim is more likely to be true. However, many results will be impossible to replicate due to the size of trials, costs, and urgency of the research question. A recent example is the many vaccine trials performed to develop a vaccine against COVID-198. Other examples concern studies with unique study populations, such as large-scale epidemiological studies (Peng, Dominici, and Zeger 2006), but the same is true for unique investigations in sport and exercise science.\nWhen studies are not likely to be replicated, reproducibility of the analyses and results has been suggested to be a minimum standard for scientific studies. Reproducibility means that independent researchers can draw similar results or conclusions from the same data (Peng, Dominici, and Zeger 2006).\nPeng et al. (Peng, Dominici, and Zeger 2006) suggests that a fully reproducible study has\n\nAvailable data.\nComputer code (software) that produces the results of the study.\nDocumentation that describes the software and data used in the study, and\nways to share the data and code.\n\nThe above principally relates to the trust we can place in scientific results. However, the minimum reproducibility standard also has advantages for the individual researcher (or master’s student)! When working with reproducible methods, we will develop ways of documenting and automating our analyses. This way of working with analyses will make it easier to collaborate with others. And, as it turns out, your most frequent collaborator is you in the future!\nReproducible data analysis means that you will make it explicit and transparent. In traditional data analysis, most activities are in the “black box.” To avoid bias (Ioannidis 2005), the “black box” needs to be opened, and you need to actively make transparent decisions all along the analytic pipeline (Leek and Peng 2015). This pipeline preferably involves the whole problem-solving cycle described by Spiegelhalter (Spiegelhalter 2019). However, the tools we will learn in this course focus primarily on the steps from the experimental design to the presentation of statistical results (Leek and Peng 2015). These steps include data collection (and storage), data cleaning, exploratory data analysis, statistical modeling, and statistical inference (and communication) (Leek and Peng 2015)."
  },
  {
    "objectID": "01-intro-to-data.html#tools-in-data-science",
    "href": "01-intro-to-data.html#tools-in-data-science",
    "title": "2  Introduction to data science",
    "section": "2.3 Tools in data science",
    "text": "2.3 Tools in data science\nWays to interpret and make sense of data involve different methods. These methods are often implemented in computer software, which means that when you want to understand data as a practitioner (scientist, coach, analyst), you must master some computer software. Microsoft’s Excel is one of the most common software used to understand data, even among professional data scientists9. You can do fantastic stuff with Excel! In the world of sport and exercise, Excel has been used in diverse activities such as scientific investigations, planning and recording training for world champions10, and scheduling appointments.\nFor scientific research, most people use additional software to do statistical analyses. If you have spent time in higher education, you have probably heard about SPSS, Stata, or Jamovi. These are all specialized software used for statistical analyses.\nThe tools mentioned above can all be used as part of a fully reproducible workflow. However, some software solutions suit this requirement better than others. Going back to the description of reproducible science as made by Peng et al. (Peng, Dominici, and Zeger 2006), we want software where analyses can be\n\nHuman- and computer-readable, meaning that we want to be able to write scripts or computer programs that execute the analyses.\nDocumented, meaning that along the code, we want to be able to describe what the code does.\nAvailable and able to share with others, meaning that our analyses can be run on open and free software to maximize the ability to share them.\n\nThis means that the software we would prefer should be run using scripts (as opposed to point and click) and be free of charge (and open source, as opposed to expensive and proprietary). These criteria can be fulfilled when we use software written around the R language (although alternatives exist 11).\nR is a computer language especially well suited for reproducible data analysis. As users can contribute software extensions, also called packages, many specialized software implementations exist for tasks such as creating figures or analyzing specific data. Around R, people have been developing auxiliary software for reproducible data analysis. The negative part of all these opportunities is that using R requires effort. The learning curve is steep!\nEven though you might not use R ever again after this course, trying to learn it will let you know something about programming, modern data science capabilities, statistical analysis, and software/computers in general. These areas are all aspects of our modern society and are transferable regardless of what computer language we are talking about.\nA big challenge when working with complex analyses or other large projects over time is keeping track of changes. Another challenge might be effective collaboration with others and with yourself in the future. To overcome these challenges, we can use a version control system connected to a social platform for distributing computer code and data. Github is a web-based platform that provides this functionality. It is a potent combination if you want to collaborate and share what you are working on."
  },
  {
    "objectID": "01-intro-to-data.html#footnotes",
    "href": "01-intro-to-data.html#footnotes",
    "title": "2  Introduction to data science",
    "section": "",
    "text": "Read more about the quantified self movement in this Wikipedia article↩︎\nSee this website for intriguing examples↩︎\nSee e.g. Apples Privacy Policy.↩︎\nunderstat.com stores match specific data from major leagues. Data are available through software packages such as worldfootballR↩︎\nbiathlonresults.com/ hosts results from the international biathlon federation. An example of analyzed data can be seen here.↩︎\nEnsembl and the National center for biotechnology information are commonly used databases in the biomedical sciences.↩︎\nWe published our raw data together with a recent paper (Mølmen et al 2021 doi: 10.1186/s12967-021-02969-1.) together with code to analyze it in a public repository.↩︎\nhttps://www.evaluate.com/vantage/articles/news/snippets/its-official-covid-19-vaccine-trials-rank-among-largest↩︎\n(See for example this ranking)[https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html].↩︎\nThe amount of time used by different coaches to create their own specific coaching software really makes many of them amateur software engineers. See for example this training journal from swedish orienteering.↩︎\nIn addition to R, Python offers a free open source environment for reproducible analyses. The choice between the two are matter of taste.↩︎\nSee the quarto documentation for details on creating pdfs and installing TeX distributions https://quarto.org/docs/output-formats/pdf-basics.html↩︎"
  },
  {
    "objectID": "01-intro-to-data.html#references",
    "href": "01-intro-to-data.html#references",
    "title": "2  Introduction to data science",
    "section": "2.6 References",
    "text": "2.6 References\n\n\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” Journal Article. PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nLeek, J. T., and R. D. Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Journal Article. Nature 520 (7549): 612. https://doi.org/10.1038/520612a.\n\n\nPeng, R. D., F. Dominici, and S. L. Zeger. 2006. “Reproducible Epidemiologic Research.” Journal Article. Am J Epidemiol 163 (9): 783–89. https://doi.org/10.1093/aje/kwj093.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "02-spreadsheets.html#cells-and-simple-functions",
    "href": "02-spreadsheets.html#cells-and-simple-functions",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.1 Cells and simple functions",
    "text": "3.1 Cells and simple functions\nA spreadsheet consists of cells, these can contain values, such as text, numbers, formulas and functions. Cells may also be formatted with attributes such as color or text styles. Below is an example of some data entered in a spreadsheet.\n\n\n\n\n\nExample entries from an Excel spreadsheet\n\n\n\n\nCell B6 contains a simple formula: = C6 + D6. This formula adds cells C6 and D6 resulting in the sum, 8. In formulas, mathematical operators can be used (\\(+, -, \\times , \\div\\) ). Formulas can be also extended with inbuilt function such as showed in Table 3.1.\n\n\nTable 3.1: Often used functions in excel\n\n\nFunction\nEnglish\nNorwegian\n\n\n\n\nSum\nSUM()\nSUMMER()\n\n\nAverage\nAVERAGE()\nGJENNOMSNITT()\n\n\nStandard deviation\nSTDEV.S()\nSTDEV.S()\n\n\nCount\nCOUNT()\nANTALL()\n\n\nIntercept\nINTERCEPT()\nSKJÆRINGSPUNKT()\n\n\nSlope\nSLOPE()\nSTIGNINGSTALL()\n\n\nIf\nIF()\nHVIS()\n\n\n\n\nThe sum, average, standard deviation, and count are simple functions for summarizing data. Intercept and slope are functions used to get simple associations from two sets of numbers (based on a regression model). The IF function is an example of a function that can be used to enter data in a cell conditionally. For example, IF cell A1 contains a certain number, then cell B1 should display another specified text.\nWhen looking for tips and tricks online, you may come across functions for excel in other languages than what is installed on your computer. To translate functions and for a complete overview of functions included in Microsoft Excel, see this website en.excel-translator.de/."
  },
  {
    "objectID": "02-spreadsheets.html#tidy-data-and-data-storage",
    "href": "02-spreadsheets.html#tidy-data-and-data-storage",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.2 Tidy data and data storage",
    "text": "3.2 Tidy data and data storage\nHadley Wickham (the author of many commonly used R packages) quotes Tolstoy when describing the principle of tidy data (Wickham 2014). This quote is so famous that it has given name to a principle. The principle in turn comes in many variants but basically states that when something goes wrong, it can be wrong in multiple ways. But when it is right/correct/works/succeeds, it does so in only one way1. This principle can be applied to data sets. There are so many ways that formatting of data sets can be problematic, but a limited set of principles makes it good.\n\n\n\n\n\nLeo Tolstoy at the time when he was (possibly) authoring Anna Karenina. (Source: https://en.wikipedia.org/wiki/Leo_Tolstoy)\n\n\n\n\nA tidy data set consists of values originating from observations and belonging to variables. A variable is a definition of the values based on attributes. An observation may consist of several variables (Wickham 2014).\nA tidy data set typically has one observation per row and one variable per column. Let’s say that we want to collect data from a strength test. A participant (participant is a variable) in our study conducts tests before and after the intervention (time is a variable) in two exercises (exercise is a variable), and we record the maximal strength in kg (load is a variable). The data set will look like the table below (Table 3.2).\n\n\nTable 3.2: Example of tidy data\n\n\nParticipant\nTime\nExercise\nLoad\n\n\n\n\nBruce Wayne\npre\nBench press\n95\n\n\nBruce Wayne\npost\nBench press\n128\n\n\nBruce Wayne\npre\nLeg press\n180\n\n\nBruce Wayne\npost\nLeg press\n280\n\n\n\n\nAnother example contains variables that actually carries two pieces of information in one variable. We again did a strength test, this time as maximal isometric contractions and in each test consisted of two attempts. We record this in two different variables, attempt 1 and 2. The resulting data set could look something like in Table Table 3.3.\n\n\nTable 3.3: Another example of tidy data.\n\n\nParticipant\nTime\nExercise\nAttempt1\nAttempt2\n\n\n\n\nSelina Kyle\npre\nIsometric\n81.3\n92.5\n\n\nSelina Kyle\npost\nIsometric\n97.1\n114.1\n\n\n\n\nTo make this data set tidy we need to extract the attempt information and record it in another variable as seen in Table 3.4.\n\n\nTable 3.4: A third example of tidy data.\n\n\nParticipant\nTime\nExercise\nAttempt\nload\n\n\n\n\nSelina Kyle\npre\nIsometric\n1\n81.3\n\n\nSelina Kyle\npre\nIsometric\n2\n92.5\n\n\nSelina Kyle\npost\nIsometric\n1\n97.1\n\n\nSelina Kyle\npost\nIsometric\n2\n114.1\n\n\n\n\nThis transformation naturally gives additional rows to the data set. It is sometimes referred to as “long format” data instead of the structure where each attempt is given separate variables, called “wide format.” You will notice during the course that the long format is most convenient for most purposes. This is true when we create graphs and do statistical modeling. But sometimes, a variable must be structured in a wide format to allow certain operations.\nIf we follow what is recommended by Broman and Woo (Broman and Woo 2018), it is clear that each cell in a spreadsheet should only contain one value. If we, for example, decide to format a cell to a certain color, we add data to that cell on top of the actual data. You might add color to a cell to remember to add or change data. However, this information is lost when you use the data set in other software. Instead, you should add another variable to allow such data to be properly recorded. Using a variable called comments, you can add text describing information about that particular observation, information that is not lost when you use the data set in another software."
  },
  {
    "objectID": "02-spreadsheets.html#recording-data",
    "href": "02-spreadsheets.html#recording-data",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.3 Recording data",
    "text": "3.3 Recording data\nA trade secret2 from people who work all day with data and programming is that they are lazy. Lazy in the sense that you want to type as little as possible and avoid moving your arm to the computer mouse whenever possible. When recording data, we can be lazy too. We can do this by shortening variable names and not using CAPITAL letters when entering text in data storage. After a hard day at the keyboard, you will be happy to write strtest instead of Strength Test. The extra effort of using two capital letters might be the thing to tip you over the edge3. However, we should not be too lazy either; variable names and values should be “short but meaningful” (Broman and Woo 2018).\n\n\n\n\n\nD-FENS Foster gets pushed over the edge (Source: https://en.wikipedia.org/wiki/Falling_Down)\n\n\n\n\nData and variables should also be consistent. Do not mix data type; use a consistent way of entering e.g., dates and time, and do not use spaces or special characters. To enforce this, you might want to start your data collection by writing up a data dictionary describing all variables you collect. The dictionary can set the rules for your variables. This dictionary can also guide your data validation.\nIn Excel, you can use data validation to set rules for data entry. For example, if you have a numeric variable, you can set Excel only to accept numbers in a specified set of cells. Such rules make it harder to enter erroneous data."
  },
  {
    "objectID": "02-spreadsheets.html#saving-data",
    "href": "02-spreadsheets.html#saving-data",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.4 Saving data",
    "text": "3.4 Saving data\nData from spreadsheets can be saved as special spreadsheet files, such as .xlsx. This format allows for functions, multiple spreadsheets in the same file (tabs), and cell formatting. You do not need this fancy format if you follow the tips described above and in (Broman and Woo 2018). Instead, you can store your data as a .csv file. This format may be read and edited with Excel (or another spreadsheet software) and in plain text. Data entered in this format (comma-separated values; csv) can look like this in a text editor:\n Participant;Time;Exercise;Attempt;load\n Selina Kyle;pre;Isometric;1;81.3  \n Selina Kyle;pre;Isometric;2;92.5\n Selina Kyle;post;Isometric;1;97.1\n Selina Kyle;post;Isometric;2;114.1\nThis format is quite lovely. The data takes little space; the simple format requires that data is well documented using e.g., a data dictionary; and it is available for many other software as the format is simple. You can document the data using a README file that could describe the purpose and methods of data collection, how the data is structured, and what kind of data the variables contains. A simple README file can be written in a text editor such as Notepad and saved as a .txt file. Later in this course, we will introduce a “markup” language often used to create README files containing a syntax that formats the text to a more pleasant style when converted to other formats."
  },
  {
    "objectID": "02-spreadsheets.html#footnotes",
    "href": "02-spreadsheets.html#footnotes",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "",
    "text": "See https://en.wikipedia.org/wiki/Anna_Karenina_principle↩︎\nA trade secret as in “not generally known to the public”. See en.wikipedia.org/wiki/Trade_secret.↩︎\nIn the movie Falling Down, Michael Douglas plays a unemployed engineer who gets push over edge, would it have been enough with a few to many capital letters?↩︎"
  },
  {
    "objectID": "02-spreadsheets.html#references",
    "href": "02-spreadsheets.html#references",
    "title": "3  Storing data in spreadsheets and tabular data",
    "section": "3.5 References",
    "text": "3.5 References\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” Journal Article. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nStephen, G. Powell, R. Baker Kenneth, and Lawson Barry. 2009. “Errors in Operational Spreadsheets.” Journal Article. Journal of Organizational and End User Computing (JOEUC) 21 (3): 24–36. https://doi.org/10.4018/joeuc.2009070102.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal Article. Journal of Statistical Software; Vol 1, Issue 10 (2014). https://www.jstatsoft.org/v059/i10.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Journal Article. Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites"
  },
  {
    "objectID": "01-intro-to-data.html#installing-and-getting-to-know-the-required-software",
    "href": "01-intro-to-data.html#installing-and-getting-to-know-the-required-software",
    "title": "2  Introduction to data science",
    "section": "2.4 Installing and getting to know the required software",
    "text": "2.4 Installing and getting to know the required software\nAs noted above, there are multiple computer languages and software solutions that could satisfy our needs. However, in this course, we will focus on a combination of continuously improved tools to make it easy for the user to collaborate and communicate data analyses. Below is a checklist of what you must install on your system to take full advantage of the proposed tools.\n\n2.4.1 R and RStudio\nR is a free, open-source software designed for statistical computing. We will use R as a part of an environment (using R Studio, introduced below). To download and install R:\n\nGo to https://cran.uib.no/,\nSelect your operating system (Download R for Windows, MacOS or Linux).\n\nIf you have Windows, choose base, click on “Download R (…) for windows”, save and run the file. The installation process should be self explanatory.\nIf you have MacOS, download and install the latest release.\n\nRun the installer to install R.\n\nRStudio is a software designed to make it easier to use R. It is free to download and use. It is designed as an integrated development environment that lets you organize your work together with R and other tools. Install it by going to https://www.posit.co/.\n\nSelect “Products” and RStudio IDE\nScroll down and find the FREE open source edition\nDownload the installer made for your operating system.\n\n\n\n2.4.2 Git and Github\nGit is a software that you need to install on your system in order to use version control. Github is the web platform that allows collaboration and web-based storage of your work. First, we will install git.\nFor windows:\n\nIf you have Windows, Go to https://git-scm.com/downloads and download the latest version for your operating system.\nRun the installer. Make a note of where you installed it!\n\nFor Mac:\n\nIf you are on Mac, the easiest thing is to first install Homebrew, this will make it easy to get the latest version of what we will need. Go to https://brew.sh/ and follow the instructions. Note that you will need to open the terminal and enter the install command.\nInstall git by entering the follwing command in a freshly opened terminal:\n\nbrew install git\nCheck if git was installed by restarting the terminal and write\ngit --version\nAdditional warnings might appear indicating that you’ll need some extra software. More specifically, you might need Xcode command line tools. To install these, go to your terminal and enter\nxcode-select --install\nIf you had problems with the homebrew installation itself or the brew installation of git before, try again after installing xcode command line tools.\n\n\n2.4.3 Connecting to GitHub\nFirst we will let RStudio know where git is located\n\nOpen RStudio, go to Global Options under the Tools menu. Go to the Git/SVN sub-menu and find the folder where git.exe is located by browsing in the “Git executable” field.\n\nOn windows:\nIf you have installed git using default settings your git.exe should be located in C:/Program Files/Git/bin/git.exe.\nOn Mac:\nIf you have installed git using homebrew, your git version may be found in /usr/local/bin/git.\nTo register for a Github account\n\nGo to Github.com.\nFind “Sign up” and follow the instructions.\n\nNext we need to connect our git software to github. This is done by authentication. There are several options, however below are two options that should work right away!\n\n2.4.3.1 Installing GitHub desktop\n\nGo to desktop.github.com\nDownload the installer and follow the instructions.\nOpen GitHub Desktop and go to File &gt; Options &gt; Accounts and select Sign In to Github.com, follow the instructions\n\n\n\n2.4.3.2 Installing Github CLI\nIf you were successful in authenticating with Github desktop as described above, you should be all set. However, as an alternative you could install and use Github CLI. This is a collection of command line commands that makes it easy to use github from the command line. I recommend installing them:\n\nGo to https://cli.github.com/ and follow the instructions.\n\n\nFor windows, install GitHub CLI with the installer.\nFor Mac, use homebrew: brew install gh\n\nNext we will perform the authentication process:\n\nOpen a terminal and type gh auth login, follow the instructions.\n\nDone!\n\n\n\n2.4.4 A note on Git and clients\nAs noted above, git is a software containing a number of functions for version control of files collected in a folder (or repository). A client in this context refers to a user interface that makes it easy to communicate with git. RStudio has some features that makes it possible to execute git commands by clicking, however this client is not very powerful, you might want another, or several other alternatives.\nFirst, git is available from the command line. It might look like this:\ngit add -A\nWe will touch upon more git commands for the command line later. The above adds all changes you have made to a list of changes that will be included in your next snapshot of your project. More on that later!\nSeveral Git clients can be run at the same time. This means that you might do some git on the command line in a terminal window in RStudio, and you might follow the changes in a graphical user interface, such as GitHub Desktop. The graphical user interface lets you navigate more easily and might help you understand what git is doing. We will be using GitHub desktop, so you make sure you have installed it (see above).\n\n\n2.4.5 Quarto and friends\nThe R community has pioneered literate programming for data analysis by early adoption of file formats that lets the user combine computer code and output with text (Peng, Dominici, and Zeger 2006). A well adopted file format in recent years have been R markdown which combines R code with text and lets the user compile reports in multiple output formats from a source document. R markdown is an “R-centric” approach to literate programming. Even though it lets you combine multiple computer languages, all code execution goes through R. Recently, a new format has been introduced, Quarto, which is not executed through R but its own dedicated software, Quarto.\nRmarkdown and Quarto have many similarities in that you can use markdown, a well established markup language to format text with a plain text editor (like notepad). This means that for the R user, most differences between RMarkdown and quarto in formatting your documents are irrelevant for getting started.\nAs quarto authoring requires its own software, we need to do some installation.\n\nGo to quarto.org\nClick “Get Started” and follow the instructions.\n\nA nice output from a quarto source documents is a PDF. In order to create PDFs using R/RStudio/quarto we need to install a version of the typesetting system TeX. Quarto recommends12 using tinytex which is easily installed after you have installed quarto.\n\nOpen up RStudio and a fresh terminal\ntype quarto install tinytex and follow the instructions.\n\nYou should be ready to go now!"
  },
  {
    "objectID": "01-intro-to-data.html#summing-up-and-where-to-find-help",
    "href": "01-intro-to-data.html#summing-up-and-where-to-find-help",
    "title": "2  Introduction to data science",
    "section": "2.5 Summing up and where to find help",
    "text": "2.5 Summing up and where to find help\nWe have installed R, RStudio, git, GitHub desktop/CLI, quarto and tinytex. You have also created a github account. These are the tools that you will need to go further in this course. But what if you run into problems? Do not worry, the internet is at your service! A lot of people work very hard to make it easy for beginners to adopt their tools. Documentation of the tools we have installed so far is available through google or any other search engine. People are also very helpful in answering questions, answers to large and small problems can be found in forums such as stack overflow(see below).\nLearning new skills, like doing data analysis by programming, can be hard but rewarding. If you want to make your learning experience less hard, consider these points:\n\nThere are (almost always) multiple solutions to a problem. When faced with difficulties, do not give up trying to search for a perfect single solution. Instead know that there are multiple ways of defining the problem and therefore multiple ways of making stuff work.\nSomeone else has already had the same problem. The internet is full of questions and answers, also related to what ever problem you might have. Learning how to write “googleable” questions is a great skill. By adding “in R” to your problem in a google search term often helps finding R related solutions.\nFind your motivation. The skills that you will learn in this course are transferable to countless potential work related roles for the future you! To be able to showcase these skills may lead you to your dream job! Find your motivation for learning how to analyze data and communicating insights!\n“Microdosing” statistical learning. Replace your social media influencers with R users and data scientists! I find R people on Twitter and mastodon. Tweets and posts in this format keeps your R brain going!\n\n\n2.5.1 A (small) list of reference material and resources\n\nR for Data Science is a very comprehensive guide to working with R. It can be used chapter by chapter or by looking for tips on specific subjects.\nThe official An Introduction to R released by the R Core Team gives a thorough overview of R. This document can be used to find explanations to basic R code.\nLearning statistics with R Is a free textbook where statistical concepts are integrated with learning R. Use this book as a reference.\nHappy Git and GitHub for the useR is used as background material for our workshop in version control and collaborative data analysis.\nTidyverse Is a collection of R packages that makes it easier to be productive in R. Here you will find documentation for ggplot, dplyr and tidyr which are all packages that we will use extensively in the course.\nStack overflow is a web platform where users provide answers to questions raised by other users. Here you will find answers to many of your R-related questions. Stack overflow will likely come up if you google a R problem by you can also search the website.\nR bloggers collects blog posts from R users, here you can find interesting use cases of R and tips."
  },
  {
    "objectID": "01-intro-to-data.html#references-and-footnotes",
    "href": "01-intro-to-data.html#references-and-footnotes",
    "title": "2  Introduction to data science",
    "section": "2.6 References and footnotes",
    "text": "2.6 References and footnotes\n\n\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” Journal Article. PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nLeek, J. T., and R. D. Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Journal Article. Nature 520 (7549): 612. https://doi.org/10.1038/520612a.\n\n\nPeng, R. D., F. Dominici, and S. L. Zeger. 2006. “Reproducible Epidemiologic Research.” Journal Article. Am J Epidemiol 163 (9): 783–89. https://doi.org/10.1093/aje/kwj093.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "02-spreadsheets.html#references-and-footnotes",
    "href": "02-spreadsheets.html#references-and-footnotes",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.5 References and footnotes",
    "text": "3.5 References and footnotes\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” Journal Article. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nStephen, G. Powell, R. Baker Kenneth, and Lawson Barry. 2009. “Errors in Operational Spreadsheets.” Journal Article. Journal of Organizational and End User Computing (JOEUC) 21 (3): 24–36. https://doi.org/10.4018/joeuc.2009070102.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal Article. Journal of Statistical Software; Vol 1, Issue 10 (2014). https://www.jstatsoft.org/v059/i10.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Journal Article. Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "03-intro-to-r.html#installing-r",
    "href": "03-intro-to-r.html#installing-r",
    "title": "4  Getting to know R and RStudio",
    "section": "4.2 Installing R",
    "text": "4.2 Installing R\nR is a free, open-source software designed for statistical computing. We will use R as a part of an environment (using R Studio, introduced below). To download and install R:\n\nGo to https://cran.uib.no/,\nselect your operating system (Download R for Windows, MacOS or Linux).\n\nIf you have Windows, choose base, click on “Download R (…) for windows”, save and run the file. The installation process should be self explanatory.\nIf you have MacOS, download and install the latest release."
  },
  {
    "objectID": "03-intro-to-r.html#installing-r-studio",
    "href": "03-intro-to-r.html#installing-r-studio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.3 Installing R Studio",
    "text": "4.3 Installing R Studio\nRStudio is a software designed to make it easier to use R. It is free to download and use. It is designed as an integrated development environment that lets you organize your work together with R and other tools. Install it by going to https://www.rstudio.com/.\n\nSelect “Products” and RStudio\nGo to desktop and select “DOWNLOAD RSTUDIO DESKTOP”\nSelect the free open source license and download the file made for your operating system (use the installers)."
  },
  {
    "objectID": "03-intro-to-r.html#getting-to-know-r-and-rstudio",
    "href": "03-intro-to-r.html#getting-to-know-r-and-rstudio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.4 Getting to know R and RStudio",
    "text": "4.4 Getting to know R and RStudio\n\n\n\n\nR is a software used for scientific/statistical computing. If R is the engine, RStudio is the rest of the car. What does this mean?\nWhen doing operations in R, you are actually interacting with R through RStudio. RStudio have some important components to help you interact with R.\n\n4.4.1 The source\nThe source is where you keep your code. When writing your code in a text-file, you can call it a script, this is essentially a computer program where you tell R what to do. It is executed from top to bottom. You can send one line of code, multiple lines or whole sections into R. In the image below, the source window is in the top left corner.\n\n\n4.4.2 Environment\nThe environment is where all your objects are located. Objects can be variables or data sets that you are working with. In RStudio the environment is listed under the environment tab (bottom left in the image).\nCopy and run the code below.\n\na &lt;- c(1, 2, 4)\n\nWhat happened in your environment?\n\n\n4.4.3 The console\nHere you can directly interact with R. This is also where output from R is printed. In the image below, the console is in the top right corner.\n\n\n4.4.4 Files, plots, packages and help files\nIn RStudio files are accessible from the Files tab. The files tab shows the files in you root folder. The root folder is where R will search for files if you till it to. We will talk more about the root folder later in connection with projects. Plots are displayed in the Plot tab. Packages are listed in the packages tab. If you access the help files, these will be displayed in the help tab. In the image below all these tabs are in the bottom right corner.\n\n\n\n\n\nRStudio when first opened up.\n\n\n\n\n\n\n4.4.5 Customizing the apperance of RStudio\nTo access options for RStudio, go to Tools -&gt; Global options\n\n\n\n\n\nAccessing options for your RStudio IDE\n\n\n\n\nUnder appearance you can customize the theme of RStudio, select something that is easy on the eye!\n\n\n\n\n\nAccessing options for your RStudio IDE and selection a theme\n\n\n\n\nUnder pane layout, you can set where you want your tabs, I like to have the source on the left, above the environment. This way you can have the source window at full vertical size and still look at plots and the console to the right.\n\n\n\n\n\nAccessing options for your RStudio IDE and set the panes"
  },
  {
    "objectID": "03-intro-to-r.html#reproducible-computing",
    "href": "03-intro-to-r.html#reproducible-computing",
    "title": "4  Getting to know R and RStudio",
    "section": "4.5 Reproducible computing",
    "text": "4.5 Reproducible computing\n\n\n\n\nComputations are reproducible when you can show how they were performed. This is achieved by creating “programs” from where your analyses are done. In R, these programs are lines or R code stored in a text-file, either .R-files or .Rmd-files. .R-files are scripts only containing code and comments. A .Rmd-file is a special script combining text and computer code, when the Rmd-file is executed, it creates a report and outputs the results from the code.\nThis means that to work in a reproducible way, you need to script all your operations.\n\n\n\n\n\nReproducible vs. non-reproducible workflow\n\n\n\n\nImportantly, in RStudio you can shut down storing temporary objects in a environment that is relaunched on start up. What is the consequence of having such a situation?\nTo disable this option, set save works pace to NEVER!\n\n\n\n\n\nSet the workspace option to never save."
  },
  {
    "objectID": "03-intro-to-r.html#packages",
    "href": "03-intro-to-r.html#packages",
    "title": "4  Getting to know R and RStudio",
    "section": "4.5 Packages",
    "text": "4.5 Packages\nThe R ecosystem consists of packages. These are functions organized in a systematic manner. Functions are created to perform a specialized task. And packages often have many function used to do e.g. analyses of a specific kind of data, or more general task such as making figures or handle data.\nIn this course we will use many different packages, for example dplyr, tidyr and ggplot2. dplyr and tidyr are packages used to transform and clean data. ggplot2 is used for making figures.\nTo install a package, you use the install.packages() function. You only need to do this once on your computer (unless you re-install R). You can write the following code in your console to install dplyr.\n\ninstall.packages(\"dplyr\")\n\nAlternatively, click “Packages” and “Install” and search for the package you want to install. To use a package, you have to load it into your environment. Use the library() function to load a package.\n\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "03-intro-to-r.html#installing-and-using-swirl",
    "href": "03-intro-to-r.html#installing-and-using-swirl",
    "title": "4  Getting to know R and RStudio",
    "section": "4.5 Installing and using swirl",
    "text": "4.5 Installing and using swirl\nSwirl is a great way to get to know how to talk with R. Swirl consists of lessons created for different topics. Install swirl by typing the following into your console:\n\ninstall.packages(\"swirl\")\n\nWhen swirlis installed you will need to load the package This means that all functions that are included in package becomes available to you in your R session. To load the package you use the library function.\n\nlibrary(\"swirl\")\n\nWhen you run the above command in your console you will get a message saying to call swirl() when you are ready to learn. I would like you to run the course “R Programming: The basics of programming in R”. Swirl will ask if you want to install it. After installation, just follow the instructions in the console. To get out of swirl, just press ESC."
  },
  {
    "objectID": "03-intro-to-r.html#r-scripts",
    "href": "03-intro-to-r.html#r-scripts",
    "title": "4  Getting to know R and RStudio",
    "section": "4.6 R scripts",
    "text": "4.6 R scripts\nAs pointed out elsewhere (Wickham and Grolemund 2017; Peng, Dominici, and Zeger 2006), programming is an important part of a reproducible data analysis. This lets you build your analysis, go back a change components of it an re-run it with any number of changes. In this process you will probably learn more about your data. Putting all these steps in a program lets you save the whole analytic process. In R you may start by working with R scripts. These are basically text files that are written with a special syntax that can be interpreted by your computer. Additionally you have the possibility to add comments that makes the code more readable. R code is generally easy to read. But you will likely need additional comments to make it easier to show what you intend to do.\nA R script can be thought of as a computer program that when executed from top to bottom perform as series of steps in the order that they appear in the file. A feature of a well working program is that it is self-contained, i.e. it contains all parts needed to run. If you need to load a package or data, make sure that these steps are in the beginning of the script.\nA nice feature of combining code and comments can be that you first write a plan in plain language and then add computer code to perform the steps that you want to do. Below is a simple example. Comments start with a #, this is interpreted by R as non-code line and will be ignored. The R code is structured in series, the first steps are needed to perform sequential steps. As mentioned, the work-flow of creating the example below would be to first make a plan by writing the comments and the adding the code.\n\n# Create a data set in a data.frame\ndf &lt;- data.frame(x = rnorm(100, 100, 10), \n                 y = runif(100, min = 10, max = 25)) \n\n# Add column x and y together in a new variable called z\ndf$z &lt;- df$x + df$y\n\n\n# Make a figure of the resulting data frame by plotting x against z\nwith(df, plot(x, z))\n\nWhen working in RStudio, you can run a bit of the code by selecting it and pressing CTRL + ENTER (CMD + ENTER on Mac). You can also execute a line simply by having your cursor on a specific line and press CTRL + ENTER. Code execution means that the specific part of the script (line or section) is “copied” to the console and “activated”.\nR Scripts can also be “sourced”. This means that the whole script will be executed from to to bottom when you tell R to do so. Lets say that you have a script that creates a figure and saves it, called figure1.R. By using the source() function you can tell R to execute the script (source(\"figure1.R\")). As you can see in this example the filename extension .R tells you that a file can be intepreted as a R script."
  },
  {
    "objectID": "03-intro-to-r.html#r-markdown-files",
    "href": "03-intro-to-r.html#r-markdown-files",
    "title": "4  Getting to know R and RStudio",
    "section": "4.7 R markdown files",
    "text": "4.7 R markdown files\nR markdown files are more advanced computer programs as they in a structured way combines plain text and code to create an output file such as a .html, .pdf or .doc document. The text parts are written using a special syntax, markdown. The point of markdown is that you will use the same syntax that is later possible to convert to multiple formats. The syntax let’s you do all formatting explicitly, for example instead of getting your mouse to superscript some text you can add syntax a^2^ to achieve a2.\nA full guide to RMarkdown can be found on the official R markdown web pages. I suggest you take the time to get an overview of this langiage as it will make more fluent in the tools that enables reproducible computing. When writing R markdown, it is handy to have a cheat sheet close by when writing, here is an example1.\nIf you do not want to write text in a simple text editor, RStudio has it’s own “visual markdown editor”. This editor contains similar functions by press of buttons as in for example word. R Markdown files has the file name extension .Rmd.\n\n4.7.1 Starting up your first R markdown file\nA R markdown report is basically a text document containing plain text and code. When you compile your report, the code will be evaluated and figures, calculations and so on will be performed per your specifications. The resulting file will be an html, docx or pdf file. You can choose if you would like to display your code or not but your code is always available in your source document. R Markdown is very versatile, you can make word documents, blog posts, websites and pdf documents2.\nWhen in R Studio, you can start a new document using File &gt; New File &gt; R Markdown…. This will launch a file in your script window looking something like this:\n---\ntitle: \"Untitled\"\nauthor: \"Daniel Hammarström\"\ndate: \"2020 05 09\"\noutput: html_document\n---\n\n\n\n## R Markdown\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;.\n\nThis is not an empty document and you have to remove the pre-written instructions. These instructions are quite handy though.\nBasically, in code chunks you write R code, this code will be evaluated and the output will be displayed in the file you create. Between code chunks you can write markdown text. This will be displayed as ordinary text in your created document. The plain text sections can also contain code.\nA code chunk is created using\n```{r, eval=TRUE}\n1 + 1\n```\nThis code chunk calculates 1+1, when you compile the document, the result of this calculation will be shown below the code chunk. The same computation can be made “inline”. An inline code chunk is created using `r 1+1`, here only the result of this computation will be shown in your text.\nWhen you compile the doucument it is called “knitting”, R uses a package called knitr made to compile R Markdown files. In the upper part of the source window, there is a button kalled Knit. When you press it, RStudio will aske you to save the Rmd file and an output file will be created.\n\n\n4.7.2 Microsoft Word intergration\nSometimes it is usefull to “knit” to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown can be used as a source for word documents (.docx).\nTo create a word document from your Rmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: word_document\n---\nThe output: word_document tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use thiss as reference.\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\n---\nEdit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the “Title” style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it.\nWhen you knit the document again, your updated styles will be used your word document.\nHere you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here.\n\n\n4.7.3 Adding references\nReferences/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are avaliable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib.\nThe bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\nbibliography: bibliography.bib\n---\nAdd citations to the file in bibtex-format. Here is an example:\n@Article{refID1,\n   Author=\"Ellefsen, S.  and Hammarstrom, D.  and Strand, T. A.  and Zacharoff, E.  and Whist, J. E.  and Rauk, I.  and Nygaard, H.  and Vegge, G.  and Hanestadhaugen, M.  and Wernbom, M.  and Cumming, K. T.  and Rønning, R.  and Raastad, T.  and Rønnestad, B. R. \",\n   Title=\"{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}\",\n   Journal=\"Am. J. Physiol. Regul. Integr. Comp. Physiol.\",\n   Year=\"2015\",\n   Volume=\"309\",\n   Number=\"7\",\n   Pages=\"R767--779\",\n   Month=\"Oct\"}\nThe part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form\nBlood flow-restricted training leads to similar adaptations as traditional training [@refID1].\nThis will appear in text as:\n\nBlood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015).\n\nThe reference will end up in the end of the document (as on this webpage).\nYou can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with “scandinavian” letters (å æ ä ø ö).\nRecently RStudio added support for adding citations inside the visual markdown editor.\n\n\n\n\nEllefsen, S., D. Hammarstrom, T. A. Strand, E. Zacharoff, J. E. Whist, I. Rauk, H. Nygaard, et al. 2015. “Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training.” Am. J. Physiol. Regul. Integr. Comp. Physiol. 309 (7): R767–779.\n\n\nPeng, R. D., F. Dominici, and S. L. Zeger. 2006. “Reproducible Epidemiologic Research.” Journal Article. Am J Epidemiol 163 (9): 783–89. https://doi.org/10.1093/aje/kwj093.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "03-intro-to-r.html#footnotes",
    "href": "03-intro-to-r.html#footnotes",
    "title": "4  Getting to know R and RStudio",
    "section": "",
    "text": "Cheat sheets are available in R Studio: Help &gt; Cheatsheets↩︎"
  },
  {
    "objectID": "03-intro-to-r.html#reproducible-data-science-using-rstudio",
    "href": "03-intro-to-r.html#reproducible-data-science-using-rstudio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.2 Reproducible data science using RStudio",
    "text": "4.2 Reproducible data science using RStudio\nWhen starting to work more systematically in RStudio we will set some rules that will allow for reproducible programming. Remember from Chapter 2 that part of a fully reproducible study is software/code that produces the results. It turns out that when working interactively with R you can fool yourself to belive that you have included all steps needed to produce some results in your script. However, variables may be stored in your environment but not by assigning values to them in your script. This will become a problem if you want to share your code, a certain value/variable needed to make the program work may be missing from your script.\nTo avoid making such a mistake it is good practice not to save variables in your environment between sessions, everything should be scripted and documented and assumed not defined elsewhere. In RStudio we can make an explicit setting Not to save the workspace (See Figure 4.3).\n\n\n\nFigure 4.3: Never save the workspace."
  },
  {
    "objectID": "03-intro-to-r.html#the-anatomy-of-rstudio",
    "href": "03-intro-to-r.html#the-anatomy-of-rstudio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.1 The Anatomy of RStudio",
    "text": "4.1 The Anatomy of RStudio\nThe appearance of RStudio can be changed for a more pleasant user experience. I like a dark theme as it is easier on the eye. We can also move the different components of RStudio. I like to have The console on the top right and the source on the top left. I think this makes it easier to see output when coding interactively.\nAll this will be clearer as thing evolve, but for now, start R Studio, go to Tools &gt; Global options and make it personal (see Figure 4.1)!\n\n\n\nFigure 4.1: Customize the appearance of RStudio\n\n\nAs you may have spotted in the image above, it is possible to change the font of your editor. I like Fira code.\n\n\n\n\n\n\nDefining concepts\n\n\n\nSource editor: Is where scripts are edited.\nEnvironment: In R, the environment is where data variables and structures are saved during execution of code.\nScript: Your script is the document containing your computer code. This is your computer program (using a loose definition of a software program).\nVariables: In R, variables are containers for data values.\nWorkspace: This is your environments as represented on your computer. A workspace can be, but should not be saved between sessions.\n\n\n\n4.1.1 The source editor\nThe source editor is where you edit your code. When writing your code in a text-file, you can call it a script, this is essentially a computer program where you tell R what to do. It is executed from top to bottom. You can send one line of code, multiple lines or whole sections into R. In the image below (Figure 4.2), the source window is in the top left corner.\n\n\n4.1.2 Environment\nThe environment is where all your objects are located. Objects can be variables or data sets that you are working with. In RStudio the environment is listed under the environment tab (bottom left in the image).\nCopy the code below to a R script. To run it line by line, set your cursor on the first line a press Ctrl+Enter.What happened in your environment? Press Ctrl+Enter again and you will see a plot in the plot window. Amazing stuff!\n\na &lt;- c(1, 2, 3, 4)\n\nplot(a)\n\n\n\n4.1.3 The console\nBy pressing Ctrl+Enter from the script, as described above, you sent your code to the console. You can also interact with R directly here. By writing a in the console and hitting enter you will get the value from the object called a. This means that it is also where output from R is usually printed. In the image below, the console is in the top right corner.\n\n\n4.1.4 Files, plots, packages and help files\nIn RStudio files are accessible from the Files tab. The files tab shows the files in you root folder. The root folder is where R will search for files if you tell it to. We will talk more about the root folder later in connection with projects. Plots are displayed in the Plot tab. Packages are listed in the packages tab. If you access the help files, these will be displayed in the help tab. In the image below all these tabs are in the bottom right corner. More on help files and packages later.\n\n\n\nFigure 4.2: Interacting with RStudio"
  },
  {
    "objectID": "03-intro-to-r.html#file-formats-for-editing-r-code",
    "href": "03-intro-to-r.html#file-formats-for-editing-r-code",
    "title": "4  Getting to know R and RStudio",
    "section": "4.3 File formats for editing R code",
    "text": "4.3 File formats for editing R code\nRStudio has capabilities to highlight code for multiple languages. We will focus on R. The most basic file format for R code is an R script, as we have already touched upon. An R script contains code and comments. Code is executed by R and comments are ignored. Ideally, R scripts are commented to improve readability of what the do. Commenting code is also a good way of creating a roadmap of what you want to do. In the image below (Figure 4.4), R code is written based on a plan written with comments. Note that when a line starts with at least one # it is interpreted by R as a comment.\n\n\n\nFigure 4.4: Commenting and coding in an R script"
  },
  {
    "objectID": "03-intro-to-r.html#file-formats-for-editing-and-executiong-r-code",
    "href": "03-intro-to-r.html#file-formats-for-editing-and-executiong-r-code",
    "title": "4  Getting to know R and RStudio",
    "section": "4.4 File formats for editing and executiong R code",
    "text": "4.4 File formats for editing and executiong R code\n\n4.4.1 R scripts\nRStudio has capabilities to highlight code for multiple languages. We will focus on R. The most basic file format for R code is an R script, as we have already touched upon. An R script contains code and comments. Code is executed by R and comments are ignored. Ideally, R scripts are commented to improve readability of what the do. Commenting code is also a good way of creating a roadmap of what you want to do. In the image below (Figure 4.4), R code is written based on a plan written with comments. Note that when a line starts with at least one # it is interpreted by R as a comment.\n\n\n\nFigure 4.4: Commenting and coding in an R script\n\n\nTry the code for yourself to see what it produces. The details will be covered later.\n\n## Create two vectors of random numbers\nx &lt;- rnorm(10, 0, 1)\ny &lt;- rnorm(10, 10, 10)\n\n## Create an x-y plot of the two vectors\nplot(x, y)\n\n\n\n4.4.2 R markdown and quarto files\nThe more advanced file formats for R are RMarkdown (.rmd) and quarto (.qmd) files. These have the capabilities of combining formatted text with computer code. The source document may contain multiple pieces of code organized in code chunks together with text formatted with markdown syntax. A meta data field in the top of the source file specifies settings for the conversion to output formats. Multiple output formats are available, including HTML, word and PDF. The image below shows the basic outline of a very simple quarto file destined to create a HTML document.\nNotice also that RStudio offers an visual editor where the output is approximated and formatting is available from a menu.\nAdding headlines and makes it possible to navigate the document through the outline or the list of components in the bottom of the document.\n\n\n\nAuthoring in a quarto source document and preview in the visual editor\n\n\nR markdown and quarto have many similarities as the basic organization is similar between the two. The text parts are written using a special syntax, markdown. The point of markdown is that you will use the same syntax that is later possible to convert to multiple formats. The syntax let’s you do all formatting explicitly, for example instead of getting your mouse to superscript some text you can add syntax a^2^ to achieve a2.\nA full guide to RMarkdown can be found on the official R markdown web pages. I suggest you take the time to get an overview of this language as it will make you more fluent in the tools that enables reproducible computing. When writing R markdown, it is handy to have a cheat sheet close by when writing, here is an example for Rmarkdown, and here is another one for quarto 1.\n\n4.4.2.1 Microsoft Word intergration in R Markdown and Quarto\nSometimes it is useful to “knit” to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown/Quarto can be used as a source for word documents (.docx).\nTo create a word document from your Rmd-file/qmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: word_document\n---\nThe output: word_document (or format: docx when using quarto) tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use this as reference. See here for the equivalent formatting of quarto documents\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\n---\nEdit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the “Title” style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it.\nWhen you knit the document again, your updated styles will be used your word document.\nHere you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here.\n\n\n4.4.2.2 Adding references to R Markdown and Quarto files\nReferences/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are availiable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib.\nThe bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\nbibliography: bibliography.bib\n---\nAdd citations to the file in bibtex-format. Here is an example:\n@Article{refID1,\n   Author=\"Ellefsen, S.  and Hammarstrom, D.  and Strand, T. A.  and Zacharoff, E.  and Whist, J. E.  and Rauk, I.  and Nygaard, H.  and Vegge, G.  and Hanestadhaugen, M.  and Wernbom, M.  and Cumming, K. T.  and Rønning, R.  and Raastad, T.  and Rønnestad, B. R. \",\n   Title=\"{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}\",\n   Journal=\"Am. J. Physiol. Regul. Integr. Comp. Physiol.\",\n   Year=\"2015\",\n   Volume=\"309\",\n   Number=\"7\",\n   Pages=\"R767--779\",\n   Month=\"Oct\"}\nThe part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form\nBlood flow-restricted training leads to similar adaptations as traditional training [@refID1].\nThis will appear in text as:\n\nBlood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015).\n\nThe reference will end up in the end of the document (as on this webpage).\nYou can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with “scandinavian” letters (å æ ä ø ö).\nRecently RStudio added support for adding citations inside the visual markdown editor."
  },
  {
    "objectID": "03-intro-to-r.html#r-scripts-1",
    "href": "03-intro-to-r.html#r-scripts-1",
    "title": "4  Getting to know R and RStudio",
    "section": "4.6 R scripts",
    "text": "4.6 R scripts\nAs pointed out elsewhere (Wickham and Grolemund 2017; Peng, Dominici, and Zeger 2006), programming is an important part of a reproducible data analysis. This lets you build your analysis, go back a change components of it an re-run it with any number of changes. In this process you will probably learn more about your data. Putting all these steps in a program lets you save the whole analytic process. In R you may start by working with R scripts. These are basically text files that are written with a special syntax that can be interpreted by your computer. Additionally you have the possibility to add comments that makes the code more readable. R code is generally easy to read. But you will likely need additional comments to make it easier to show what you intend to do.\nA R script can be thought of as a computer program that when executed from top to bottom perform as series of steps in the order that they appear in the file. A feature of a well working program is that it is self-contained, i.e. it contains all parts needed to run. If you need to load a package or data, make sure that these steps are in the beginning of the script.\nA nice feature of combining code and comments can be that you first write a plan in plain language and then add computer code to perform the steps that you want to do. Below is a simple example. Comments start with a #, this is interpreted by R as non-code line and will be ignored. The R code is structured in series, the first steps are needed to perform sequential steps. As mentioned, the work-flow of creating the example below would be to first make a plan by writing the comments and the adding the code.\n\n# Create a data set in a data.frame\ndf &lt;- data.frame(x = rnorm(100, 100, 10), \n                 y = runif(100, min = 10, max = 25)) \n\n# Add column x and y together in a new variable called z\ndf$z &lt;- df$x + df$y\n\n\n# Make a figure of the resulting data frame by plotting x against z\nwith(df, plot(x, z))\n\nWhen working in RStudio, you can run a bit of the code by selecting it and pressing CTRL + ENTER (CMD + ENTER on Mac). You can also execute a line simply by having your cursor on a specific line and press CTRL + ENTER. Code execution means that the specific part of the script (line or section) is “copied” to the console and “activated”.\nR Scripts can also be “sourced”. This means that the whole script will be executed from to to bottom when you tell R to do so. Lets say that you have a script that creates a figure and saves it, called figure1.R. By using the source() function you can tell R to execute the script (source(\"figure1.R\")). As you can see in this example the filename extension .R tells you that a file can be intepreted as a R script."
  },
  {
    "objectID": "03-intro-to-r.html#microsoft-word-intergration-in-r-markdown-and-quarto",
    "href": "03-intro-to-r.html#microsoft-word-intergration-in-r-markdown-and-quarto",
    "title": "4  Getting to know R and RStudio",
    "section": "4.6 Microsoft Word intergration in R Markdown and Quarto",
    "text": "4.6 Microsoft Word intergration in R Markdown and Quarto\nSometimes it is useful to “knit” to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown/Quarto can be used as a source for word documents (.docx).\nTo create a word document from your Rmd-file/qmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: word_document\n---\nThe output: word_document (or format: docx when using quarto) tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use this as reference. See here for the equivalent formatting of quarto documents\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\n---\nEdit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the “Title” style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it.\nWhen you knit the document again, your updated styles will be used your word document.\nHere you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here.\n\n4.6.1 Adding references\nReferences/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are availiable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib.\nThe bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\nbibliography: bibliography.bib\n---\nAdd citations to the file in bibtex-format. Here is an example:\n@Article{refID1,\n   Author=\"Ellefsen, S.  and Hammarstrom, D.  and Strand, T. A.  and Zacharoff, E.  and Whist, J. E.  and Rauk, I.  and Nygaard, H.  and Vegge, G.  and Hanestadhaugen, M.  and Wernbom, M.  and Cumming, K. T.  and Rønning, R.  and Raastad, T.  and Rønnestad, B. R. \",\n   Title=\"{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}\",\n   Journal=\"Am. J. Physiol. Regul. Integr. Comp. Physiol.\",\n   Year=\"2015\",\n   Volume=\"309\",\n   Number=\"7\",\n   Pages=\"R767--779\",\n   Month=\"Oct\"}\nThe part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form\nBlood flow-restricted training leads to similar adaptations as traditional training [@refID1].\nThis will appear in text as:\n\nBlood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015).\n\nThe reference will end up in the end of the document (as on this webpage).\nYou can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with “scandinavian” letters (å æ ä ø ö).\nRecently RStudio added support for adding citations inside the visual markdown editor."
  },
  {
    "objectID": "03-intro-to-r.html#basics-r-programming-installing-and-using-swirl",
    "href": "03-intro-to-r.html#basics-r-programming-installing-and-using-swirl",
    "title": "4  Getting to know R and RStudio",
    "section": "4.3 Basics R programming, Installing and using swirl",
    "text": "4.3 Basics R programming, Installing and using swirl\nSwirl is a great way to get to know how to talk with R. Swirl consists of lessons created for different topics. Install swirl by typing the following into your console:\n\ninstall.packages(\"swirl\")\n\nWhen swirlis installed you will need to load the package This means that all functions that are included in package becomes available to you in your R session. To load the package you use the library function.\n\nlibrary(\"swirl\")\n\nWhen you run the above command in your console you will get a message saying to call swirl() when you are ready to learn. I would like you to run the course “R Programming: The basics of programming in R”. Swirl will ask if you want to install it. After installation, just follow the instructions in the console. To get out of swirl, just press ESC."
  },
  {
    "objectID": "03-intro-to-r.html#references-and-footnotes",
    "href": "03-intro-to-r.html#references-and-footnotes",
    "title": "4  Getting to know R and RStudio",
    "section": "4.6 References and footnotes",
    "text": "4.6 References and footnotes\n\n\n\n\nEllefsen, S., D. Hammarstrom, T. A. Strand, E. Zacharoff, J. E. Whist, I. Rauk, H. Nygaard, et al. 2015. “Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training.” Am. J. Physiol. Regul. Integr. Comp. Physiol. 309 (7): R767–779."
  },
  {
    "objectID": "04-first-graph.html#resources",
    "href": "04-first-graph.html#resources",
    "title": "5  Creating your first graph",
    "section": "5.1 Resources",
    "text": "5.1 Resources\nThere are several good resources aimed at ggplot2:\n\nChapter 2 in R for data science\nThe ggplot2 book\nThe ggplot2 cheatsheet"
  },
  {
    "objectID": "04-first-graph.html#learning-objectives",
    "href": "04-first-graph.html#learning-objectives",
    "title": "5  Creating your first graph",
    "section": "5.2 Learning objectives",
    "text": "5.2 Learning objectives\nAfter working through this chapter, you should be able to answer:\n\nWhat are geoms?\nWhat is mapping data to aesthetics?\nWhat are theme components?\n\nYou should also be able to create your first graph."
  },
  {
    "objectID": "04-first-graph.html#prerequisites",
    "href": "04-first-graph.html#prerequisites",
    "title": "5  Creating your first graph",
    "section": "5.3 Prerequisites",
    "text": "5.3 Prerequisites\nTo follow the exercises below you will need to some data. For the purpose of this course, I have created a package that contains the data sets we need. In this chapter we will work with the cyclingstudy data set. To install the package (exscidata) you will need another package called remotes.\nThe code below first checks if the package remotes is installed, or more specifically, if \"remotes\" cannot be found in the list of installed packages. Using the if function makes install.packages(remotes) conditional. If we do not find \"remotes\" among installed packages, then install remotes.\nThe next line of code does the same with the exscidata package. However, since the package is not on CRAN but hosted on GitHub we will need to use remotes to install it. The part of the second line of code that says remotes::install_github(\"dhammarstrom/exscidata\") uses the function install_github without loading the remotes package. The last line of the code below loads the package exscidata using the library function.\n\n# Check if remotes is not installed, if TRUE, install remotes\nif (!\"remotes\" %in% installed.packages()) install.packages(remotes)\n\n# Check if exscidata is not installed, if TRUE, install exscidata from github\nif (!\"exscidata\" %in% installed.packages()) remotes::install_github(\"dhammarstrom/exscidata\")\n\n# Load exscidata\nlibrary(exscidata)\n\nNext we need to load the tidyverse package. This package in turn loads several packages that we will use when transforming data and making our figures. I will include the line of code that checks if the package is installed, if not, R will download and install it. We subsequently load the package using library.\n\n# Check if tidyverse is not installed, if TRUE, install remotes\nif (!\"tidyverse\" %in% installed.packages()) install.packages(tidyverse)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe are now ready to explore the data set. But first we should talk about the main components of the ggplot2 system."
  },
  {
    "objectID": "04-first-graph.html#the-ggplot2-system",
    "href": "04-first-graph.html#the-ggplot2-system",
    "title": "5  Creating your first graph",
    "section": "5.4 The ggplot2 system",
    "text": "5.4 The ggplot2 system\nWhen using the ggplot2 system we can think of the resulting graph as containing data that has been mapped to different coordinates, colors, shapes, sizes, and other attributes that determine what is being visualized. We are using different geometric representations of the data in the visualization.\nWhen we map data in ggplot we use a specific function, aes() (short for aesthetic). We will use this inside the main engine, ggplot(). For this first simple example, we will create a data set by simulating some data. When you simulate data in R, you can tell R what should be the starting point in the random number generator. Using set.seed(100), we can recreate the same numbers from our “number generator” later. In the example below, we use rnorm() to simulate numbers from a normal distribution. Using the arguments n = 10, mean = 0, and sd = 1, we simulate randomly picking ten numbers from a distribution with a mean of 0 and a standard deviation of 1. These numbers are stored in a data frame that is assigned to an object that we have named dat.\n\n# Set the seed for random generation of numbers\nset.seed(100)\n\n# Store data in a data frame\ndat &lt;- data.frame(x = rnorm(10, mean = 0, sd = 1), \n                  y = rnorm(10, mean = 10, sd = 2))\n\nThe data set consists of two variables. We will start the process of creating the graph by creating the canvas, and this basically sets the border of the figure we want to make. The ggplot() function takes the data set as its first argument, followed by the aes() function that maps data to coordinates and other attributes. In this case, we have mapped our data to the x- and y-coordinates of the figure.\n\nggplot(dat, aes(x = x, y = y))\n\n\n\n\nFigure 5.1: An empty ggplot canvas.\n\n\n\n\nAs you can see in Figure 5.1, the code above creates an “empty canvas” that has enough room to visualize our data. The x- and y-axes are adjusted to give room for graphical representations of the data. Next we need to add geometric shapes (geom for short). These are functions that we add to the plot using the + sign. These functions all start with geom_ and has and ending that describes the geometric shape, like for example point or line.\nWe will add geom_point() to our empty canvas as plotted in Figure 5.1. The geom_point function inherits the mapping from from ggplot(). Shapes, in this case points will be placed according to x- and y-coordinates specified in aes() used in the main ggplot function call. This means that we do not need to specify anything in geom_point at this stage.\n\nggplot(dat, aes(x = x, y = y)) + geom_point()\n\n\n\n\nFigure 5.2: A ggplot canvas with points added.\n\n\n\n\nIn Figure 5.2 we have added black points to each x- and y-coordinate representing x and y from our data set.\nTo extend the example we will add data to our data set. In the code below, we create a new variable in the data set using $ effectively giving us a new column in the data. We use rep(\"A\", 5) to replicate the letter A five times and the same for B. The c() function combines the two in a single vector. We can use head(dat) to see what we accomplished with these operations. The head() function prints the first six rows from the data set.\n\ndat$z &lt;- c(rep(\"A\", 5), rep(\"B\", 5))\n\nhead(dat)\n\n            x         y z\n1 -0.50219235 10.179772 A\n2  0.13153117 10.192549 A\n3 -0.07891709  9.596732 A\n4  0.88678481 11.479681 A\n5  0.11697127 10.246759 A\n6  0.31863009  9.941367 B\n\n\nWe can see that we have an additional variable, z that contains the letters \"A\" and \"B\". This new variable can be used to add more information to the plot. Let’s say that we want to map the z variable to different colors. We do this by adding color = z to aes. This means that we want the z variable to determine colors.\n\nggplot(dat, aes(x = x, y = y, color = z)) + geom_point()\n\n\n\n\nFigure 5.3: A ggplot canvas with colored points added.\n\n\n\n\nIn Figure 5.3 we can see that different colors are used for the two letters \"A\" and \"B\". Other attributes can also be specified like shape, fill or size. The shape specifies the appearance of the points. When we use use data to map to shapes, ggplot2 will start from the standard shape.\n\n\n\n\n\nFigure 5.4: Shapes in R\n\n\n\n\nPossible shapes in the standard framework in R are shown in Figure 5.4. Shapes 0 to 20 can change colors while shapes 21 to 25 may have different border colors but also different fill colors. We may use this information to change the shape, color and fill of our points. Let’s say that instead of colored points we want filled points. We would then change the color = z argument to fill = z and select a point shape that can be filled (shapes 21-25, see Figure 5.4. Notice in the code below that shape = 21 has been added to geom_point(). We have specified how points should be displayed.\n\nggplot(dat, aes(x = x, y = y, fill = z)) + geom_point(shape = 21)\n\n\n\n\nFigure 5.5: A ggplot canvas with filled points added.\n\n\n\n\nSince shape is an attribute we can map data to it. If we want data to determine both shape and fill we could add this information in the aes() function by setting both shape = z and fill = z. We now have to specify what shapes ggplot should use in order to be sure we can combine both shapes and fill. We will use scale_fill_manual and scale_shape_manual to do this. These functions lets you specify different values for aesthetics. Notice that we removed shape = 21 from the geom_point() function, but we added size to increase the size of the points (see Figure 5.6).\n\nggplot(dat, aes(x = x, y = y, fill = z, shape = z)) + \n  geom_point(size = 3) +\n  scale_fill_manual(values = c(\"red\", \"green\")) + \n  scale_shape_manual(values = c(21, 23))\n\n\n\n\nFigure 5.6: Data mapped to fill and shape, and size specified manually to override the default."
  },
  {
    "objectID": "04-first-graph.html#different-geoms-using-real-data",
    "href": "04-first-graph.html#different-geoms-using-real-data",
    "title": "5  Creating your first graph",
    "section": "5.5 Different geoms using real data",
    "text": "5.5 Different geoms using real data\nWe have seen that the basic ggplot2 figure maps underlying data to coordinates and geometric representations, such as points. We will go further by using some real data. We will be using the cyclingstudy data set from the exscidata-package. We will start by loading the data and select a few columns that we are interested in.\nBy using data(\"cyclingstudy\") we will load the data set that is part of the exscidata-package to our environment. By looking at the environment tab you can see that this operation adds a data set to the environment. It has 80 observations and 101 variables. Using the glimpse() function from dplyr (which is loaded by loading tidyverse) we will get an overview of all variables in the data set. I have omitted the output from the code below, feel free to run the code in a quarto- or rmarkdown-document on your own.\n\n# Load the data and have a first look\ndata(\"cyclingstudy\")\nglimpse(cyclingstudy)\n\nWe will store a selected set of variables in a new object for ease of use. We will call this object cycdat. We select variables using the function with the very suitable name select where the first argument specifies the data set, following arguments specifies what variables we want. Let’s say that we are interested in squat jump height. The exscidata package comes with descriptions of the data sets. By writing ?cyclingstudy in your console you will see the description of the data in your help tab. Squat jump is recorded as sj.max, we select this variable together with subject, group and timepoint to create a smaller data set.\n\n# Assign a selected set of variables to a smaller data set\ncycdat &lt;- select(cyclingstudy, subject, group, timepoint, sj.max)\n# Printing the data set\ncycdat\n\n# A tibble: 80 × 4\n   subject group timepoint sj.max\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1       1 INCR  pre         31.0\n 2       2 DECR  pre         31.6\n 3       3 INCR  pre         26.8\n 4       4 DECR  pre         29.2\n 5       5 DECR  pre         31.2\n 6       6 INCR  pre         34.2\n 7       7 MIX   pre         30.1\n 8       8 MIX   pre         32.8\n 9       9 MIX   pre         22.7\n10      10 INCR  pre         29.7\n# ℹ 70 more rows\n\n\nBy printing the object we can see that we have a tibble of 80 rows and 4 columns. A tibble can to a large extent be regarded as a data frame, and we will use these words interchangeably. Tibbles are new in the sense that they are developed as part of the tidyverse (Wickham and Grolemund 2017) 1. Printing a tibble will display the first 10 rows as we can see from the resulting output.\n\n5.5.1 A plot of values per group\nLet’s say that we want to see how the values differs between groups. Box-plots are a good way to start as they will bring a standardized way of summarizing data. Box-plots can be plotted using the geom_boxplot function. Notice below that we put group on the x-axis (the first argument in the aes function) and sj.max on the y-axis. By doing so ggplot will make the x-axis discrete and the y-axis continuous.\n\n# Creating a box-plot of all values per group\nggplot(cycdat, aes(group, sj.max)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.7: Boxplot of all data per group from the cycling dataset.\n\n\n\n\nWe can add layers of more geoms to the same plot. We might want to add individual data points also. geom_jitter might be a good place to start. This geom is good as it can be plotted over a group variable and points gets “jittered” or spread so we avoid overlap.\n\n# Creating a boxplot of all values per group\nggplot(cycdat, aes(group, sj.max)) + geom_boxplot() + geom_jitter()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5.8: Box-plot and jittered points of all data per group from the cycling dataset.\n\n\n\n\nNotice that we get warnings saying that there are some data missing, these values are removed from the calculation of summary statistics in the box-plots and omitted from plotting of the points.\n\n\n5.5.2 Data over time per group and individual\nIn the data set we have a time variable consisting of the labels “pre”, “meso1”, “meso2” and “meso3”. When we load the data into R we do so without providing information about the order of these labels. R will put them in alphabetical order when order is required (as in a figure). If we want to plot these data in the right order, we have to tell R that these data should have an order. We will convert the timepoint variable to a factor. Factors are variables that can contain more information than what is contained in each cell. Using the factor function we will set the order of the timepoint variable. We assign this transformation of the variable to its original place in the data frame.\n\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"))\n\nWe are now ready to plot data over time, where the time variable is correctly ordered. Let’s use the box-plot again to plot all values over time.\n\n# Creating a boxplot of all values per time point\nggplot(cycdat, aes(timepoint, sj.max)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.9: Boxplot of all data per time-point from the cycling dataset.\n\n\n\n\nWe do not see any great tendencies in the whole data set. To further explore the data we might want to have different boxes per group per time. We can accomplish this by adding fill = group to our aes function.\n\n# Creating a boxplot of all values per group over time\nggplot(cycdat, aes(timepoint, sj.max, fill = group)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.10: Boxplot of all data per time-point and group from the cycling dataset.\n\n\n\n\nThis is possible because geom_boxplots can be filled. The same separation of groups would have been accomplished using color = group, however, then the boxes would get different border colors instead. You might have noticed that the box-plots do not contain all the data, a few data points are outside \\(1.5 \\times IQR\\) (interquartile range). This, by standard definitions, defines the data point as an “outlier”.\nAs mentioned above, box-plots does some summarizing and not all data is shown. To explore further we might want to track every participant. To do this we have to tell ggplot how to group the data. In aes() the group argument let’s you connect lines based on some grouping variable, in our case it will be subject. We will use a line to connect each participants score over time. Using color = group will additionally give every line a different color depending on which group it belongs to.\n\n# Creating a line plot of all values per participant over time, color per group\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \ngeom_line()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.11: Figure with lines corresponding to indivudal values per participant.\n\n\n\n\nIn Figure 5.11, each line represents a participant, different colors represents different groups.\n\n\n5.5.3 Titles and labels\nOften we need to add information to the plot to better communicate its message. Such information could be appropriate titles on axes and legends and extra text needed to explain aspects of the plot. Using the labs() function we can add information that will replace variable names that are being used for all variables that have been mapped in the figure. In the figure below we will start by adding better axis titles. This information goes into x and y in labs() which simply changes the titles of the x- and y-axis.\n\n# Creating a line plot of all values per participant over time, color per group, \n# adding axis labels\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.12: Figure with updated axis labels\n\n\n\n\nThe resulting Figure 5.12 now have better titles for each axis. Notice in the code above that titles needs to be specified with quotation marks. This is a tricky aspect of R, if we would have omitted the quotation marks we would have told R to look for objects by the name of e.g. Time-point, and this would actually mean that we tried to subtract time from point since - is interpreted as a minus sign.\nWe might want to add information to the legend also. Since we specified color = group in the aes() function, the same can be manipulated in labs. Lets just add a capital G.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.13: Additional labels\n\n\n\n\nWe still have the original labels for the time variable. Remember that we used the factor function above to set the order of the labels. Actually we specified the “levels” of the factor. We can use the same function to add better “labels”. In the code below, I will first change the variable in the data set and then use the exact same code for the plot.\n\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"), \n                           labels = c(\"Pre-training\", \"Meso-cycle 1\", \"Meso-cycle 2\", \"Meso-cycle 3\"))\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.14: Changing labels by changing a factor variable prior to plotting\n\n\n\n\nThe same goes for the group variable. You can try to change the levels and labels of the grouping variable to make it more descriptive. You can type ?cyclingstudy in your console to read about the group variable and then use this information to write better labels using the factor function. In the factor function, the first argument is the variable you want to use as basis of your new factor, the second argument you need to specify is levels which sets the order and lastly you will need to set the labels for each level using labels =. If you write ?factor in your console you will get the help pages for the factor function.\nClick here to display a possible solution\n\n\n# Change the grouping variable\ncycdat$group &lt;- factor(cycdat$group, levels = c(\"DECR\", \"INCR\", \"MIX\"), \n                           labels = c(\"Decreased\\nintensity\", \n                                      \"Increased\\nintensity\", \n                                      \"Mixed\\nintensity\"))\n\n# Plotting the data as before with the new information added\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\")\n\nNote: Adding \\n in the the text string breaks the line to get two rows.\n\n\n\n5.5.4 Annotations\nAnnotation may become handy when you want to add elements to the graph that is not in the data set. Using ggplot2, annotations are added using the annotate() function. This function first needs to be specified with a geom, these are commonly text or lines or segments. In the code chunk below are several examples of annotations. First I save the plot as an object called myplot and then add different annotations to it.\n\nmyplot &lt;- ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\") \n\n\n# A text annotation\nmyplot + annotate(\"text\", x = 1, y = 37, label = \"This is an annotation\")\n\n# A line/segment \nmyplot + annotate(\"segment\", x = 1, xend = 3, y = 25, yend = 35,  colour = \"red\", size = 4)\n\nYou can copy the code and run it yourself to see the results. annotate is documented here but documentation can also be accessed by typing ?annotate in your console. Try to read the documentation and add a transparent rectangle to a previous plot.\nClick here for a solution\n\n\n# Change the grouping variable\ncycdat$group &lt;- factor(cycdat$group, levels = c(\"DECR\", \"INCR\", \"MIX\"), \n                           labels = c(\"Decreased\\nintensity\", \n                                      \"Increased\\nintensity\", \n                                      \"Mixed\\nintensity\"))\n\n# Plotting the data as before with the new information added\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\") +\n  # A rectangular annotation (alpha = 0.4 makes the rectangle transparent)\n annotate(\"rect\", xmin = 1, xmax = 2, ymin = 30, ymax = 35, alpha = 0.4)\n\nNote: Adding \\n in the the text string breaks the line to get two rows."
  },
  {
    "objectID": "04-first-graph.html#themes",
    "href": "04-first-graph.html#themes",
    "title": "5  Creating your first graph",
    "section": "5.6 Themes",
    "text": "5.6 Themes\nThemes in ggplot2 can be used to change everything else about the plot concerning text, colors etc. ggplot2 has some built in themes that are easily activated by adding them to the plot. For example the theme_bw() function will change the theme to a black and white one as in the figure below.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\") + \n  theme_bw() # Adding a pre-specified theme\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.15: A figure using the black and white theme from theme_bw.\n\n\n\n\nA collection of built in themes are documented here. Individual components of the theme can also be changed using the theme() function. There is a long list of theme components that can be changed using this function. The list can be found here.\nIf we put the theme function last in the ggplot call we will modify the existing theme. Let’s say that we want to change the color of the text on the x axis.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\") + \n  theme_bw() +\n  theme(axis.text.x = element_text(color = \"steelblue\", size = 12, face = \"bold\"))\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.16: A figure using the black and white theme from theme_bw, with modifications\n\n\n\n\nThe component axis.text.x can be modified using a function that changes appearance of text components, namely element_text. Similarly, other components are changed with specific functions for lines and rectangular shapes (see the help pages for theme)."
  },
  {
    "objectID": "04-first-graph.html#test-your-understandning",
    "href": "04-first-graph.html#test-your-understandning",
    "title": "5  Creating your first graph",
    "section": "5.7 Test your understandning",
    "text": "5.7 Test your understandning\nIn this section you can try to implement what we have discussed above. An example solution exists below each figure by press of button.\nIn Figure 5.17, I have used the VO2max data from the cyclingstudy data set. I have made changes to the time variable (timepoint) to make the labels better. I have added a title to the figure and changed the appearance of the text. I will use an extra package called (ggtext)[https://wilkelab.org/ggtext/index.html] to make it possible to use markdown syntax in axis labels. In order to use ggtext you have to install it from CRAN.\n\n\n\n\n\nFigure 5.17: Example figure 1\n\n\n\n\nClick for a solution\n\n\n# Load the package ggtext to make markdown avalable in axis labels.\nlibrary(ggtext) \n\n# For ease of use I save a smaller dataset in a new object\ncycdat &lt;- select(cyclingstudy, subject, timepoint, VO2.max)\n\n# Change the labels of the time variable\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"), \n                           labels = c(\"Pre-training\", \"Meso-cycle 1\", \"Meso-cycle 2\", \"Meso-cycle 3\"))\n\n\n# create the basic plot\n\nggplot(data = cycdat, aes(timepoint, VO2.max, group = subject)) + \n  # Add lines to connect dots. Putting the lines first and plotting points on top\n  geom_line() + \n  # Add points foe each participant/time\n  geom_point(size = 3, fill = \"lightblue\", shape = 21) + \n\n  # Adding correct axis titles and a figure title\n  labs(x = \"Time-point\", \n       y = \"VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt; -1&lt;/sup&gt;)\", \n       title = \"Maximal aerobic power in response to systematic training in trained cyclists\") +\n  \n  # Changing the text rendering using element_markdown from the ggtext package.\n  theme(axis.title.y = element_markdown(size = 12)) \n\nNote: Adding \\n in the the text string breaks the line to get two rows."
  },
  {
    "objectID": "04-first-graph.html#footnotes",
    "href": "04-first-graph.html#footnotes",
    "title": "5  Creating your first graph",
    "section": "",
    "text": "See Chapter 10 in R for data science (2 edition)↩︎"
  },
  {
    "objectID": "05-first-tables.html#introduction",
    "href": "05-first-tables.html#introduction",
    "title": "6  Wrangling data to create your first table",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nWe can use tables to communicate a lot of information in a compact form while maintaining precision. This advantage is why creating tables is essential for effectively communicating data. We can easily create tables programmatically as part of an R markdown or quarto document. R has many “table generator” packages that translate your draft table to an output format of your choice. A great format to start authoring an analysis in is HTML; however, most “table generators” need to know your output format to be properly formatted and work in the output format. Below we will introduce a new package for the purpose of creating tables. This package, gt has the advantage that it does not require the user to change code when we switch to another output format. The gt package can create tables in HTML, PDF and word format.\nSince we are concerned with reproducibility, we would like to avoid copy-and-paste operations. The strength of writing reports in R markdown or quarto is the ability to combine data, code, and text to produce a formatted output programmatically. Therefore, We will choose a table generator that allows for consistently selecting multiple formats. One such table generator is part of the gt package.\nAs mentioned previously, authoring in R markdown and quarto makes little difference. However, we will now focus on the more modern file format, quarto, knowing that examples and tutorials written in R markdown will translate to quarto with few problems.\nThe basic workflow of creating a table in R markdown or quarto is to transform the data into a nice format and then get this underlying data into the table generator. The table generator is written in a code chunk, and upon rendering the source file, the table generator will create, for example, HTML output. In this chapter, we will introduce some data-wrangling tools since the table we will produce consists of summarized data. The functions we introduce are found in the packages dplyr and tidyr. These packages are loaded as part of the tidyverse package.\n\n6.1.1 Resources\nAll tidyversepackages are well documented and generally well represented in help forums. Google is your friend when looking for help.\nThe gt package is now a mature package for generating tables in R. This chapter is written on the basis of this package. If you are looking for alternatives, the kable function from the knitr package is described in a newly developed book, available online called the R Markdown Cookbook. The package, kableExtra comes with excellent vignettes for both html and pdf outputs. kableExtra provides extra functions to customize your basic knitr table. Note that kabale and kableExtra will only produce output in HTML and pdf-formats. Another package the can create tables in HTML, pdf, and word formats is the flextable package"
  },
  {
    "objectID": "05-first-tables.html#making-table-1",
    "href": "05-first-tables.html#making-table-1",
    "title": "6  Wrangling data to create your first table",
    "section": "6.2 Making “Table 1”",
    "text": "6.2 Making “Table 1”\nThe first table in many reports in sport and exercise studies is the “Participant characteristics” table. This first table summarizes background information on the participants. We will try to create this table based on data from (Hammarström et al. 2020). These data can be found in the exscidata package. To load the data and other required packages run the following code.\n\nlibrary(tidyverse) # for data wrangling\nlibrary(gt) # for creating tables\nlibrary(exscidata) # the dxadata\n\nThe end result of this exercise can be found below in Table 6.1. This summary table contains the average and standard deviation per group for the variables age, body mass and stature (height) and body fat as a percentage of the body mass. This table is a reproduction of the first part of Table 1 from (Hammarström et al. 2020).\n\n\n\n\n\n\nTable 6.1:  Participant characteristics \n  \n    \n    \n       \n      \n        Female\n      \n      \n        Male\n      \n    \n    \n      Included\n      Excluded\n      Included\n      Excluded\n    \n  \n  \n    N\n18\n4\n16\n3\n    Age (years)\n22 (1.3)\n22.9 (1.6)\n23.6 (4.1)\n24.3 (1.5)\n    Mass (kg)\n64.4 (10)\n64.6 (9.7)\n75.8 (11)\n88.2 (22)\n    Stature (cm)\n168 (6.9)\n166 (7.6)\n183 (5.9)\n189 (4.6)\n    Body fat (%)\n34.1 (5.6)\n28.8 (8.7)\n20.4 (6)\n24.3 (15)\n  \n  \n  \n    \n       Values are mean and (SD)\n    \n  \n\n\n\n\n\nWe have to make several operations to re-create this table. First we can select the columns we want to work with further from the data set that also contains a lot of other variables. Let us start by looking at the full data set. Below we use the function glmipse from the dplyr package (which is loaded with tidyverse).\n\ndata(\"dxadata\")\n\nglimpse(dxadata)\n\nRows: 80\nColumns: 59\n$ participant      &lt;chr&gt; \"FP28\", \"FP40\", \"FP21\", \"FP34\", \"FP23\", \"FP26\", \"FP36…\n$ time             &lt;chr&gt; \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre…\n$ multiple         &lt;chr&gt; \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"L\", \"L\"…\n$ single           &lt;chr&gt; \"R\", \"L\", \"L\", \"L\", \"L\", \"L\", \"R\", \"L\", \"L\", \"R\", \"R\"…\n$ sex              &lt;chr&gt; \"female\", \"female\", \"male\", \"female\", \"male\", \"female…\n$ include          &lt;chr&gt; \"incl\", \"incl\", \"incl\", \"incl\", \"incl\", \"excl\", \"incl…\n$ age              &lt;dbl&gt; 24.5, 22.1, 26.8, 23.1, 24.8, 24.2, 20.5, 20.6, 37.4,…\n$ height           &lt;dbl&gt; 170.0, 175.0, 184.0, 164.0, 176.5, 163.0, 158.0, 181.…\n$ weight           &lt;dbl&gt; 66.5, 64.0, 85.0, 53.0, 68.5, 56.0, 60.5, 83.5, 65.0,…\n$ BMD.head         &lt;dbl&gt; 2.477, 1.916, 2.306, 2.163, 2.108, 2.866, 1.849, 2.21…\n$ BMD.arms         &lt;dbl&gt; 0.952, 0.815, 0.980, 0.876, 0.917, 0.973, 0.871, 0.91…\n$ BMD.legs         &lt;dbl&gt; 1.430, 1.218, 1.598, 1.256, 1.402, 1.488, 1.372, 1.42…\n$ BMD.body         &lt;dbl&gt; 1.044, 0.860, 1.060, 0.842, 0.925, 0.984, 0.923, 1.01…\n$ BMD.ribs         &lt;dbl&gt; 0.770, 0.630, 0.765, 0.636, 0.721, 0.737, 0.648, 0.70…\n$ BMD.pelvis       &lt;dbl&gt; 1.252, 1.078, 1.314, 1.044, 1.154, 1.221, 1.194, 1.32…\n$ BMD.spine        &lt;dbl&gt; 1.316, 0.979, 1.293, 0.899, 1.047, 1.089, 1.006, 1.14…\n$ BMD.whole        &lt;dbl&gt; 1.268, 1.082, 1.325, 1.119, 1.181, 1.350, 1.166, 1.24…\n$ fat.left_arm     &lt;dbl&gt; 1168, 715, 871, 610, 788, 372, 932, 1312, 388, 668, 5…\n$ fat.left_leg     &lt;dbl&gt; 4469, 4696, 3467, 3023, 3088, 2100, 4674, 5435, 1873,…\n$ fat.left_body    &lt;dbl&gt; 6280, 4061, 7740, 3638, 6018, 2328, 4896, 9352, 2921,…\n$ fat.left_whole   &lt;dbl&gt; 12365, 9846, 12518, 7565, 10259, 5048, 10736, 16499, …\n$ fat.right_arm    &lt;dbl&gt; 1205, 769, 871, 610, 741, 374, 940, 1292, 413, 716, 5…\n$ fat.right_leg    &lt;dbl&gt; 4497, 4900, 3444, 3017, 3254, 2082, 4756, 5455, 1782,…\n$ fat.right_body   &lt;dbl&gt; 6082, 3923, 8172, 3602, 5699, 2144, 4705, 8674, 2640,…\n$ fat.right_whole  &lt;dbl&gt; 12102, 9862, 12856, 7479, 10020, 4821, 10806, 15876, …\n$ fat.arms         &lt;dbl&gt; 2373, 1484, 1742, 1220, 1529, 747, 1872, 2604, 802, 1…\n$ fat.legs         &lt;dbl&gt; 8965, 9596, 6911, 6040, 6342, 4182, 9430, 10890, 3655…\n$ fat.body         &lt;dbl&gt; 12362, 7984, 15912, 7239, 11717, 4472, 9601, 18026, 5…\n$ fat.android      &lt;dbl&gt; 1880, 963, 2460, 1203, 1933, 527, 1663, 3183, 1240, 1…\n$ fat.gynoid       &lt;dbl&gt; 5064, 5032, 4779, 3739, 4087, 2740, 5217, 6278, 2309,…\n$ fat.whole        &lt;dbl&gt; 24467, 19708, 25374, 15044, 20278, 9869, 21542, 32375…\n$ lean.left_arm    &lt;dbl&gt; 1987, 1931, 2884, 1753, 2652, 2425, 1913, 2266, 3066,…\n$ lean.left_leg    &lt;dbl&gt; 7059, 7190, 10281, 6014, 8242, 7903, 6829, 8889, 9664…\n$ lean.left_body   &lt;dbl&gt; 9516, 10693, 13847, 9736, 11387, 10573, 8954, 11482, …\n$ lean.left_whole  &lt;dbl&gt; 20305, 21778, 29332, 19143, 24185, 22946, 18809, 2431…\n$ lean.right_arm   &lt;dbl&gt; 2049, 2081, 2888, 1754, 2487, 2439, 1930, 2236, 3253,…\n$ lean.right_leg   &lt;dbl&gt; 7104, 7506, 10200, 6009, 8685, 7841, 6950, 8923, 9198…\n$ lean.right_body  &lt;dbl&gt; 9199, 10304, 14593, 9636, 10779, 9733, 8602, 10672, 1…\n$ lean.right_whole &lt;dbl&gt; 19605, 21310, 29643, 18792, 23653, 21837, 19407, 2372…\n$ lean.arms        &lt;dbl&gt; 4036, 4012, 5773, 3508, 5139, 4864, 3843, 4501, 6319,…\n$ lean.legs        &lt;dbl&gt; 14163, 14696, 20482, 12023, 16928, 15744, 13779, 1781…\n$ lean.body        &lt;dbl&gt; 18715, 20998, 28440, 19372, 22166, 20306, 17556, 2215…\n$ lean.android     &lt;dbl&gt; 2669, 2782, 3810, 2455, 2904, 2656, 2297, 3094, 3344,…\n$ lean.gynoid      &lt;dbl&gt; 6219, 7209, 10233, 5866, 7525, 5970, 5825, 8175, 7760…\n$ lean.whole       &lt;dbl&gt; 39910, 43088, 58976, 37934, 47837, 44783, 38216, 4804…\n$ BMC.left_arm     &lt;dbl&gt; 181, 138, 204, 144, 180, 173, 140, 173, 220, 226, 225…\n$ BMC.left_leg     &lt;dbl&gt; 567, 508, 728, 441, 562, 574, 482, 631, 633, 630, 672…\n$ BMC.left_body    &lt;dbl&gt; 622, 414, 696, 367, 526, 465, 370, 629, 473, 629, 509…\n$ BMC.left_whole   &lt;dbl&gt; 1680, 1321, 1945, 1201, 1527, 1580, 1131, 1688, 1544,…\n$ BMC.right_arm    &lt;dbl&gt; 198, 150, 210, 142, 176, 183, 140, 176, 224, 251, 226…\n$ BMC.right_leg    &lt;dbl&gt; 574, 514, 739, 431, 552, 565, 491, 641, 622, 636, 690…\n$ BMC.right_body   &lt;dbl&gt; 592, 428, 730, 351, 502, 409, 358, 582, 420, 616, 483…\n$ BMC.right_whole  &lt;dbl&gt; 1582, 1288, 1958, 1130, 1451, 1466, 1229, 1668, 1478,…\n$ BMC.arms         &lt;dbl&gt; 379, 288, 414, 285, 356, 357, 280, 348, 444, 478, 451…\n$ BMC.legs         &lt;dbl&gt; 1142, 1022, 1467, 872, 1115, 1139, 974, 1272, 1255, 1…\n$ BMC.body         &lt;dbl&gt; 1214, 842, 1426, 718, 1028, 874, 728, 1211, 893, 1245…\n$ BMC.android      &lt;dbl&gt; 80, 57, 90, 44, 56, 54, 43, 77, 52, 72, 59, 60, 65, 5…\n$ BMC.gynoid       &lt;dbl&gt; 314, 285, 427, 245, 299, 262, 241, 379, 335, 378, 332…\n$ BMC.whole        &lt;dbl&gt; 3261, 2609, 3903, 2331, 2978, 3046, 2360, 3356, 3022,…\n\n\nWe can see that we got 80 rows and 59 columns in the data set. The columns of interest to us are:\n\nparticipant\ntime\nsex\ninclude\nage\nheight\nweight\nfat.whole\n\nFor a full description of the data set, you can type ?dxadata in your console. The participant column is good to have to keep track of the data set in the beginning. time is needed to remove some observation that are not needed. This pre-training table only sums up information from before the intervention starts. sex is a grouping variable together with include, Table 1 in (Hammarström et al. 2020) uses Sex and Inclusion in data analysis as grouping for descriptive data. The other variables are used to describe the data sample.\n\n6.2.1 The pipe operator and select\nAs mentioned above, we will start by selecting the variables we want to work further with. Using the select function from dplyr we can select columns that we need. In the code below I will use select as part of a “pipe”. Think of the pipe as doing operations in sequel. Each time you use the pipe operator (%&gt;%) you say “then do”. The code below translates to:\n\nTake the data set dxadata, then do\nselect() the following variables, then do\nprint()\n\nprint, is a function that outputs the results of the operations. In each new function of a pipe, the data that we take with us from the above line ends up as the first argument. A representation of this behavior can be expressed as:\nDATA %&gt;%   FUNCTION(DATA, ARGUMENTS) %&gt;%   FUNCTION(DATA, ARGUMENTS) %&gt;%   FUNCTION(DATA)\nWe do not need to type the data part, instead the pipe operator (%&gt;%) gathers the data from each step and puts it in the subsequent function.\nCopy the code below to your own quarto document and run it. When using quarto you might want to set “Chunk output in console” in the settings menu. In my experience, this makes developing code a bit faster.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  print() # print the output\n\nNotice that I have added short comments after each line to make it clear what I want to accomplish. We will build further on the above code, and this is a common workflow. Using pipes, it is easy to extend the code by adding lines doing certain operations, one at the time. Notice also that the select function uses a list of variable names with include:weight being short for “take all variables from include to weight”.\n\n\n6.2.2 Filter observations\nThe next step will be to filter observations. We need to remove the observations that comes from the post-intervention tests. The time variable contains to values pre and post to remove post-values we need to tell R to remove all observations (rows) containing post. We will use the filter function from dplyr. This will be our first experience with logical operators. Let’s try out two alternatives, copy the code to your console to see the results.\n\n## Alternative 1: ##\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter away all observation with \"post\"\n  filter(time != \"post\") %&gt;%\n  \n  print() # print the output\n\n\n## Alternative 2: ##\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  print() # print the output\n\nThe above code should give the same output. The operator != says “not equal to”, the operator == says “equal to”. Notice that R uses two equal signs to say equal to. A single equal sign is used as an assignment operator in R.\n\n\n6.2.3 Create or change variables\nThe next problem for us is that we need to manipulate or combine information from two variables in order to calculate body fat percentage. The formula that we will use is simply expressing body fat as a percentage of the body weight.\n\\[\\text{Body fat (\\%)} = \\frac{\\text{Body fat (g)}/1000}{\\text{Body weight (kg)}} \\times 100\\] By using the mutate function we can add or manipulate existing variables in a pipe. Mutate takes as arguments a list of new variables:\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  print() # print the output\n\nIn the code above, we overwrite the variable fat.whole with the re-calculated variable.\n\n\n6.2.4 Grouped operations and summary statistics\nIn a pipe, we can group the data set giving us opportunities to calculate summary statistics over one or several grouping variables. In Table 1 in (Hammarström et al. 2020), include and sex are the two grouping variables. Using the group_by() function from dplyr sets the grouping of the data frame. If we use functions that summarizes data, such summaries will be per group. In Table 1 in (Hammarström et al. 2020) the number of participants in each group are specified. We can use the function n() to calculate the number of observations per group in a mutate call.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  print() # print the output\n\nThe new variable n now contains the number of observations in each group. For now we can regard this as a new variable. Each participant belongs to a specified group, and this specific group has n number of members.\nWe can now go further and use the summarise function. Instead of adding variables to the existing data set, summarize reduces the data set using some summarizing function, such as mean() or sd(). These summary statistics are what we are looking for in our data set. Example of other summarizing functions for descriptive data are min() and max() for the minimum and maximum.\nWe can use the summarise() function to calculate the mean and standard deviation for the weight variable.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Summarise weight\n  summarise(weight.m = mean(weight), \n            weight.s = sd(weight)) %&gt;%\n  \n  print() # print the output\n\nTry out the code in your own quarto document. The above example gives us what we want, however, it means that we need to type a lot. Instead of needing to make a summary for each variable, we can combine the variables in a long format. To get to the long format we will use the pivot_longer() function. This function gathers several variables into two columns, one with the variables names as values and a second column with each value from the original variables. In our case we want to gather the variables age, height, weight, fat.whole and n. I will call the new variables that we create variable and value.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  print()\n\nThe cols = age:n part of pivot_longer specifies what columns to gather. The data set is still grouped by include and sex. We may now proceed by summarizing over these groups, however, we need to add another group to specify that we want different values per variable. To do this we re-specify the grouping. After this we add the summarise function.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  print()\n\nIf you run the above code you will notice that the the standard deviation of each variable is larger than zero except for n which has no variability. This is because we created it per group and simply calculated it as the sum of observations.\nTake a look at Table 1 in (Hammarström et al. 2020). The format of the descriptive statistics are mean (SD), this is a preferred way of reporting these statistics. In order to achieve this we need to “manually” convert the numbers. In the example below, I will start by making a new variable by simply pasting the numbers together. I will also add the parentheses.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = paste0(m, \" (\", s, \")\"))\n  print()\n\nIn mutate(ms = paste0(m, \" (\", s, \")\")), the paste0 function simply glues components together to form a string of text. First, the vector of means are being used, then we add a parenthesis, followed by the SD and finally a parenthesis.\nIf you run the above code you will notice that you end up with numbers looking like this:\n167.666666666667 (6.86851298231541)\nThis is neither good or good looking. We have to take care of the decimal places. There are a number of ways to do this but in this case the function signif seems to make the situation better. signif rounds to significant digits. This means that we will get different rounding depending on the “size” of the value. I find signif(m, 3) to be a good starting point.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = paste0(signif(m, 3), # Use signif to round to significant numbers\n                     \" (\",\n                     signif(s, 3), \n                     \")\")) %&gt;%\n  print()\n\nThings are starting to look good. Run the code, what do you think. A problem with the above is that we do not want any variability in the n variable. So if the variable is n we do not want that kind of formatting. It is time to add a conditional statement. In dplyr there are easy-to-use if/else functions. The function if_else sets a condition, if this is met then we can decide what to do, and likewise decide what to do if it is not met.\nThis looks something like this inside a dplyr pipe:\n\n... %&gt;%\n  if_else(IF_THIS_IS_TRUE, THE_DO_THIS, OTHERWISE_DO_THIS) %&gt;%\n  print()\n\nIf variable is n, then we only want to display m otherwise we want the full code as described above: paste0(signif(m, 3), \" (\", signif(s, 3), \")\"). We add this to the code:\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\"))) %&gt;%\n  print()\n\nThe as.character part is needed because the output of if_else must be the same regardless of what the outcome of the test is.\nWe are getting close to something!\nThe next step is to remove variables that we do not longer need. The select function will help us with that. we can remove m and s by select(-m, -s), the minus sign tells R to remove them from the list of variables in the data set. We can then combine the grouping variables into a include_sex variable. Similarly to what we did above, we can simply paste them together. Now we will use the paste (function instead of paste0). In paste we specify a separator, maybe _ is a nice alternative. Selecting away the individual variables from the new combined one leaves us with this code and data set.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  print()\n\nIf ungroup is not used, we cannot select away variables since they are used to group the data set. We will now perform the last operations before we can make it a table. To make it formatted as in Table 1 in (Hammarström et al. 2020), we can make the present data set wider. Each group as its own column in addition to the variable name column. We will use the opposite function to pivot_longer, namely pivot_wider1. pivot_wider takes a variable or “key” column and a “values” column and divide the values based on the “key”.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n  print()\n\nA final step is to format the variable variable(!). The easiest is to make it a factor variable with specified levels and names. In the factor function we use levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\") to specify the order of values contained in the variable. Using labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \"Stature (cm)\", \"Body fat (%)\", we set corresponding labels on each level. After we have added this information to the variable we can use arrange to sort the data set. arrange will sort the data set based on the order we have given to the variable. select will help you sort the columns to match what we want.\n\n#| eval: false\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n    mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  print()\n\n`summarise()` has grouped output by 'include', 'sex'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 5 × 5\n  variable     excl_female excl_male   incl_female incl_male  \n  &lt;fct&gt;        &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n1 N            4           3           18          16         \n2 Age (years)  22.9 (1.57) 24.3 (1.46) 22 (1.25)   23.6 (4.11)\n3 Mass (kg)    64.6 (9.71) 88.2 (22.4) 64.4 (10.4) 75.8 (10.7)\n4 Stature (cm) 166 (7.59)  189 (4.58)  168 (6.87)  183 (5.88) \n5 Body fat (%) 28.8 (8.69) 24.3 (15.3) 34.1 (5.64) 20.4 (5.99)\n\n\n\n\n6.2.5 Starting the table generator - The gt() function.\nThe next step is to “pipe” everything into the table generator.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n    mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  # Piping into the table generator (gt)\n\n  gt()  \n\nAs per our strategy to first summarize and set up the data table, we already have nice first draft. However, we need to format variable names and add column labels. We can also add a footnote.\nThe gt package has several functions for manipulating the raw tables created with the gt() function. The gt package also use a consistent vocabulary for tables as seen in Figure 6.1.\n\n\n\nFigure 6.1: Nomenclature for parts of a gt table (source: https://gt.rstudio.com/)\n\n\nFirst, using tab_footnote() we can add the footnote indicating that “Values are mean and (SD)”. We do this by piping the whole table, created with gt() into tab_footnote(footnote = \"Values are mean and (SD)\"). We have two columns representing females and two representing males, these can be more clearly separated by adding a spanner column label. This column label adds rows to the table. Using tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) we add “Female” above the two columns representing females. We can do the same for males. Using cols_label() we specify new column names to match what we want. The resulting full code can be seen below.\n\ndxadata %&gt;%\n  select(participant, time, sex, include:weight, fat.whole) %&gt;%\n  mutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\n  filter(time == \"pre\") %&gt;%\n  group_by(sex, include) %&gt;%\n  mutate(n = n()) %&gt;%\n\n  pivot_longer(names_to =  \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  group_by(sex, include, variable) %&gt;%\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  ungroup() %&gt;%\n  mutate(m = signif(m, digits = 3), \n         s = signif(s, digits = 2), \n         ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n         sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\n  dplyr::select(-m, -s, - sex, -include) %&gt;%\n\n  pivot_wider(names_from = sex_incl, \n              values_from = ms) %&gt;%\n  select(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\n  mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  gt() %&gt;%\n  tab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\n  tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\n  tab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\n  cols_label(variable = \" \", \n             female_incl = \"Included\", \n             female_excl = \"Excluded\", \n             male_incl = \"Included\", \n             male_excl = \"Excluded\")\n\n\n\n6.2.6 Working with tables in quarto\nIf we where to use this table in a report created with quarto we would like to be able to cross-reference it. This will work if we add information to the code chunk where the table is created. More specifically we need to set a table label and a table caption. Quarto has built in support for cross referencing figures and tables. Adding chunk options that sets a label and table caption will make it possible to reference the table. Note that the label must start with “tbl-” to make quarto identify it as a table.\n```{r}\n#| label: tbl-participant-characteristics\n#| tbl-cap: \"Participant characteristics\"\n\ndxadata %&gt;%\nselect(participant, time, sex, include:weight, fat.whole) %&gt;%\nmutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\nfilter(time == \"pre\") %&gt;%\ngroup_by(sex, include) %&gt;%\nmutate(n = n()) %&gt;%\n\npivot_longer(names_to =  \"variable\", \n             values_to = \"value\", \n             cols = age:n) %&gt;%\ngroup_by(sex, include, variable) %&gt;%\nsummarise(m = mean(value), \n          s = sd(value)) %&gt;%\nungroup() %&gt;%\nmutate(m = signif(m, digits = 3), \n       s = signif(s, digits = 2), \n       ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n       sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\ndplyr::select(-m, -s, - sex, -include) %&gt;%\n\npivot_wider(names_from = sex_incl, \n            values_from = ms) %&gt;%\nselect(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\nmutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                         labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                    \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\narrange(variable) %&gt;%\n\ngt() %&gt;%\ntab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\ntab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\ntab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\ncols_label(variable = \" \", \n           female_incl = \"Included\", \n           female_excl = \"Excluded\", \n           male_incl = \"Included\", \n           male_excl = \"Excluded\")\n           \n```\nThe above code will produce referable table, as seen in Table 6.2!\n\n  dxadata %&gt;%\n  select(participant, time, sex, include:weight, fat.whole) %&gt;%\n  mutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\n  filter(time == \"pre\") %&gt;%\n  group_by(sex, include) %&gt;%\n  mutate(n = n()) %&gt;%\n\n  pivot_longer(names_to =  \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  group_by(sex, include, variable) %&gt;%\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  ungroup() %&gt;%\n  mutate(m = signif(m, digits = 3), \n         s = signif(s, digits = 2), \n         ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n         sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\n  dplyr::select(-m, -s, - sex, -include) %&gt;%\n\n  pivot_wider(names_from = sex_incl, \n              values_from = ms) %&gt;%\n  select(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\n  mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  gt() %&gt;%\n  tab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\n  tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\n  tab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\n  cols_label(variable = \" \", \n             female_incl = \"Included\", \n             female_excl = \"Excluded\", \n             male_incl = \"Included\", \n             male_excl = \"Excluded\")\n\n`summarise()` has grouped output by 'sex', 'include'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\nTable 6.2:  Participant characteristics \n  \n    \n    \n       \n      \n        Female\n      \n      \n        Male\n      \n    \n    \n      Included\n      Excluded\n      Included\n      Excluded\n    \n  \n  \n    N\n18\n4\n16\n3\n    Age (years)\n22 (1.3)\n22.9 (1.6)\n23.6 (4.1)\n24.3 (1.5)\n    Mass (kg)\n64.4 (10)\n64.6 (9.7)\n75.8 (11)\n88.2 (22)\n    Stature (cm)\n168 (6.9)\n166 (7.6)\n183 (5.9)\n189 (4.6)\n    Body fat (%)\n34.1 (5.6)\n28.8 (8.7)\n20.4 (6)\n24.3 (15)\n  \n  \n  \n    \n       Values are mean and (SD)"
  },
  {
    "objectID": "05-first-tables.html#footnotes",
    "href": "05-first-tables.html#footnotes",
    "title": "6  Wrangling data to create your first table",
    "section": "",
    "text": "All this talk about pivot, take a break and watch this clip from the hit series “Friends”, its about “pivot”!↩︎"
  },
  {
    "objectID": "05-first-tables.html#references-and-footnotes",
    "href": "05-first-tables.html#references-and-footnotes",
    "title": "6  Wrangling data to create your first table",
    "section": "6.4 References and footnotes",
    "text": "6.4 References and footnotes\n\n\n\n\nHammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. “Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis.” Journal Article. The Journal of Physiology 598 (3): 543–65. https://doi.org/10.1113/JP278455.\n\n\nHaun, C. T., C G. Vann, C. Brooks Mobley, Shelby C. Osburn, Petey W. Mumford, Paul A. Roberson, Matthew A. Romero, et al. 2019. “Pre-Training Skeletal Muscle Fiber Size and Predominant Fiber Type Best Predict Hypertrophic Responses to 6 Weeks of Resistance Training in Previously Trained Young Men.” Journal Article. Frontiers in Physiology 10 (297). https://doi.org/10.3389/fphys.2019.00297.\n\n\nHaun, C. T., C. G. Vann, C. B. Mobley, P. A. Roberson, S. C. Osburn, H. M. Holmes, P. M. Mumford, et al. 2018. “Effects of Graded Whey Supplementation During Extreme-Volume Resistance Training.” Journal Article. Front Nutr 5: 84. https://doi.org/10.3389/fnut.2018.00084."
  },
  {
    "objectID": "04-first-graph.html#references-and-footnotes",
    "href": "04-first-graph.html#references-and-footnotes",
    "title": "5  Creating your first graph",
    "section": "5.8 References and footnotes",
    "text": "5.8 References and footnotes\n\n\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "05-first-tables.html#an-exercise-in-data-wrangling-and-tables",
    "href": "05-first-tables.html#an-exercise-in-data-wrangling-and-tables",
    "title": "6  Wrangling data to create your first table",
    "section": "6.3 An exercise in data wrangling and tables",
    "text": "6.3 An exercise in data wrangling and tables\nIn total 30 college students performed a heavy-resistance training protocol where training volume was constantly increased over six weeks. In (Haun et al. 2018), a part of the study, focusing on supplementation was reported. In (Haun et al. 2019), participants were divided into two clusters based on training responses and the authors aimed to answer the question what separates high- from low-responders to resistance training.\nIn this exercise we want to reproduce a big part of Table 1 in (Haun et al. 2019). The Table as re-produced here can be seen below. See the original article for explanation of clusters. To select variables, see the data description in the exscidata package, the data set is called hypertrophy.\n\n\n\n\n\nBaseline characteristics at PRE and back squat training volume\nbetween clusters\n  \n    \n    \n       \n      HIGH (n = 10)\n      LOW (n = 10)\n    \n  \n  \n    Age (years)\n20.9 (1.9)\n21.5 (1)\n    Training age (years)\n5.5 (2.3)\n5.5 (2)\n    Body mass (kg)\n78.8 (8)\n83.1 (12.8)\n    DXA LBM (kg)\n62.2 (5.9)\n65.1 (9.7)\n    DXA FM (kg)\n13.5 (4.9)\n14.5 (4.9)\n    Type II fiber (%)\n59.4 (16.9)\n50.2 (13.6)\n    3RM back squat (kg)\n127 (23.3)\n135.4 (14.1)\n    Total back squat training volume (kg)\n106610.2 (18679.4)\n111820.8 (12962.5)\n  \n  \n  \n    \n       Values are mean and (SD)\n    \n  \n\n\n\n\nClick for a possible solution\n\n\n# load the data\ndata(hypertrophy)\n\n\nhypertrophy %&gt;%\n# Select the variables needed to reproduce the table\n  dplyr::select(PARTICIPANT, \n                GROUP, \n                CLUSTER,\n                AGE, \n                BODYMASS_T1, \n                TRAINING_AGE,\n                PERCENT_TYPE_II_T1, \n                SQUAT_3RM, \n                DXA_LBM_T1,\n                DXA_FM_T1, \n                SQUAT_VOLUME) %&gt;%\n  # Pivot longer to gather all variables\n  pivot_longer(cols = AGE:SQUAT_VOLUME, names_to = \"variable\", values_to = \"values\") %&gt;%\n  # Remove participants not belonging to a cluster\n  filter(!is.na(CLUSTER)) %&gt;%\n  # Create a grouping before summarizing\n  group_by(CLUSTER, variable) %&gt;%\n  summarise(m = mean(values), \n            s = sd(values)) %&gt;%\n  # For nice printing, paste mean and SD \n  mutate(m.s = paste0(round(m,1), \" (\", round(s,1), \")\")) %&gt;%\n  # Select only variables needed for the table\n  select(CLUSTER, variable, m.s) %&gt;%\n  # Transform the data set to a wide format based on clusters\n  pivot_wider(names_from = CLUSTER, values_from = m.s) %&gt;%\n  # Re-arrange the \"variable\" variable, correct order with levels, and correct labels\n  mutate(variable = factor(variable, levels = c(\"AGE\",\n                                                \"TRAINING_AGE\", \n                                                \"BODYMASS_T1\", \n                                                \"DXA_LBM_T1\", \n                                                \"DXA_FM_T1\", \n                                                \"PERCENT_TYPE_II_T1\", \n                                                \"SQUAT_3RM\", \n                                                \"SQUAT_VOLUME\"), \n                                      labels = c(\"Age (years)\", \n                                                \"Training age (years)\", \n                                                \"Body mass (kg)\", \n                                                \"DXA LBM (kg)\", \n                                                \"DXA FM (kg)\", \n                                                \"Type II fiber (%)\", \n                                                \"3RM back squat (kg)\", \n                                                \"Total back squat training volume (kg)\"))) %&gt;%\n  # Sort/order the dataset\n  arrange(variable) %&gt;%\n  # Use gt to output the table with appropriate caption and column names. \n  gt() %&gt;%\n  cols_label(variable = \" \", HIGH = \"HIGH (n = 10)\", LOW = \"LOW (n = 10)\") %&gt;%\n    tab_footnote(footnote = \"Values are mean and (SD)\")"
  },
  {
    "objectID": "06-writing-reports.html#rstudio-projects-and-your-reproducible-report",
    "href": "06-writing-reports.html#rstudio-projects-and-your-reproducible-report",
    "title": "7  Writing your first reproducible report",
    "section": "7.1 RStudio projects and your reproducible report",
    "text": "7.1 RStudio projects and your reproducible report\nWhen you build an analysis in a R markdown or quarto file, R will use the folder that the source file is in as the root directory. This directory (or folder) is the top directory in a file system. This means that R will look for data or other files used to generate the report in this folder structure. Think of this folder as ./ (confusing, I know! But bare with me!). Any sub-folders to the root directory can be called things like\n\n./data/ (a folder where you keep data files),\n./figures/ (a folder where you output figures from analyses).\n\nThe R markdown or quarto file, being in the root directory will have the “address” ./my_rmarkdown_file.Rmd.\nThis has several advantages, as long as you stick to one rule: When doing an analysis, always use relative paths (“addresses” to files and folders). Never reference a folder or file by their absolute path. The absolute path for the file I’m writing in now is /Users/Daniel1/Documents/projects/quant-methods/06-writing-reports.qmd. The relative path is ./06-writing-reports.qmd. When working in a “project” you may move the folder containing your project to other locations, but relative paths will not break.\nIf you want to share your analysis, all you need to do is share the folder with all content with your friend. If you use relative paths, everything will work on your friends computer. If you use absolute paths, nothing will work, unless your friends computer uses the same folder structure (highly unlikely).\nRStudio projects makes it easy to jump back and forth between projects. The project menu (top right corner in RStudio) contains all your recent projects. When starting a new project, R will create a .Rproj file that contains the settings for your project. If you start a project and click this file, a settings menu will appear where you can customize settings for your particular project.\nWhat does this have to do with my quarto/RMarkdown file? As mentioned above, the source file is often written in a context where you have data and other files that help you create your desired output. By always working in a project makes it easy to keep every file in the right place."
  },
  {
    "objectID": "06-writing-reports.html#getting-started-with-r-projects",
    "href": "06-writing-reports.html#getting-started-with-r-projects",
    "title": "7  Writing your first reproducible report",
    "section": "7.2 Getting started with R projects",
    "text": "7.2 Getting started with R projects\nTo start a new project in RStudio:\n\nPress the project menu in the upper right corner, choose “Start a project in a brand new working directory”\nIn the next menu, select “New Project” and chose a suitable location on your machine for the project to live.\nUn-check the option of creating a git repository. We will do this later.\nName the project with an informative name. “Project1” is not good enough, “rproject-tutorial” or “rproject-report-workshop” is better as you will be able to track it down afterwards.\n\nWe have now started up a brand new project without version control. The next step is to make sure the setting of the project is up date with our Global settings in RStudio. By clicking the .Rproj file in our files tab, we will open up a settings window. These are the settings for the project. Under General we see that we can set RStudio to handle the workspace and history as default. This means that our global options will be used. The global options regarding workspace should be to never save workspace, do not restore on start up and do not save history.\n\n7.2.1 What folder am I in?\nThe great advantage of an RStudio Project is that it will make it easier to keep everything contained in our folder. To check what folder we are currently in, type getwd() in the console. R should return the full path to our working directory. If this is the case, success. If not, you have probably not succeeded in opening up a project, or you have somehow told R to set another working directory.\nThe working directory is the root directory. It is possible to set the working directory manually. However, we should aim not to do that! The R command setwd() should not be used as it breaks relative paths.\nSee R for Data Science, chapter 7 for more details on RStudio projects."
  },
  {
    "objectID": "06-writing-reports.html#authoring-reports-in-quarto",
    "href": "06-writing-reports.html#authoring-reports-in-quarto",
    "title": "7  Writing your first reproducible report",
    "section": "7.3 Authoring reports in quarto",
    "text": "7.3 Authoring reports in quarto\nSo much fuzz just for writing a report? Yes, it is a bit more work to get started. The upside is that this system is easier to navigate with increasing complexity compared to a system where text, figures, tables and software are located on different locations in your computer and the final report requires copy-paste operations.\nAs mentioned before, we will focus on the more modern format for authoring reports in R, quarto. In this section we will introduce the basic building blocks of a report and how to put them together. We have already covered figures and tables, now its time to put the into context.\n\n7.3.1 The Markdown syntax, and friends\nWe have already mentioned the markup language markdown1. This enables an author like yourself to format your text in a plain text editor. This has the advantage of keeping formatting explicit and available from the keyboard. In a word editor like MS Word, formatting is sometimes not obvious and you need to point and click make changes. The R-markdown style of markdown includes the ability to combine code in code chunks and embedded in text. This makes it possible to include code output in the final report. Another technical achievement that makes RMarkdown and quarto possible is Pandoc, a general document conversion software. Pandoc can convert files from one format to another, this includes the operations that we use, from markdown to HTML, PDF or Word. Both markdown and pandoc are free and open source software that makes life easy for us!\n\n7.3.1.1 Markdown basics\nThe idea of using markdown is that everything is formatted in plain text. This requires a little bit of extra syntax. We can use bold or italic, striketrough and superscript. Lists are also an option as numbered:\n\nItem one\nItem two\n\nAnd, as unordered\n\nItem x\nItem y\n\nWith sub item z\n\n\nLinks can be added like this.\nA table can be added also, like this:\n\n\n\nColumn 1\nColumn2\n\n\n\n\nItem1\nItem 2\n\n\n\nThe whole section above will look like this in your plain text editor:\n\nThe idea of using markdown is that everything is formatted in plain text. \nThis requires a little bit of extra syntax. We can use **bold** or *italic*, \n~~striketrough~~ and ^superscript^. Lists are also an option as numbered:\n\n1. Item one\n2. Item two\n\nAnd, as unordered\n\n* Item x\n* Item y\n  + With sub item z\n  \nLinks can be added [like this](https://rmarkdown.rstudio.com/authoring_basics.html).\n\nA table can be added also, like this:\n\n|Column 1|Column2|\n|---| ---|\n|Item1 | Item 2|\n\n\n\n\n7.3.2 Additional formatting\nIn addition to plain markdown, we can also write HTML or \\(\\LaTeX\\) in RMarkdown or quarto files.\nHTML is convenient when we want to add formatted text beyond the capabilities of markdown, such as color. Some formatting might be considered more easily remembered such as subscript and superscript. Notice that HTML and markdown syntax can be combined:\nSome Markdown text with some blue text, superscript.\n\n\nSee here for syntax\nHTML is convenient when we want to add formatted text \nbeyond the capabilities of markdown, such as \n&lt;span style=\"color:red\"&gt;color&lt;/span&gt;. Some formatting \nmight be considered more easily remembered such as \n&lt;sub&gt;subscript&lt;/sub&gt; and &lt;sup&gt;superscript&lt;/sup&gt;. \n\nNotice that HTML and markdown syntax can be combined:\n\nSome Markdown text with &lt;span style=\"color:blue\"&gt;some *blue* \n  text, &lt;sup&gt;&lt;span style=\"color:red\"&gt;super&lt;/span&gt;**script**&lt;/sup&gt;&lt;/span&gt;.\n\n\n\\(\\LaTeX\\) is another plain text formatting system, or markup language, but it far more complex than markdown. Text formatting using \\(\\LaTeX\\) is probably not needed for simpler documents as markdown and HTML will be enough. The additional advantage of using \\(\\LaTeX\\) comes with equations.\nEquations can be written inline, such as the standard deviation \\(s = \\sqrt{\\frac{\\sum{(x_i - \\bar{x})^2}}{n-1}}\\). An equation can also be written on the center of the document\n\\[\nF=ma\n\\tag{7.1}\\]\nWe are also able to cross-reference the equation Equation 7.1 for force (\\(F\\)).\nA larger collection of equations is sometimes needed to describe a statistical model, as in Equation 7.2.\n\\[\n\\begin{align}\n\\text{y}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{x}_i,\n\\end{align}\n\\tag{7.2}\\]\nThe equation above could look like this in your editor, including the tag ({#eq-model}) used for cross-referencing:\n\n$$\n\\begin{align}\n\\text{y}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{x}_i,\n\\end{align}\n$$ {#eq-model}\n\nSee this wikibook on \\(\\LaTeX\\) for an overview on mathematics in \\(\\LaTeX\\).\n\n\n7.3.3 Code chunks\nUsing RMarkdown syntax we can add a code chunk using the following syntax:\n```{r}\n#| label: fig-simple-plot\n#| message: false\n#| echo: true\n\ndat &lt;- data.frame(a = rnorm(10, 10, 10), \n                  b = runif(10, 1, 20))\n\nplot(dat)\n\n```\nWe recognize the R code inside the code chunk but we have only touched upon code chunk settings. These are settings that tells R (or quarto) how to handle output from the code chunk. message: false indicate that any messages from R code should not be displayed in the output document. echo: true indicates that the code in the code chunk should be displayed in the output document. The label is important as it enables cross-referencing the output. If your code chunk outputs a figure the prefix fig- must be in the label to enable cross-referencing. Likewise, if your code chunk creates an table, the prefix tbl- must be in the label. Possible code chunk settings also include figure and table captions.\nSettings can also be specified in the YAML field in quarto files. We might not want to display our code, messages or warnings in the final output. We would specify this in the YAML field as\n\n---\ntitle: \"A basic quarto report without code\"\nexecute:\n  echo: false\n  message: false\n  warning: false\n---\n\nSee here for documentation on execution options for code chunks in quarto. See also Chapter 29 in R for data science.\n\n\n7.3.4 Cross-referencing, references and footnotes\nWe have mentioned cross-referencing above, this basically means referencing specific parts of your document in the text. A figure might be mentioned in the text, such as Figure 7.1. To insert the cross-reference in text, use the @fig-label syntax where fig- is the required prefix for figures and label is a user defined unique identifier. The label should be included in the code chunk under such as #| label: fig-label. The equivalent prefix for tables is tbl-.\n\n\n\n\n\nFigure 7.1: This is an example of a Figure with a caption.\n\n\n\n\nWe might want to cross-reference a section in our document. This is easily done by inserting a tag at the section header such as {#sec-cross-reference}, this tag can be referenced in text using @sec-cross-reference resulting in Section 7.3.4. The sec- part is the required prefix for a section.\nFor additional details on cross-referencing, see the quarto documentation on cross-referencing.\nCitations are mandatory in academic writing. Be sure to take advantage of the built in support for citations. When writing in quarto (or RMarkdown) we can think of a reference as having three parts. The identifier, the reference and the style. We use the identifier when authoring. For example, let’s cite the R for Data Science book, we do this by using the following syntax (Wickham and Grolemund 2017). The syntax requires that we have linked a bibliography to the document. The bibliography should include the reference, with the same identifier. The bibliography is a collection of reference entries written in bibtext format (see below). It must be included in the document meta data field (YAML field).\n\n@book{r4ds,\n  title={R for data science},\n  author={Wickham, Hadley and {\\c{C}}etinkaya-Rundel, Mine and Grolemund, Garrett},\n  year={2023},\n  publisher={\" O'Reilly Media, Inc.\"}\n}\n\nNotice the identifier. When adding the citation [@r4ds] it will turn out to (Wickham and Grolemund 2017) in the formatted text and added to the bottom of the document as a full reference. If we want another citation style we can specify a file responsible for citation styles. The default is the Chicago style. Specifying a citation style file in YAML will change the style, for example csl: my-citation-style.csl tells quarto to use the file my-citation-style.csl when formatting citations. This file can be edited or copied from a large collection of possible styles located in the citation style language repository. The repository is hosted on GitHub and searchable, click “Go to file” and type “vancouver” to get examples of CSL files that uses a Vancouver-type citation style.\nFootnotes can be handy when writing. In the default mode, these will be included as superscript numbers, like this2, numbered by order of appearance.\nThe syntax for including footnotes is straight forward. Notice that the text for the footnote is included below the paragraph using the identifier created in the text.\n\n\nSee here for footnote syntax\nFootnotes can be handy when writing. In the default mode, \nthese will be included as superscript numbers, like \nthis[^footnote], numbered by order of appearance. \n\n[^footnote]: This is a footnote.\n\n\nSee the quarto documentation on citations and footnotes.\nsee also Chapter 29 in R for data science."
  },
  {
    "objectID": "06-writing-reports.html#additional-files-and-folder-structures-in-a-complete-analysis-project",
    "href": "06-writing-reports.html#additional-files-and-folder-structures-in-a-complete-analysis-project",
    "title": "7  Writing your first reproducible report",
    "section": "7.4 Additional files and folder structures in a complete analysis project",
    "text": "7.4 Additional files and folder structures in a complete analysis project\nAs we starting to notice, a report authored in quarto or R Markdown often requires additional files to render properly. We might have a collection of references, some data sets and possibly some analysis files that are not included in the quarto or R markdown file. To keep everything organized I recommend a general folder structure for every analysis project. This structure might change as the project grows or changes. The parts listed below are what I usually end up with as a common set in the majority of projects I work with3.\n\n7.4.1 The readme-file\nThe README-file can be, or should be an important file for you. When a project is larger than very tiny, it becoms complex and you should include a README-file to tell others and yourself what the project is about and how it is organized. Creating a file called README.md in a GitHub folder automatically renders it on the main page of your repository (more about that later). Here you have the opportunity to outline the project and explain the organization of your projects folder/repository.\nI find it very helpful to work with the README-file continuously as the project evolves. It helps me remember where the project is going.\nA very basic ouline of the README-file can be\n\n# My project\n\nAuthor: \nDate: \n\n## Project description \nA description of what this prject is about, the \npurpose and how to get there. \n\n## Organization of the repository\n\nFiles are organized as...\n\n## Changes and logs\n2023-08-15: Added a description of the project...\n\n\n\n7.4.2 /resources\nI usually include a sub-folder called resources. Here I keep CSL-files, the bibliography, any styling or templates used to render the report. Keeping this in a separate folder keeps the top-folder clean.\n\n\n7.4.3 /data\nThe data folder is an important one. Here I keep all data that exists as e.g., .csv or .xlsx files. If I create data in the project, such as combined data sets that are stored for more convienient use, I keep these in a sub-folder (e.g., data/derived-data/)4. If there is a lot of raw unprocessed data, these might be stored in data/raw-data/ with specific sub-folders.\n\n\n7.4.4 /figures\nIf you want to make figures for presentations or submission to a journal, you might want to save output as .tiff or .pdf files. When doing this it might be a good idea to structure a figure-folder with e.g. figure1.R that renders to e.g. figure1.pdf. If you only include figure output in the quarto, the figure folder might contain R-scripts that produces the figures. The end results are included in the quarto document by sourcing the R-script. This detour might make it easier to find code for a specific figure once your project is large enough.\n\n\n7.4.5 /R\nR-scripts that are not figures but contains analyses or data cleaning or the like can be stored in R scripts in a specific folder. The reason to keep R scripts separate from a quarto file might be that they are large and produces some output, like a data set, that is later used in the report file. It makes it easier to find and work on specific code without breaking other parts of your project. Actually, it is a good idea to “build” the parts of your analysis as smaller parts."
  },
  {
    "objectID": "07-version-control-git.html#why-version-control",
    "href": "07-version-control-git.html#why-version-control",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.1 Why version control",
    "text": "8.1 Why version control\nGithub is a platform for collaborative coding. As we have noted before, collaboration concerns both others and you, in the future! This means that having a formal system for keeping track of your projects is a good thing.\nGithub also provides version control. Version control can help you track changes in your entire analysis or writing project. This is helpful when multiple files make up a complex project, including e.g. scripts, data and manuscript files. It is also helpful when multiple collaborators work together (e.g. writing a report). You will, by using version control, avoid overwriting other peoples work. With multiple changes made to the project, merging will create the latest up-to-date version. When you change a file in your analysis you will be required to describe the changes you have made. Git creates a record of your changes. This also means that we have “backups” of previous versions."
  },
  {
    "objectID": "07-version-control-git.html#a-simplecomplicated-start",
    "href": "07-version-control-git.html#a-simplecomplicated-start",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.2 A simple/complicated start",
    "text": "8.2 A simple/complicated start\nI use version control when working in R by setting up repositories on www.github.com and then cloning it to my local computer. A repository is a folder containing all the files in a specific project. Using projects in RStudio makes it easy to synchrionize local projects with the online version control system.\nWhen cloning a project, you download all files to your personal computer, you are then free to work on the project without interference from others. When you have created a new file you add the file to version control and commit the changes. This means that your change has got a unique identity in the history of your project. You may now push changes to the online repository which is the online version of your work.\nWhen collaborating with others, pull requests makes it easy for others to make changes to your repository that you have to accept or decline. This is somewhat equivalent to suggesting a change in a word document with track changes activated."
  },
  {
    "objectID": "07-version-control-git.html#step-by-step-instructions-on-setting-up-a-version-controlled-r-project.",
    "href": "07-version-control-git.html#step-by-step-instructions-on-setting-up-a-version-controlled-r-project.",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.3 Step by step instructions on setting up a version controlled R project.",
    "text": "8.3 Step by step instructions on setting up a version controlled R project.\n\n8.3.1 Step 1. Create a free github account:\nGo to www.github.com, press sign up and follow the instructions.\n\n\n8.3.2 Step 2. Download git.\nGit is the software responsible for actually keeping track of all your files. We need to have this installed locally to make this work. Go to https://git-scm.com/downloads and download the version compatible with your system (windows or mac). Install git by following the instructions.\n\n\n8.3.3 Step 3. Connect git to RStudio\nIn RStudio, click Tools -&gt; Global options -&gt; Git/SVN. Click to “Enable version control interface …”. Now you have to point RStudio to your git executable. On my computer after following the standard installation of git, the executable is found in: C:/Program Files/Git/bin/git.exe. Browse to find your copy. RStudio has to be restarted to make the changes work.\n\n\n8.3.4 Step 4. Create a new online repository\nGo to www.github.com and sign in using your user name and password created in step 1. Click “New” to create a new repository. Give the repository a name. If you want you can add a description of the repository, this can be something like “statistics report 5 in IDR4000”. You can decide if you want the repository to be public or private. To share it with others without giving special permissions, it needs to be public.\nUnder “Initialize this repository with” you can select if you want to start your repository with a README file. This is good if you want to describe what the repository contains.\nAfter selecting what you want, press Create repository. Copy the link on the resulting page under quick setup.\n\n\n8.3.5 Step 5. Create a new project in RStudio\nWe will now connect the online repository to a RStudio project. Click the projects button in the top right corner and select new project, select a version controlled project, select Git and paste the link from github (step 4) into the field for repository URL.\n\n\n8.3.6 Step 6. Make changes to your project\nYou now have an “empty” project. You can add for example a R Markdown file by writing one and saving it in your local repository. In the tab containing the Environment, History etc. you should have a tab that says Git. Here you can commit changes and push them.\nLet us say that you have created a file called report.Rmd. In this file you have written an analysis. You now want to add these changes to version control. You can do this by clicking the file under Staged and then commit. You will need to write a commit message, a short description of what you have done and the press commit. The changes are now saved and version controlled. To upload these changes to your online repository, press push.\nWhen you have a lot of changes RStudio is a bit slow if you use the interface. An alternative is to use the terminal.\nA simple setup is to use the git bash terminal. Under Tools, go to Terminal and terminal options. Select Git Bash in the list “New terminals open with”. Where you have the console, there is also a tab called Terminal, if not, start a new one with Tools -&gt; Terminal -&gt; New Terminal.\nThe simplest commands are as follow:\nYou have made changes and want to upload them, in the terminal, write:\ngit add -A\nThis adds all changes to your next commit, next, write:\ngit commit -m \"A commit message\"\nThe “A commit message” is the description of the changes you have made. And finally write:\ngit push\nThis will push your changes to your repository.\nLet’s say that someone else have made changes, or you have made changes online to your repository. You then want to start by downloading the latest changes to your computer. If you are using the RStudio interface, under Git press pull. If you are using the terminal, write:\ngit pull"
  },
  {
    "objectID": "07-version-control-git.html#forking-and-pull-requests",
    "href": "07-version-control-git.html#forking-and-pull-requests",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.4 Forking and pull requests",
    "text": "8.4 Forking and pull requests\nCreating a fork in git means that you make a copy of someones repository. In the web interface you can press “Fork” in the upper right corner when you are visiting another repository. This will copy the content of the repository to your collection of repositories. You may now change the content of the repository and issue a “Pull request”. The “Pull request” are suggested changes to the project (or repository). The “Pull request” may then be scrutinized by the maintainer of the original repository, they may accept the changes and if so, you have contributed in the project."
  },
  {
    "objectID": "07-version-control-git.html#additional-great-things-about-github",
    "href": "07-version-control-git.html#additional-great-things-about-github",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.4 Additional great things about GitHub",
    "text": "8.4 Additional great things about GitHub\nGitHub has great capabilities for managing projects. You can for example:\n\nPost issues that are suggestions or questions regarding a repository. Issues can be categorized with labels and assigned.\nYou can create to-do lists in the Projects tab (in the web interface). This could be a nice way of sharing and tracking the progress of a project.\nYou can build a wiki. This is simply a collection of pages that can be used to document the repository or a project (in a wider sense) that you are working on.\nAll of the above can be private and public. You can choose whom have access to your repository. This makes it easy to work on a project even if you need to keep things a secret.\n\nIn this course I want you to contribute to the wiki pages of the course. The wiki is hosted at github.com/dhammarstrom/IDR4000-2021. To contribute you need to have created your own GitHub user account."
  },
  {
    "objectID": "07-version-control-git.html#when-will-this-knowledge-be-handy",
    "href": "07-version-control-git.html#when-will-this-knowledge-be-handy",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.5 When will this knowledge be handy?",
    "text": "8.5 When will this knowledge be handy?\nWhen writing your master thesis, it will be extremely easy to share your code with your supervisor or other students, whit whom you collaborate. You can just invite someone to make changes in your repository and then download them. As noted several times before, your most frequent collaborator is you. Using git makes it easy to keep track of changes in your project and it keeps your most frequent collaborator from messing up your work.\nVersion control workflows are part of almost all technology companies, and will most certainly be part of many more types of businesses, institutions and workplaces in the future as we need to collaborate on large, complex projects. Knowing about these systems is in that sense quite handy!"
  },
  {
    "objectID": "07-version-control-git.html#resources",
    "href": "07-version-control-git.html#resources",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.6 Resources",
    "text": "8.6 Resources\nThere are of course more functions in git, here are some resources for deeper understanding.\n\nExtensive resources can be found on Happy Git and GitHub for the useR\nKarl Broman provides a “minimal tutorial”\nGitHub hosts resources for learning Git"
  },
  {
    "objectID": "06-writing-reports.html#footnotes",
    "href": "06-writing-reports.html#footnotes",
    "title": "7  Writing your first reproducible report",
    "section": "",
    "text": "Markdown was introduced in 2004 as a syntax to convert plain text to formatted HTML. Markdown is primarily attributed to John Gruber.↩︎\nThis is a footnote.↩︎\nThis organization was initially inspired by Karl Broman’s steps towards reproducible science.↩︎\nAgain, an important note from Karl Broman, “Organize your data and code”↩︎"
  },
  {
    "objectID": "06-writing-reports.html#references-and-footnotes",
    "href": "06-writing-reports.html#references-and-footnotes",
    "title": "7  Writing your first reproducible report",
    "section": "7.6 References and footnotes",
    "text": "7.6 References and footnotes\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "07-version-control-git.html#three-ways-of-hooking-up-to-github",
    "href": "07-version-control-git.html#three-ways-of-hooking-up-to-github",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.2 Three ways of hooking up to GitHub",
    "text": "8.2 Three ways of hooking up to GitHub\n\n8.2.1 Create a new repository on GitHub and clone it\nAccess your personal GitHub account and click New under repositories. This is equivalent to going to www.github.com/new. GitHub will ask for a repository name, a description and whether you want the repository to be public or not. You can also chose to add a Readme-file.\nNames and descriptions are important, a better name and description makes it easier for you and others to find and make use of your repository. Even when making repositories for school assignments, a good name will likely make it more re-usable in the future. The same is true for the readme file. So, name the repository with a descriptive name, write a short description with the purpose of the repository and add a readme-file to the repository.\nA public repository is open for everyone, private repositories have restricted access.\nOnce the repository is created you can clone it. This means that you will copy the content to your local machine (PC/Mac). In RStudio this is most conveniently done by starting a new RStudio project and selecting Version Control in the project menu. You will be asked to copy the address shown under “Code” on GitHub.\n\n\n8.2.2 Create an online repository from a local folder\nLet’s say that we have a local folder that is a RStudio Project, without version control and we want to create a online repository together with version control. We can use GitHub desktop to accomplish this or GitHub CLI.\nUsing the terminal and GitHub desktop:\n\nThe first step is to make the local folder a git repository, in RStudio with the project running go to a terminal and type git init. The terminal will let you know that you have initialized a git repository.\nStart up GitHub desktop, under File choose Add local repository and find the folder on your computer where you have your RStudio project. Once open in GitHub desktop you will see all changes and additions of new files.\nCommit your changes by writing a first commit message, and possibly a longer description of the commit.\nClick “Publish repository”, you will be asked to edit the name and description of the repository and choose whether to have the repository private or not (see above for recommendations).\nGo to GitHub.com and check if the repository is published.\n\nUsing the terminal and GitHub terminal client (CLI):\n\nBe sure to be in your RStudio project and use the terminal in RStudio to initiate a git repository, type git init in the terminal.\nAlso in the terminal type gh repo create, this will guide you through the same process as with GitHub desktop but all selections are done in the terminal.\n\n\n\n8.2.3 Create an online repository from a local git repository\nIf you have already initialized a RStudio project as a git repository you can follow the steps above without the git init command. Using git init on an already initialized git repository will reinitialize it. This will not remove git history of the repository (see here for documentation)."
  },
  {
    "objectID": "07-version-control-git.html#git-commands-and-workflows",
    "href": "07-version-control-git.html#git-commands-and-workflows",
    "title": "8  Version control and collaborative coding – introducing git and github.com",
    "section": "8.3 Git commands and workflows",
    "text": "8.3 Git commands and workflows\n\n8.3.1 Add, commit and push\nThe day to day workflow when working on a git project involves making changes to your files and saving those changes locally and in the version control system. By the end of the day you might also want to make sure all changes are synchronized with the online repository.\nThis workflow includes the git commands add, commit and push.\nUsing the terminal git add &lt;filename&gt; or git add -A adds a specific file or all changes to a list of changes to be commit into version history. The equivalent operation in GitHub desktop is checking all boxes under changes. This is done automatically and you have to uncheck files or changes that you do not want to commit to history.\nIn the terminal we can commit changes to the git history using the command git commit -m \"a commit description message\" the additional part -m \"a message... is a required commit message. It is good to be informative if you need to find a specific change to a file. In GitHub desktop this is easily done by writing a commit message under summary in the bottom left corner once you have changes in your repository.\nThe last step, git push, means that you are uploading all changes to the online repository. This will update the repository on GitHub, your version history is now up to date in your online repository. This also means that you have an online backup of your work.\n\n\n8.3.2 Collaboration, pull, clone and fork\nCollaboration is most often done with yourself in the future. The git pull command (using the terminal) downloads all changes to your working directory. You want to do this when you have changes in the online repository that is not synchronized with the local repository. This might be the case if you have made changes to your repository on GitHub, like added a readme file. Or if you are collaborating with someone who have made changes to the repository. I work on multiple computers, sometimes on the same repository, the online repository is where a keep the most up to date version of my project.\nUsing GitHub desktop, we can click Fetch origin to get the latest changes from the online repository. GitHub desktop will suggest to pull these changes to the working directory after you have done this operation.\nWe have already covered git clone, this essentially means downloading an online repository to your local machine. This is most easily done while initializing a new RStudio project.\nA fork is a copy of someones online repository that is created as a new repository under your user. You now have access to this repository and can make changes. The repository can have its own life or be used to create changes that later are suggested as changes to the “parent repository”.\nIn this course you can fork a template for the portfolio exam. This is an example where your fork will have its own life.\nIf a fork is used to suggest changes this is done through a pull request. Using the web client (GitHub), we can click create pull request when inside a forked repository. This will take you a few steps where you are expected to describe changes to the repository. The original author will get a notification to review the pull request and can chose to incorporate the changes into the parent repository.\n\n\n8.3.3 Branches\nMuch like a fork, we can create copies of our own repository. These are called branches. A branch might contain changes that we want to try out before we make it the “official” version of our repository. These changes can include experiments that might mess up things or break code.\nUsing GitHub desktop we can create a new branch by clicking Current branch in the upper left and then Create branch."
  },
  {
    "objectID": "06-writing-reports.html#quarto-formats",
    "href": "06-writing-reports.html#quarto-formats",
    "title": "7  Writing your first reproducible report",
    "section": "7.5 Quarto formats",
    "text": "7.5 Quarto formats\nQuarto brings many possibilities for authoring data-driven formats, including but not restricted to websites, books, blogs and presentations. In this course"
  }
]
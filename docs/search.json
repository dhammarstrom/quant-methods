[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "",
    "text": "1 Introduction\nWelcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. These course notes covers almost the entire course through the combination of video lectures, tutorials and references to the course literature and external resources.\nThis book contains lecture notes for the course. Assignments, tutorials and other cours material has been moved to the course workshop site"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Quantitative methods and statistics (In Sport and Exercise Science)",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites"
  },
  {
    "objectID": "01-intro-to-data.html#about-data-in-the-world-of-sport-and-exercise",
    "href": "01-intro-to-data.html#about-data-in-the-world-of-sport-and-exercise",
    "title": "2  Introduction to data science",
    "section": "2.1 About data in the world of sport and exercise",
    "text": "2.1 About data in the world of sport and exercise\nData are everywhere. Most of us always walk around with a data collection device in our pockets. This device (your mobile phone) records and store data about you throughout the day. Such data are the basis of the quantified self-movement1, which has grown in popularity as capabilities to record data from daily life have improved. People interested in quantifying their personal life does so for different reasons, but often with the intent to improve their health2.\nMuch of this kind of data is readily available to us because data privacy policies regard it as personal 3. With some effort, you can get your data out of your iPhone to explore, for example, your daily step count. I discovered that my phone(s) has been collecting data for me since 2016, and I tend to walk fewer steps on Sundays than Saturdays (see Figure Figure 2.1).\n\n\n\n\n\nFigure 2.1: Step count data from my iPhone displayed as all avalable data points (A, after data cleaning) and average step per weekday, per year and season (B).\n\n\n\n\nData are also collected and stored in publicly available databases. Such databases are created for the purpose of storing specific types of data, such as soccer4 or biathlon results5, or biological information, such as gene sequences6. Even data from scientific studies are often publicly available7, meaning we can perform scientific studies on unique data sets without collecting the data ourselves.\nThe above examples show an abundance of available data. The problem is that to understand a phenomenon better, we need techniques and methods to make sense of the data, and this is where data science and data literacy comes in. In the world of sports and exercise, regardless if you are interested in doing scientific investigations, coaching a soccer team or individual athletes, or helping patients recover from surgery using exercise therapy, you are faced with the problem of handling and making sense of data. Essential skills and a deeper understanding of data science are transferable between such areas of practice. One broader aim of this course is for you to develop skills to better understand data.\n\nThink about the literature! Spiegelhalter (The Art of Statistics, in the introduction chapter) talks about how statistics has evolved towards the broader field of data science. In data science, statistical theory and methods are just parts of the problem solving cycle. Try to think about how you would use the PPDAC cycle as a exercise coach and a scientist. What are the similarities and differences?"
  },
  {
    "objectID": "01-intro-to-data.html#replication-and-reproducibility",
    "href": "01-intro-to-data.html#replication-and-reproducibility",
    "title": "2  Introduction to data science",
    "section": "2.2 Replication and Reproducibility",
    "text": "2.2 Replication and Reproducibility\nIn scientific research, replication is a way to confirm scientific claims. When an independent group of researchers can confirm a result, the claim is more likely to be true. However, many results will be impossible to replicate due to the size of trials, costs, and urgency of the research question. A recent example is the many vaccine trials performed to develop a vaccine against COVID-198. Other examples concern studies with unique study populations, such as large-scale epidemiological studies (Peng, Dominici, and Zeger 2006), but the same is true for unique investigations in sport and exercise science.\nWhen studies are not likely to be replicated, reproducibility of the analyses and results has been suggested to be a minimum standard for scientific studies. Reproducibility means that independent researchers can draw similar results or conclusions from the same data (Peng, Dominici, and Zeger 2006).\nPeng et al. (Peng, Dominici, and Zeger 2006) suggests that a fully reproducible study has\n\nAvailable data.\nComputer code (software) that produces the results of the study.\nDocumentation that describes the software and data used in the study, and\nways to share the data and code.\n\nThe above principally relates to the trust we can place in scientific results. However, the minimum reproducibility standard also has advantages for the individual researcher (or master’s student)! When working with reproducible methods, we will develop ways of documenting and automating our analyses. This way of working with analyses will make it easier to collaborate with others. And, as it turns out, your most frequent collaborator is you in the future!\nReproducible data analysis means that you will make it explicit and transparent. In traditional data analysis, most activities are in the “black box.” To avoid bias (Ioannidis 2005), the “black box” needs to be opened, and you need to actively make transparent decisions all along the analytic pipeline (Leek and Peng 2015). This pipeline preferably involves the whole problem-solving cycle described by Spiegelhalter (Spiegelhalter 2019). However, the tools we will learn in this course focus primarily on the steps from the experimental design to the presentation of statistical results (Leek and Peng 2015). These steps include data collection (and storage), data cleaning, exploratory data analysis, statistical modeling, and statistical inference (and communication) (Leek and Peng 2015)."
  },
  {
    "objectID": "01-intro-to-data.html#tools-in-data-science",
    "href": "01-intro-to-data.html#tools-in-data-science",
    "title": "2  Introduction to data science",
    "section": "2.3 Tools in data science",
    "text": "2.3 Tools in data science\nWays to interpret and make sense of data involve different methods. These methods are often implemented in computer software, which means that when you want to understand data as a practitioner (scientist, coach, analyst), you must master some computer software. Microsoft’s Excel is one of the most common software used to understand data, even among professional data scientists9. You can do fantastic stuff with Excel! In the world of sport and exercise, Excel has been used in diverse activities such as scientific investigations, planning and recording training for world champions10, and scheduling appointments.\nFor scientific research, most people use additional software to do statistical analyses. If you have spent time in higher education, you have probably heard about SPSS, Stata, or Jamovi. These are all specialized software used for statistical analyses.\nThe tools mentioned above can all be used as part of a fully reproducible workflow. However, some software solutions suit this requirement better than others. Going back to the description of reproducible science as made by Peng et al. (Peng, Dominici, and Zeger 2006), we want software where analyses can be\n\nHuman- and computer-readable, meaning that we want to be able to write scripts or computer programs that execute the analyses.\nDocumented, meaning that along the code, we want to be able to describe what the code does.\nAvailable and able to share with others, meaning that our analyses can be run on open and free software to maximize the ability to share them.\n\nThis means that the software we would prefer should be run using scripts (as opposed to point and click) and be free of charge (and open source, as opposed to expensive and proprietary). These criteria can be fulfilled when we use software written around the R language (although alternatives exist 11).\nR is a computer language especially well suited for reproducible data analysis. As users can contribute software extensions, also called packages, many specialized software implementations exist for tasks such as creating figures or analyzing specific data. Around R, people have been developing auxiliary software for reproducible data analysis. The negative part of all these opportunities is that using R requires effort. The learning curve is steep!\nEven though you might not use R ever again after this course, trying to learn it will let you know something about programming, modern data science capabilities, statistical analysis, and software/computers in general. These areas are all aspects of our modern society and are transferable regardless of what computer language we are talking about.\nA big challenge when working with complex analyses or other large projects over time is keeping track of changes. Another challenge might be effective collaboration with others and with yourself in the future. To overcome these challenges, we can use a version control system connected to a social platform for distributing computer code and data. Github is a web-based platform that provides this functionality. It is a potent combination if you want to collaborate and share what you are working on."
  },
  {
    "objectID": "01-intro-to-data.html#installing-and-getting-to-know-the-required-software",
    "href": "01-intro-to-data.html#installing-and-getting-to-know-the-required-software",
    "title": "2  Introduction to data science",
    "section": "2.4 Installing and getting to know the required software",
    "text": "2.4 Installing and getting to know the required software\nAs noted above, there are multiple computer languages and software solutions that could satisfy our needs. However, in this course, we will focus on a combination of continuously improved tools to make it easy for the user to collaborate and communicate data analyses. Below is a checklist of what you must install on your system to take full advantage of the proposed tools.\n\n2.4.1 R and RStudio\nR is a free, open-source software designed for statistical computing. We will use R as a part of an environment (using R Studio, introduced below). To download and install R:\n\nGo to https://cran.uib.no/,\nSelect your operating system (Download R for Windows, MacOS or Linux).\n\nIf you have Windows, choose base, click on “Download R (…) for windows”, save and run the file. The installation process should be self explanatory.\nIf you have MacOS, download and install the latest release.\n\nRun the installer to install R.\n\nRStudio is a software designed to make it easier to use R. It is free to download and use. It is designed as an integrated development environment that lets you organize your work together with R and other tools. Install it by going to https://www.posit.co/.\n\nSelect “Products” and RStudio IDE\nScroll down and find the FREE open source edition\nDownload the installer made for your operating system.\n\n\n\n2.4.2 Git and Github\nGit is a software that you need to install on your system in order to use version control. Github is the web platform that allows collaboration and web-based storage of your work. First, we will install git.\nFor windows:\n\nIf you have Windows, Go to https://git-scm.com/downloads and download the latest version for your operating system.\nRun the installer. Make a note of where you installed it!\n\nFor Mac:\n\nIf you are on Mac, the easiest thing is to first install Homebrew, this will make it easy to get the latest version of what we will need. Go to https://brew.sh/ and follow the instructions. Note that you will need to open the terminal and enter the install command.\nInstall git by entering the follwing command in a freshly opened terminal:\n\nbrew install git\nCheck if git was installed by restarting the terminal and write\ngit --version\nAdditional warnings might appear indicating that you’ll need some extra software. More specifically, you might need Xcode command line tools. To install these, go to your terminal and enter\nxcode-select --install\nIf you had problems with the homebrew installation itself or the brew installation of git before, try again after installing xcode command line tools.\n\n\n2.4.3 Connecting to GitHub\nFirst we will let RStudio know where git is located\n\nOpen RStudio, go to Global Options under the Tools menu. Go to the Git/SVN sub-menu and find the folder where git.exe is located by browsing in the “Git executable” field.\n\nOn windows:\nIf you have installed git using default settings your git.exe should be located in C:/Program Files/Git/bin/git.exe.\nOn Mac:\nIf you have installed git using homebrew, your git version may be found in /usr/local/bin/git.\nTo register for a Github account\n\nGo to Github.com.\nFind “Sign up” and follow the instructions.\n\nNext we need to connect our git software to github. This is done by authentication. There are several options, however below are two options that should work right away!\n\n2.4.3.1 Installing GitHub desktop\n\nGo to desktop.github.com\nDownload the installer and follow the instructions.\nOpen GitHub Desktop and go to File &gt; Options &gt; Accounts and select Sign In to Github.com, follow the instructions\n\n\n\n2.4.3.2 Installing Github CLI\nIf you were successful in authenticating with Github desktop as described above, you should be all set. However, as an alternative you could install and use Github CLI. This is a collection of command line commands that makes it easy to use github from the command line. I recommend installing them:\n\nGo to https://cli.github.com/ and follow the instructions.\n\n\nFor windows, install GitHub CLI with the installer.\nFor Mac, use homebrew: brew install gh\n\nNext we will perform the authentication process:\n\nOpen a terminal and type gh auth login, follow the instructions.\n\nDone!\n\n\n\n2.4.4 A note on Git and clients\nAs noted above, git is a software containing a number of functions for version control of files collected in a folder (or repository). A client in this context refers to a user interface that makes it easy to communicate with git. RStudio has some features that makes it possible to execute git commands by clicking, however this client is not very powerful, you might want another, or several other alternatives.\nFirst, git is available from the command line. It might look like this:\ngit add -A\nWe will touch upon more git commands for the command line later. The above adds all changes you have made to a list of changes that will be included in your next snapshot of your project. More on that later!\nSeveral Git clients can be run at the same time. This means that you might do some git on the command line in a terminal window in RStudio, and you might follow the changes in a graphical user interface, such as GitHub Desktop. The graphical user interface lets you navigate more easily and might help you understand what git is doing. We will be using GitHub desktop, so you make sure you have installed it (see above).\n\n\n2.4.5 Quarto and friends\nThe R community has pioneered literate programming for data analysis by early adoption of file formats that lets the user combine computer code and output with text (Peng, Dominici, and Zeger 2006). A well adopted file format in recent years have been R markdown which combines R code with text and lets the user compile reports in multiple output formats from a source document. R markdown is an “R-centric” approach to literate programming. Even though it lets you combine multiple computer languages, all code execution goes through R. Recently, a new format has been introduced, Quarto, which is not executed through R but its own dedicated software, Quarto.\nRmarkdown and Quarto have many similarities in that you can use markdown, a well established markup language to format text with a plain text editor (like notepad). This means that for the R user, most differences between RMarkdown and quarto in formatting your documents are irrelevant for getting started.\nAs quarto authoring requires its own software, we need to do some installation.\n\nGo to quarto.org\nClick “Get Started” and follow the instructions.\n\nA nice output from a quarto source documents is a PDF. In order to create PDFs using R/RStudio/quarto we need to install a version of the typesetting system TeX. Quarto recommends12 using tinytex which is easily installed after you have installed quarto.\n\nOpen up RStudio and a fresh terminal\ntype quarto install tinytex and follow the instructions.\n\nYou should be ready to go now!"
  },
  {
    "objectID": "01-intro-to-data.html#summing-up-and-where-to-find-help",
    "href": "01-intro-to-data.html#summing-up-and-where-to-find-help",
    "title": "2  Introduction to data science",
    "section": "2.5 Summing up and where to find help",
    "text": "2.5 Summing up and where to find help\nWe have installed R, RStudio, git, GitHub desktop/CLI, quarto and tinytex. You have also created a github account. These are the tools that you will need to go further in this course. But what if you run into problems? Do not worry, the internet is at your service! A lot of people work very hard to make it easy for beginners to adopt their tools. Documentation of the tools we have installed so far is available through google or any other search engine. People are also very helpful in answering questions, answers to large and small problems can be found in forums such as stack overflow(see below).\nLearning new skills, like doing data analysis by programming, can be hard but rewarding. If you want to make your learning experience less hard, consider these points:\n\nThere are (almost always) multiple solutions to a problem. When faced with difficulties, do not give up trying to search for a perfect single solution. Instead know that there are multiple ways of defining the problem and therefore multiple ways of making stuff work.\nSomeone else has already had the same problem. The internet is full of questions and answers, also related to what ever problem you might have. Learning how to write “googleable” questions is a great skill. By adding “in R” to your problem in a google search term often helps finding R related solutions.\nFind your motivation. The skills that you will learn in this course are transferable to countless potential work related roles for the future you! To be able to showcase these skills may lead you to your dream job! Find your motivation for learning how to analyze data and communicating insights!\n“Microdosing” statistical learning. Replace your social media influencers with R users and data scientists! I find R people on Twitter and mastodon. Tweets and posts in this format keeps your R brain going!\n\n\n2.5.1 A (small) list of reference material and resources\n\nR for Data Science is a very comprehensive guide to working with R. It can be used chapter by chapter or by looking for tips on specific subjects.\nThe official An Introduction to R released by the R Core Team gives a thorough overview of R. This document can be used to find explanations to basic R code.\nLearning statistics with R Is a free textbook where statistical concepts are integrated with learning R. Use this book as a reference.\nHappy Git and GitHub for the useR is used as background material for our workshop in version control and collaborative data analysis.\nTidyverse Is a collection of R packages that makes it easier to be productive in R. Here you will find documentation for ggplot, dplyr and tidyr which are all packages that we will use extensively in the course.\nStack overflow is a web platform where users provide answers to questions raised by other users. Here you will find answers to many of your R-related questions. Stack overflow will likely come up if you google a R problem by you can also search the website.\nR bloggers collects blog posts from R users, here you can find interesting use cases of R and tips."
  },
  {
    "objectID": "01-intro-to-data.html#references-and-footnotes",
    "href": "01-intro-to-data.html#references-and-footnotes",
    "title": "2  Introduction to data science",
    "section": "2.6 References and footnotes",
    "text": "2.6 References and footnotes\n\n\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” Journal Article. PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nLeek, J. T., and R. D. Peng. 2015. “Statistics: P Values Are Just the Tip of the Iceberg.” Journal Article. Nature 520 (7549): 612. https://doi.org/10.1038/520612a.\n\n\nPeng, R. D., F. Dominici, and S. L. Zeger. 2006. “Reproducible Epidemiologic Research.” Journal Article. Am J Epidemiol 163 (9): 783–89. https://doi.org/10.1093/aje/kwj093.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "01-intro-to-data.html#footnotes",
    "href": "01-intro-to-data.html#footnotes",
    "title": "2  Introduction to data science",
    "section": "",
    "text": "Read more about the quantified self movement in this Wikipedia article↩︎\nSee this website for intriguing examples↩︎\nSee e.g. Apples Privacy Policy.↩︎\nunderstat.com stores match specific data from major leagues. Data are available through software packages such as worldfootballR↩︎\nbiathlonresults.com/ hosts results from the international biathlon federation. An example of analyzed data can be seen here.↩︎\nEnsembl and the National center for biotechnology information are commonly used databases in the biomedical sciences.↩︎\nWe published our raw data together with a recent paper (Mølmen et al 2021 doi: 10.1186/s12967-021-02969-1.) together with code to analyze it in a public repository.↩︎\nhttps://www.evaluate.com/vantage/articles/news/snippets/its-official-covid-19-vaccine-trials-rank-among-largest↩︎\n(See for example this ranking)[https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html].↩︎\nThe amount of time used by different coaches to create their own specific coaching software really makes many of them amateur software engineers. See for example this training journal from swedish orienteering.↩︎\nIn addition to R, Python offers a free open source environment for reproducible analyses. The choice between the two are matter of taste.↩︎\nSee the quarto documentation for details on creating pdfs and installing TeX distributions https://quarto.org/docs/output-formats/pdf-basics.html↩︎"
  },
  {
    "objectID": "02-spreadsheets.html#cells-and-simple-functions",
    "href": "02-spreadsheets.html#cells-and-simple-functions",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.1 Cells and simple functions",
    "text": "3.1 Cells and simple functions\nA spreadsheet consists of cells, these can contain values, such as text, numbers, formulas and functions. Cells may also be formatted with attributes such as color or text styles. Below is an example of some data entered in a spreadsheet (Figure 3.1).\n\n\n\nFigure 3.1: Example entries from an Excel spreadsheet\n\n\nCell B6 contains a simple formula: = C6 + D6. This formula adds cells C6 and D6 resulting in the sum, 8. In formulas, mathematical operators can be used (\\(+, -, \\times , \\div\\) ). Formulas can be also extended with inbuilt function such as showed in Table 3.1.\n\n\nTable 3.1: Often used functions in excel\n\n\nFunction\nEnglish\nNorwegian\n\n\n\n\nSum\nSUM()\nSUMMER()\n\n\nAverage\nAVERAGE()\nGJENNOMSNITT()\n\n\nStandard deviation\nSTDEV.S()\nSTDEV.S()\n\n\nCount\nCOUNT()\nANTALL()\n\n\nIntercept\nINTERCEPT()\nSKJÆRINGSPUNKT()\n\n\nSlope\nSLOPE()\nSTIGNINGSTALL()\n\n\nIf\nIF()\nHVIS()\n\n\n\n\nThe sum, average, standard deviation, and count are simple functions for summarizing data. Intercept and slope are functions used to get simple associations from two sets of numbers (based on a regression model). The IF function is an example of a function that can be used to enter data in a cell conditionally. For example, IF cell A1 contains a certain number, then cell B1 should display another specified text.\nWhen looking for tips and tricks online, you may come across functions for excel in other languages than what is installed on your computer. To translate functions and for a complete overview of functions included in Microsoft Excel, see this website en.excel-translator.de/."
  },
  {
    "objectID": "02-spreadsheets.html#tidy-data-and-data-storage",
    "href": "02-spreadsheets.html#tidy-data-and-data-storage",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.2 Tidy data and data storage",
    "text": "3.2 Tidy data and data storage\nHadley Wickham (the author of many commonly used R packages) quotes Tolstoy (Figure 3.2) when describing the principle of tidy data (Wickham 2014). This quote is so famous that it has given name to a principle. The principle in turn comes in many variants but basically states that when something goes wrong, it can be wrong in multiple ways. But when it is right/correct/works/succeeds, it does so in only one way1. This principle can be applied to data sets. There are so many ways that formatting of data sets can be problematic, but a limited set of principles makes it good.\n\n\n\nFigure 3.2: Leo Tolstoy at the time when he was (possibly) authoring Anna Karenina. (Source: https://en.wikipedia.org/wiki/Leo_Tolstoy)\n\n\nA tidy data set consists of values originating from observations and belonging to variables. A variable is a definition of the values based on attributes. An observation may consist of several variables (Wickham 2014).\nA tidy data set typically has one observation per row and one variable per column. Let’s say that we want to collect data from a strength test. A participant (participant is a variable) in our study conducts tests before and after the intervention (time is a variable) in two exercises (exercise is a variable), and we record the maximal strength in kg (load is a variable). The data set will look like the table below (Table 3.2).\n\n\nTable 3.2: Example of tidy data\n\n\nParticipant\nTime\nExercise\nLoad\n\n\n\n\nBruce Wayne\npre\nBench press\n95\n\n\nBruce Wayne\npost\nBench press\n128\n\n\nBruce Wayne\npre\nLeg press\n180\n\n\nBruce Wayne\npost\nLeg press\n280\n\n\n\n\nAnother example contains variables that actually carries two pieces of information in one variable. We again did a strength test, this time as maximal isometric contractions and in each test consisted of two attempts. We record this in two different variables, attempt 1 and 2. The resulting data set could look something like in Table Table 3.3.\n\n\nTable 3.3: Another example of tidy data.\n\n\nParticipant\nTime\nExercise\nAttempt1\nAttempt2\n\n\n\n\nSelina Kyle\npre\nIsometric\n81.3\n92.5\n\n\nSelina Kyle\npost\nIsometric\n97.1\n114.1\n\n\n\n\nTo make this data set tidy we need to extract the attempt information and record it in another variable as seen in Table 3.4.\n\n\nTable 3.4: A third example of tidy data.\n\n\nParticipant\nTime\nExercise\nAttempt\nload\n\n\n\n\nSelina Kyle\npre\nIsometric\n1\n81.3\n\n\nSelina Kyle\npre\nIsometric\n2\n92.5\n\n\nSelina Kyle\npost\nIsometric\n1\n97.1\n\n\nSelina Kyle\npost\nIsometric\n2\n114.1\n\n\n\n\nThis transformation naturally gives additional rows to the data set. It is sometimes referred to as “long format” data instead of the structure where each attempt is given separate variables, called “wide format.” You will notice during the course that the long format is most convenient for most purposes. This is true when we create graphs and do statistical modeling. But sometimes, a variable must be structured in a wide format to allow certain operations.\nIf we follow what is recommended by Broman and Woo (Broman and Woo 2018), it is clear that each cell in a spreadsheet should only contain one value. If we, for example, decide to format a cell to a certain color, we add data to that cell on top of the actual data. You might add color to a cell to remember to add or change data. However, this information is lost when you use the data set in other software. Instead, you should add another variable to allow such data to be properly recorded. Using a variable called comments, you can add text describing information about that particular observation, information that is not lost when you use the data set in another software."
  },
  {
    "objectID": "02-spreadsheets.html#recording-data",
    "href": "02-spreadsheets.html#recording-data",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.3 Recording data",
    "text": "3.3 Recording data\nA trade secret2 from people who work all day with data and programming is that they are lazy. Lazy in the sense that you want to type as little as possible and avoid moving your arm to the computer mouse whenever possible. When recording data, we can be lazy too. We can do this by shortening variable names and not using CAPITAL letters when entering text in data storage. After a hard day at the keyboard, you will be happy to write strtest instead of Strength Test. The extra effort of using two capital letters might be the thing to tip you over the edge (Figure 3.33). However, we should not be too lazy either; variable names and values should be “short but meaningful” (Broman and Woo 2018).\n\n\n\nFigure 3.3: D-FENS Foster gets pushed over the edge (Source: https://en.wikipedia.org/wiki/Falling_Down)\n\n\nData and variables should also be consistent. Do not mix data type; use a consistent way of entering e.g., dates and time, and do not use spaces or special characters. To enforce this, you might want to start your data collection by writing up a data dictionary describing all variables you collect. The dictionary can set the rules for your variables. This dictionary can also guide your data validation.\nIn Excel, you can use data validation to set rules for data entry. For example, if you have a numeric variable, you can set Excel only to accept numbers in a specified set of cells. Such rules make it harder to enter erroneous data."
  },
  {
    "objectID": "02-spreadsheets.html#saving-data",
    "href": "02-spreadsheets.html#saving-data",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.4 Saving data",
    "text": "3.4 Saving data\nData from spreadsheets can be saved as special spreadsheet files, such as .xlsx. This format allows for functions, multiple spreadsheets in the same file (tabs), and cell formatting. You do not need this fancy format if you follow the tips described above and in (Broman and Woo 2018). Instead, you can store your data as a .csv file. This format may be read and edited with Excel (or another spreadsheet software) and in plain text. Data entered in this format (comma-separated values; csv) can look like this in a text editor:\n Participant;Time;Exercise;Attempt;load\n Selina Kyle;pre;Isometric;1;81.3  \n Selina Kyle;pre;Isometric;2;92.5\n Selina Kyle;post;Isometric;1;97.1\n Selina Kyle;post;Isometric;2;114.1\nThis format is quite lovely. The data takes little space; the simple format requires that data is well documented using e.g., a data dictionary; and it is available for many other software as the format is simple. You can document the data using a README file that could describe the purpose and methods of data collection, how the data is structured, and what kind of data the variables contains. A simple README file can be written in a text editor such as Notepad and saved as a .txt file. Later in this course, we will introduce a “markup” language often used to create README files containing a syntax that formats the text to a more pleasant style when converted to other formats."
  },
  {
    "objectID": "02-spreadsheets.html#references-and-footnotes",
    "href": "02-spreadsheets.html#references-and-footnotes",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "3.5 References and footnotes",
    "text": "3.5 References and footnotes\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” Journal Article. The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\nStephen, G. Powell, R. Baker Kenneth, and Lawson Barry. 2009. “Errors in Operational Spreadsheets.” Journal Article. Journal of Organizational and End User Computing (JOEUC) 21 (3): 24–36. https://doi.org/10.4018/joeuc.2009070102.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal Article. Journal of Statistical Software; Vol 1, Issue 10 (2014). https://www.jstatsoft.org/v059/i10.\n\n\nZiemann, Mark, Yotam Eren, and Assam El-Osta. 2016. “Gene Name Errors Are Widespread in the Scientific Literature.” Journal Article. Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7."
  },
  {
    "objectID": "02-spreadsheets.html#footnotes",
    "href": "02-spreadsheets.html#footnotes",
    "title": "3  Storing data in spreadsheets and understanding tabular data",
    "section": "",
    "text": "See https://en.wikipedia.org/wiki/Anna_Karenina_principle↩︎\nA trade secret as in “not generally known to the public”. See en.wikipedia.org/wiki/Trade_secret.↩︎\nIn the movie Falling Down, Michael Douglas plays a unemployed engineer who gets push over edge, would it have been enough with a few to many capital letters?↩︎"
  },
  {
    "objectID": "03-intro-to-r.html#the-anatomy-of-rstudio",
    "href": "03-intro-to-r.html#the-anatomy-of-rstudio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.1 The Anatomy of RStudio",
    "text": "4.1 The Anatomy of RStudio\nThe appearance of RStudio can be changed for a more pleasant user experience. I like a dark theme as it is easier on the eye. We can also move the different components of RStudio. I like to have The console on the top right and the source on the top left. I think this makes it easier to see output when coding interactively.\nAll this will be clearer as thing evolve, but for now, start R Studio, go to Tools &gt; Global options and make it personal (see Figure 4.1)!\n\n\n\nFigure 4.1: Customize the appearance of RStudio\n\n\nAs you may have spotted in the image above, it is possible to change the font of your editor. I like Fira code.\n\n\n\n\n\n\nDefining concepts\n\n\n\nSource editor: Is where scripts are edited.\nEnvironment: In R, the environment is where data variables and structures are saved during execution of code.\nScript: Your script is the document containing your computer code. This is your computer program (using a loose definition of a software program).\nVariables: In R, variables are containers for data values.\nWorkspace: This is your environments as represented on your computer. A workspace can be, but should not be saved between sessions.\n\n\n\n4.1.1 The source editor\nThe source editor is where you edit your code. When writing your code in a text-file, you can call it a script, this is essentially a computer program where you tell R what to do. It is executed from top to bottom. You can send one line of code, multiple lines or whole sections into R. In the image below (Figure 4.2), the source window is in the top left corner.\n\n\n4.1.2 Environment\nThe environment is where all your objects are located. Objects can be variables or data sets that you are working with. In RStudio the environment is listed under the environment tab (bottom left in the image).\nCopy the code below to a R script. To run it line by line, set your cursor on the first line a press Ctrl+Enter. What happened in your environment? Press Ctrl+Enter again and you will see a plot in the plot window. Amazing stuff!\n\na &lt;- c(1, 2, 3, 4)\n\nplot(a)\n\n\n\n4.1.3 The console\nBy pressing Ctrl+Enter from the script, as described above, you sent your code to the console. You can also interact with R directly here. By writing a in the console and hitting enter you will get the value from the object called a. This means that it is also where output from R is usually printed. In the image below, the console is in the top right corner.\n\n\n4.1.4 Files, plots, packages and help files\nIn RStudio files are accessible from the Files tab. The files tab shows the files in you root folder. The root folder is where R will search for files if you tell it to. We will talk more about the root folder later in connection with projects. Plots are displayed in the Plot tab. Packages are listed in the packages tab. If you access the help files, these will be displayed in the help tab. In the image below all these tabs are in the bottom right corner. More on help files and packages later.\n\n\n\nFigure 4.2: Interacting with RStudio"
  },
  {
    "objectID": "03-intro-to-r.html#reproducible-data-science-using-rstudio",
    "href": "03-intro-to-r.html#reproducible-data-science-using-rstudio",
    "title": "4  Getting to know R and RStudio",
    "section": "4.2 Reproducible data science using RStudio",
    "text": "4.2 Reproducible data science using RStudio\nWhen starting to work more systematically in RStudio we will set some rules that will allow for reproducible programming. Remember from Chapter 2 that part of a fully reproducible study is software/code that produces the results. It turns out that when working interactively with R you can fool yourself to belive that you have included all steps needed to produce some results in your script. However, variables may be stored in your environment but not by assigning values to them in your script. This will become a problem if you want to share your code, a certain value/variable needed to make the program work may be missing from your script.\nTo avoid making such a mistake it is good practice not to save variables in your environment between sessions, everything should be scripted and documented and assumed not defined elsewhere. In RStudio we can make an explicit setting Not to save the workspace (See Figure 4.3).\n\n\n\nFigure 4.3: Never save the workspace."
  },
  {
    "objectID": "03-intro-to-r.html#basics-r-programming-installing-and-using-swirl",
    "href": "03-intro-to-r.html#basics-r-programming-installing-and-using-swirl",
    "title": "4  Getting to know R and RStudio",
    "section": "4.3 Basics R programming, Installing and using swirl",
    "text": "4.3 Basics R programming, Installing and using swirl\nSwirl is a great way to get to know how to talk with R. Swirl consists of lessons created for different topics. Install swirl by typing the following into your console:\n\ninstall.packages(\"swirl\")\n\nWhen swirlis installed you will need to load the package This means that all functions that are included in package becomes available to you in your R session. To load the package you use the library function.\n\nlibrary(\"swirl\")\n\nWhen you run the above command in your console you will get a message saying to call swirl() when you are ready to learn. I would like you to run the course “R Programming: The basics of programming in R”. Swirl will ask if you want to install it. After installation, just follow the instructions in the console. To get out of swirl, just press ESC."
  },
  {
    "objectID": "03-intro-to-r.html#file-formats-for-editing-and-executiong-r-code",
    "href": "03-intro-to-r.html#file-formats-for-editing-and-executiong-r-code",
    "title": "4  Getting to know R and RStudio",
    "section": "4.4 File formats for editing and executiong R code",
    "text": "4.4 File formats for editing and executiong R code\n\n4.4.1 R scripts\nRStudio has capabilities to highlight code for multiple languages. We will focus on R. The most basic file format for R code is an R script, as we have already touched upon. An R script contains code and comments. Code is executed by R and comments are ignored. Ideally, R scripts are commented to improve readability of what the do. Commenting code is also a good way of creating a roadmap of what you want to do.\nIn the image below (Figure 4.4), R code is written based on a plan written with comments. Note that when a line starts with at least one # it is interpreted by R as a comment.\n\n\n\nFigure 4.4: Commenting and coding in an R script\n\n\nTry the code for yourself to see what it produces. The details will be covered later.\n\n## Create two vectors of random numbers\nx &lt;- rnorm(10, 0, 1)\ny &lt;- rnorm(10, 10, 10)\n\n## Create an x-y plot of the two vectors\nplot(x, y)\n\n\n\n4.4.2 R markdown and quarto files\nThe more advanced file formats for R are RMarkdown (.rmd) and quarto (.qmd) files. These have the capabilities of combining formatted text with computer code. The source document may contain multiple pieces of code organized in code chunks together with text formatted with markdown syntax. A meta data field in the top of the source file specifies settings for the conversion to output formats. Multiple output formats are available, including HTML, word and PDF. The image below shows the basic outline of a very simple quarto file destined to create a HTML document.\nNotice also that RStudio offers an visual editor where the output is approximated and formatting is available from a menu.\nAdding headlines and makes it possible to navigate the document through the outline or the list of components in the bottom of the document.\n\n\n\nAuthoring in a quarto source document and preview in the visual editor\n\n\nR markdown and quarto have many similarities as the basic organization is similar between the two. The text parts are written using a special syntax, markdown. The point of markdown is that you will use the same syntax that is later possible to convert to multiple formats. The syntax let’s you do all formatting explicitly, for example instead of getting your mouse to superscript some text you can add syntax a^2^ to achieve a2.\nA full guide to RMarkdown can be found on the official R markdown web pages. I suggest you take the time to get an overview of this language as it will make you more fluent in the tools that enables reproducible computing. When writing R markdown, it is handy to have a cheat sheet close by when writing, here is an example for Rmarkdown, and here is another one for quarto 1.\n\n4.4.2.1 Microsoft Word intergration in R Markdown and Quarto\nSometimes it is useful to “knit” to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown/Quarto can be used as a source for word documents (.docx).\nTo create a word document from your Rmd-file/qmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: word_document\n---\nThe output: word_document (or format: docx when using quarto) tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use this as reference. See here for the equivalent formatting of quarto documents\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\n---\nEdit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the “Title” style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it.\nWhen you knit the document again, your updated styles will be used your word document.\nHere you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here.\n\n\n4.4.2.2 Adding references to R Markdown and Quarto files\nReferences/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are availiable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib.\nThe bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information:\n---\ntitle: \"A title\"\nauthor: Daniel Hammarström\ndate: 2020-09-05\noutput: \n        word_document:\n                reference_docx: reference.docx\nbibliography: bibliography.bib\n---\nAdd citations to the file in bibtex-format. Here is an example:\n@Article{refID1,\n   Author=\"Ellefsen, S.  and Hammarstrom, D.  and Strand, T. A.  and Zacharoff, E.  and Whist, J. E.  and Rauk, I.  and Nygaard, H.  and Vegge, G.  and Hanestadhaugen, M.  and Wernbom, M.  and Cumming, K. T.  and Rønning, R.  and Raastad, T.  and Rønnestad, B. R. \",\n   Title=\"{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}\",\n   Journal=\"Am. J. Physiol. Regul. Integr. Comp. Physiol.\",\n   Year=\"2015\",\n   Volume=\"309\",\n   Number=\"7\",\n   Pages=\"R767--779\",\n   Month=\"Oct\"}\nThe part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form\nBlood flow-restricted training leads to similar adaptations as traditional training [@refID1].\nThis will appear in text as:\n\nBlood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015).\n\nThe reference will end up in the end of the document (as on this webpage).\nYou can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with “scandinavian” letters (å æ ä ø ö).\nRecently RStudio added support for adding citations inside the visual markdown editor."
  },
  {
    "objectID": "03-intro-to-r.html#packages",
    "href": "03-intro-to-r.html#packages",
    "title": "4  Getting to know R and RStudio",
    "section": "4.5 Packages",
    "text": "4.5 Packages\nThe R ecosystem consists of packages. These are functions organized in a systematic manner. Functions are created to perform a specialized task. And packages often have many function used to do e.g. analyses of a specific kind of data, or more general task such as making figures or handle data.\nIn this course we will use many different packages, for example dplyr, tidyr and ggplot2. dplyr and tidyr are packages used to transform and clean data. ggplot2 is used for making figures.\nTo install a package, you use the install.packages() function. You only need to do this once on your computer (unless you re-install R). You can write the following code in your console to install dplyr.\n\ninstall.packages(\"dplyr\")\n\nAlternatively, click “Packages” and “Install” and search for the package you want to install. To use a package, you have to load it into your environment. Use the library() function to load a package.\n\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "03-intro-to-r.html#references-and-footnotes",
    "href": "03-intro-to-r.html#references-and-footnotes",
    "title": "4  Getting to know R and RStudio",
    "section": "4.6 References and footnotes",
    "text": "4.6 References and footnotes\n\n\n\n\nEllefsen, S., D. Hammarstrom, T. A. Strand, E. Zacharoff, J. E. Whist, I. Rauk, H. Nygaard, et al. 2015. “Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training.” Am. J. Physiol. Regul. Integr. Comp. Physiol. 309 (7): R767–779."
  },
  {
    "objectID": "03-intro-to-r.html#footnotes",
    "href": "03-intro-to-r.html#footnotes",
    "title": "4  Getting to know R and RStudio",
    "section": "",
    "text": "Cheat sheets are available in R Studio: Help &gt; Cheatsheets↩︎"
  },
  {
    "objectID": "04-first-graph.html#resources",
    "href": "04-first-graph.html#resources",
    "title": "5  Creating your first graph",
    "section": "5.1 Resources",
    "text": "5.1 Resources\nThere are several good resources aimed at ggplot2:\n\nChapter 2 in R for data science\nThe ggplot2 book\nThe ggplot2 cheatsheet"
  },
  {
    "objectID": "04-first-graph.html#learning-objectives",
    "href": "04-first-graph.html#learning-objectives",
    "title": "5  Creating your first graph",
    "section": "5.2 Learning objectives",
    "text": "5.2 Learning objectives\nAfter working through this chapter, you should be able to answer:\n\nWhat are geoms?\nWhat is mapping data to aesthetics?\nWhat are theme components?\n\nYou should also be able to create your first graph."
  },
  {
    "objectID": "04-first-graph.html#prerequisites",
    "href": "04-first-graph.html#prerequisites",
    "title": "5  Creating your first graph",
    "section": "5.3 Prerequisites",
    "text": "5.3 Prerequisites\nTo follow the exercises below you will need to some data. For the purpose of this course, I have created a package that contains the data sets we need. In this chapter we will work with the cyclingstudy data set. To install the package (exscidata) you will need another package called remotes.\nThe code below first checks if the package remotes is installed, or more specifically, if \"remotes\" cannot be found in the list of installed packages. Using the if function makes install.packages(remotes) conditional. If we do not find \"remotes\" among installed packages, then install remotes.\nThe next line of code does the same with the exscidata package. However, since the package is not on CRAN but hosted on GitHub we will need to use remotes to install it. The part of the second line of code that says remotes::install_github(\"dhammarstrom/exscidata\") uses the function install_github without loading the remotes package. The last line of the code below loads the package exscidata using the library function.\n\n# Check if remotes is not installed, if TRUE, install remotes\nif (!\"remotes\" %in% installed.packages()) install.packages(remotes)\n\n# Check if exscidata is not installed, if TRUE, install exscidata from github\nif (!\"exscidata\" %in% installed.packages()) remotes::install_github(\"dhammarstrom/exscidata\")\n\n# Load exscidata\nlibrary(exscidata)\n\nNext we need to load the tidyverse package. This package in turn loads several packages that we will use when transforming data and making our figures. I will include the line of code that checks if the package is installed, if not, R will download and install it. We subsequently load the package using library.\n\n# Check if tidyverse is not installed, if TRUE, install remotes\nif (!\"tidyverse\" %in% installed.packages()) install.packages(tidyverse)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe are now ready to explore the data set. But first we should talk about the main components of the ggplot2 system."
  },
  {
    "objectID": "04-first-graph.html#the-ggplot2-system",
    "href": "04-first-graph.html#the-ggplot2-system",
    "title": "5  Creating your first graph",
    "section": "5.4 The ggplot2 system",
    "text": "5.4 The ggplot2 system\nWhen using the ggplot2 system we can think of the resulting graph as containing data that has been mapped to different coordinates, colors, shapes, sizes, and other attributes that determine what is being visualized. We are using different geometric representations of the data in the visualization.\nWhen we map data in ggplot we use a specific function, aes() (short for aesthetic). We will use this inside the main engine, ggplot(). For this first simple example, we will create a data set by simulating some data. When you simulate data in R, you can tell R what should be the starting point in the random number generator. Using set.seed(100), we can recreate the same numbers from our “number generator” later. In the example below, we use rnorm() to simulate numbers from a normal distribution. Using the arguments n = 10, mean = 0, and sd = 1, we simulate randomly picking ten numbers from a distribution with a mean of 0 and a standard deviation of 1. These numbers are stored in a data frame that is assigned to an object that we have named dat.\n\n# Set the seed for random generation of numbers\nset.seed(100)\n\n# Store data in a data frame\ndat &lt;- data.frame(x = rnorm(10, mean = 0, sd = 1), \n                  y = rnorm(10, mean = 10, sd = 2))\n\nThe data set consists of two variables. We will start the process of creating the graph by creating the canvas, and this basically sets the border of the figure we want to make. The ggplot() function takes the data set as its first argument, followed by the aes() function that maps data to coordinates and other attributes. In this case, we have mapped our data to the x- and y-coordinates of the figure.\n\nggplot(dat, aes(x = x, y = y))\n\n\n\n\nFigure 5.1: An empty ggplot canvas.\n\n\n\n\nAs you can see in Figure 5.1, the code above creates an “empty canvas” that has enough room to visualize our data. The x- and y-axes are adjusted to give room for graphical representations of the data. Next we need to add geometric shapes (geom for short). These are functions that we add to the plot using the + sign. These functions all start with geom_ and has and ending that describes the geometric shape, like for example point or line.\nWe will add geom_point() to our empty canvas as plotted in Figure 5.1. The geom_point function inherits the mapping from from ggplot(). Shapes, in this case points will be placed according to x- and y-coordinates specified in aes() used in the main ggplot function call. This means that we do not need to specify anything in geom_point at this stage.\n\nggplot(dat, aes(x = x, y = y)) + geom_point()\n\n\n\n\nFigure 5.2: A ggplot canvas with points added.\n\n\n\n\nIn Figure 5.2 we have added black points to each x- and y-coordinate representing x and y from our data set.\nTo extend the example we will add data to our data set. In the code below, we create a new variable in the data set using $ effectively giving us a new column in the data. We use rep(\"A\", 5) to replicate the letter A five times and the same for B. The c() function combines the two in a single vector. We can use head(dat) to see what we accomplished with these operations. The head() function prints the first six rows from the data set.\n\ndat$z &lt;- c(rep(\"A\", 5), rep(\"B\", 5))\n\nhead(dat)\n\n            x         y z\n1 -0.50219235 10.179772 A\n2  0.13153117 10.192549 A\n3 -0.07891709  9.596732 A\n4  0.88678481 11.479681 A\n5  0.11697127 10.246759 A\n6  0.31863009  9.941367 B\n\n\nWe can see that we have an additional variable, z that contains the letters \"A\" and \"B\". This new variable can be used to add more information to the plot. Let’s say that we want to map the z variable to different colors. We do this by adding color = z to aes. This means that we want the z variable to determine colors.\n\nggplot(dat, aes(x = x, y = y, color = z)) + geom_point()\n\n\n\n\nFigure 5.3: A ggplot canvas with colored points added.\n\n\n\n\nIn Figure 5.3 we can see that different colors are used for the two letters \"A\" and \"B\". Other attributes can also be specified like shape, fill or size. The shape specifies the appearance of the points. When we use use data to map to shapes, ggplot2 will start from the standard shape.\n\n\n\n\n\nFigure 5.4: Shapes in R\n\n\n\n\nPossible shapes in the standard framework in R are shown in Figure 5.4. Shapes 0 to 20 can change colors while shapes 21 to 25 may have different border colors but also different fill colors. We may use this information to change the shape, color and fill of our points. Let’s say that instead of colored points we want filled points. We would then change the color = z argument to fill = z and select a point shape that can be filled (shapes 21-25, see Figure 5.4. Notice in the code below that shape = 21 has been added to geom_point(). We have specified how points should be displayed.\n\nggplot(dat, aes(x = x, y = y, fill = z)) + geom_point(shape = 21)\n\n\n\n\nFigure 5.5: A ggplot canvas with filled points added.\n\n\n\n\nSince shape is an attribute we can map data to it. If we want data to determine both shape and fill we could add this information in the aes() function by setting both shape = z and fill = z. We now have to specify what shapes ggplot should use in order to be sure we can combine both shapes and fill. We will use scale_fill_manual and scale_shape_manual to do this. These functions lets you specify different values for aesthetics. Notice that we removed shape = 21 from the geom_point() function, but we added size to increase the size of the points (see Figure 5.6).\n\nggplot(dat, aes(x = x, y = y, fill = z, shape = z)) + \n  geom_point(size = 3) +\n  scale_fill_manual(values = c(\"red\", \"green\")) + \n  scale_shape_manual(values = c(21, 23))\n\n\n\n\nFigure 5.6: Data mapped to fill and shape, and size specified manually to override the default."
  },
  {
    "objectID": "04-first-graph.html#different-geoms-using-real-data",
    "href": "04-first-graph.html#different-geoms-using-real-data",
    "title": "5  Creating your first graph",
    "section": "5.5 Different geoms using real data",
    "text": "5.5 Different geoms using real data\nWe have seen that the basic ggplot2 figure maps underlying data to coordinates and geometric representations, such as points. We will go further by using some real data. We will be using the cyclingstudy data set from the exscidata-package. We will start by loading the data and select a few columns that we are interested in.\nBy using data(\"cyclingstudy\") we will load the data set that is part of the exscidata-package to our environment. By looking at the environment tab you can see that this operation adds a data set to the environment. It has 80 observations and 101 variables. Using the glimpse() function from dplyr (which is loaded by loading tidyverse) we will get an overview of all variables in the data set. I have omitted the output from the code below, feel free to run the code in a quarto- or rmarkdown-document on your own.\n\n# Load the data and have a first look\ndata(\"cyclingstudy\")\nglimpse(cyclingstudy)\n\nWe will store a selected set of variables in a new object for ease of use. We will call this object cycdat. We select variables using the function with the very suitable name select where the first argument specifies the data set, following arguments specifies what variables we want. Let’s say that we are interested in squat jump height. The exscidata package comes with descriptions of the data sets. By writing ?cyclingstudy in your console you will see the description of the data in your help tab. Squat jump is recorded as sj.max, we select this variable together with subject, group and timepoint to create a smaller data set.\n\n# Assign a selected set of variables to a smaller data set\ncycdat &lt;- select(cyclingstudy, subject, group, timepoint, sj.max)\n# Printing the data set\ncycdat\n\n# A tibble: 80 × 4\n   subject group timepoint sj.max\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1       1 INCR  pre         31.0\n 2       2 DECR  pre         31.6\n 3       3 INCR  pre         26.8\n 4       4 DECR  pre         29.2\n 5       5 DECR  pre         31.2\n 6       6 INCR  pre         34.2\n 7       7 MIX   pre         30.1\n 8       8 MIX   pre         32.8\n 9       9 MIX   pre         22.7\n10      10 INCR  pre         29.7\n# ℹ 70 more rows\n\n\nBy printing the object we can see that we have a tibble of 80 rows and 4 columns. A tibble can to a large extent be regarded as a data frame, and we will use these words interchangeably. Tibbles are new in the sense that they are developed as part of the tidyverse (Wickham and Grolemund 2017) 1. Printing a tibble will display the first 10 rows as we can see from the resulting output.\n\n5.5.1 A plot of values per group\nLet’s say that we want to see how the values differs between groups. Box-plots are a good way to start as they will bring a standardized way of summarizing data. Box-plots can be plotted using the geom_boxplot function. Notice below that we put group on the x-axis (the first argument in the aes function) and sj.max on the y-axis. By doing so ggplot will make the x-axis discrete and the y-axis continuous.\n\n# Creating a box-plot of all values per group\nggplot(cycdat, aes(group, sj.max)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.7: Boxplot of all data per group from the cycling dataset.\n\n\n\n\nWe can add layers of more geoms to the same plot. We might want to add individual data points also. geom_jitter might be a good place to start. This geom is good as it can be plotted over a group variable and points gets “jittered” or spread so we avoid overlap.\n\n# Creating a boxplot of all values per group\nggplot(cycdat, aes(group, sj.max)) + geom_boxplot() + geom_jitter()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 5.8: Box-plot and jittered points of all data per group from the cycling dataset.\n\n\n\n\nNotice that we get warnings saying that there are some data missing, these values are removed from the calculation of summary statistics in the box-plots and omitted from plotting of the points.\n\n\n5.5.2 Data over time per group and individual\nIn the data set we have a time variable consisting of the labels “pre”, “meso1”, “meso2” and “meso3”. When we load the data into R we do so without providing information about the order of these labels. R will put them in alphabetical order when order is required (as in a figure). If we want to plot these data in the right order, we have to tell R that these data should have an order. We will convert the timepoint variable to a factor. Factors are variables that can contain more information than what is contained in each cell. Using the factor function we will set the order of the timepoint variable. We assign this transformation of the variable to its original place in the data frame.\n\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"))\n\nWe are now ready to plot data over time, where the time variable is correctly ordered. Let’s use the box-plot again to plot all values over time.\n\n# Creating a boxplot of all values per time point\nggplot(cycdat, aes(timepoint, sj.max)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.9: Boxplot of all data per time-point from the cycling dataset.\n\n\n\n\nWe do not see any great tendencies in the whole data set. To further explore the data we might want to have different boxes per group per time. We can accomplish this by adding fill = group to our aes function.\n\n# Creating a boxplot of all values per group over time\nggplot(cycdat, aes(timepoint, sj.max, fill = group)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigure 5.10: Boxplot of all data per time-point and group from the cycling dataset.\n\n\n\n\nThis is possible because geom_boxplots can be filled. The same separation of groups would have been accomplished using color = group, however, then the boxes would get different border colors instead. You might have noticed that the box-plots do not contain all the data, a few data points are outside \\(1.5 \\times IQR\\) (interquartile range). This, by standard definitions, defines the data point as an “outlier”.\nAs mentioned above, box-plots does some summarizing and not all data is shown. To explore further we might want to track every participant. To do this we have to tell ggplot how to group the data. In aes() the group argument let’s you connect lines based on some grouping variable, in our case it will be subject. We will use a line to connect each participants score over time. Using color = group will additionally give every line a different color depending on which group it belongs to.\n\n# Creating a line plot of all values per participant over time, color per group\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \ngeom_line()\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.11: Figure with lines corresponding to indivudal values per participant.\n\n\n\n\nIn Figure 5.11, each line represents a participant, different colors represents different groups.\n\n\n5.5.3 Titles and labels\nOften we need to add information to the plot to better communicate its message. Such information could be appropriate titles on axes and legends and extra text needed to explain aspects of the plot. Using the labs() function we can add information that will replace variable names that are being used for all variables that have been mapped in the figure. In the figure below we will start by adding better axis titles. This information goes into x and y in labs() which simply changes the titles of the x- and y-axis.\n\n# Creating a line plot of all values per participant over time, color per group, \n# adding axis labels\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.12: Figure with updated axis labels\n\n\n\n\nThe resulting Figure 5.12 now have better titles for each axis. Notice in the code above that titles needs to be specified with quotation marks. This is a tricky aspect of R, if we would have omitted the quotation marks we would have told R to look for objects by the name of e.g. Time-point, and this would actually mean that we tried to subtract time from point since - is interpreted as a minus sign.\nWe might want to add information to the legend also. Since we specified color = group in the aes() function, the same can be manipulated in labs. Lets just add a capital G.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.13: Additional labels\n\n\n\n\nWe still have the original labels for the time variable. Remember that we used the factor function above to set the order of the labels. Actually we specified the “levels” of the factor. We can use the same function to add better “labels”. In the code below, I will first change the variable in the data set and then use the exact same code for the plot.\n\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"), \n                           labels = c(\"Pre-training\", \"Meso-cycle 1\", \"Meso-cycle 2\", \"Meso-cycle 3\"))\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\")\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.14: Changing labels by changing a factor variable prior to plotting\n\n\n\n\nThe same goes for the group variable. You can try to change the levels and labels of the grouping variable to make it more descriptive. You can type ?cyclingstudy in your console to read about the group variable and then use this information to write better labels using the factor function. In the factor function, the first argument is the variable you want to use as basis of your new factor, the second argument you need to specify is levels which sets the order and lastly you will need to set the labels for each level using labels =. If you write ?factor in your console you will get the help pages for the factor function.\nClick here to display a possible solution\n\n\n# Change the grouping variable\ncycdat$group &lt;- factor(cycdat$group, levels = c(\"DECR\", \"INCR\", \"MIX\"), \n                           labels = c(\"Decreased\\nintensity\", \n                                      \"Increased\\nintensity\", \n                                      \"Mixed\\nintensity\"))\n\n# Plotting the data as before with the new information added\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\")\n\nNote: Adding \\n in the the text string breaks the line to get two rows.\n\n\n\n5.5.4 Annotations\nAnnotation may become handy when you want to add elements to the graph that is not in the data set. Using ggplot2, annotations are added using the annotate() function. This function first needs to be specified with a geom, these are commonly text or lines or segments. In the code chunk below are several examples of annotations. First I save the plot as an object called myplot and then add different annotations to it.\n\nmyplot &lt;- ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\") \n\n\n# A text annotation\nmyplot + annotate(\"text\", x = 1, y = 37, label = \"This is an annotation\")\n\n# A line/segment \nmyplot + annotate(\"segment\", x = 1, xend = 3, y = 25, yend = 35,  colour = \"red\", size = 4)\n\nYou can copy the code and run it yourself to see the results. annotate is documented here but documentation can also be accessed by typing ?annotate in your console. Try to read the documentation and add a transparent rectangle to a previous plot.\nClick here for a solution\n\n\n# Change the grouping variable\ncycdat$group &lt;- factor(cycdat$group, levels = c(\"DECR\", \"INCR\", \"MIX\"), \n                           labels = c(\"Decreased\\nintensity\", \n                                      \"Increased\\nintensity\", \n                                      \"Mixed\\nintensity\"))\n\n# Plotting the data as before with the new information added\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Periodization strategy\") +\n  # A rectangular annotation (alpha = 0.4 makes the rectangle transparent)\n annotate(\"rect\", xmin = 1, xmax = 2, ymin = 30, ymax = 35, alpha = 0.4)\n\nNote: Adding \\n in the the text string breaks the line to get two rows."
  },
  {
    "objectID": "04-first-graph.html#themes",
    "href": "04-first-graph.html#themes",
    "title": "5  Creating your first graph",
    "section": "5.6 Themes",
    "text": "5.6 Themes\nThemes in ggplot2 can be used to change everything else about the plot concerning text, colors etc. ggplot2 has some built in themes that are easily activated by adding them to the plot. For example the theme_bw() function will change the theme to a black and white one as in the figure below.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\") + \n  theme_bw() # Adding a pre-specified theme\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.15: A figure using the black and white theme from theme_bw.\n\n\n\n\nA collection of built in themes are documented here. Individual components of the theme can also be changed using the theme() function. There is a long list of theme components that can be changed using this function. The list can be found here.\nIf we put the theme function last in the ggplot call we will modify the existing theme. Let’s say that we want to change the color of the text on the x axis.\n\nggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + \n  geom_line() +\n  labs(x = \"Time-point\",\n       y = \"Squat jump height (cm)\", \n       color = \"Group\") + \n  theme_bw() +\n  theme(axis.text.x = element_text(color = \"steelblue\", size = 12, face = \"bold\"))\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\nFigure 5.16: A figure using the black and white theme from theme_bw, with modifications\n\n\n\n\nThe component axis.text.x can be modified using a function that changes appearance of text components, namely element_text. Similarly, other components are changed with specific functions for lines and rectangular shapes (see the help pages for theme)."
  },
  {
    "objectID": "04-first-graph.html#test-your-understandning",
    "href": "04-first-graph.html#test-your-understandning",
    "title": "5  Creating your first graph",
    "section": "5.7 Test your understandning",
    "text": "5.7 Test your understandning\nIn this section you can try to implement what we have discussed above. An example solution exists below each figure by press of button.\nIn Figure 5.17, I have used the VO2max data from the cyclingstudy data set. I have made changes to the time variable (timepoint) to make the labels better. I have added a title to the figure and changed the appearance of the text. I will use an extra package called (ggtext)[https://wilkelab.org/ggtext/index.html] to make it possible to use markdown syntax in axis labels. In order to use ggtext you have to install it from CRAN.\n\n\n\n\n\nFigure 5.17: Example figure 1\n\n\n\n\nClick for a solution\n\n\n# Load the package ggtext to make markdown avalable in axis labels.\nlibrary(ggtext) \n\n# For ease of use I save a smaller dataset in a new object\ncycdat &lt;- select(cyclingstudy, subject, timepoint, VO2.max)\n\n# Change the labels of the time variable\ncycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(\"pre\", \"meso1\", \"meso2\", \"meso3\"), \n                           labels = c(\"Pre-training\", \"Meso-cycle 1\", \"Meso-cycle 2\", \"Meso-cycle 3\"))\n\n\n# create the basic plot\n\nggplot(data = cycdat, aes(timepoint, VO2.max, group = subject)) + \n  # Add lines to connect dots. Putting the lines first and plotting points on top\n  geom_line() + \n  # Add points foe each participant/time\n  geom_point(size = 3, fill = \"lightblue\", shape = 21) + \n\n  # Adding correct axis titles and a figure title\n  labs(x = \"Time-point\", \n       y = \"VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt; -1&lt;/sup&gt;)\", \n       title = \"Maximal aerobic power in response to systematic training in trained cyclists\") +\n  \n  # Changing the text rendering using element_markdown from the ggtext package.\n  theme(axis.title.y = element_markdown(size = 12)) \n\nNote: Adding \\n in the the text string breaks the line to get two rows."
  },
  {
    "objectID": "04-first-graph.html#references-and-footnotes",
    "href": "04-first-graph.html#references-and-footnotes",
    "title": "5  Creating your first graph",
    "section": "5.8 References and footnotes",
    "text": "5.8 References and footnotes\n\n\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "04-first-graph.html#footnotes",
    "href": "04-first-graph.html#footnotes",
    "title": "5  Creating your first graph",
    "section": "",
    "text": "See Chapter 10 in R for data science (2 edition)↩︎"
  },
  {
    "objectID": "05-first-tables.html#introduction",
    "href": "05-first-tables.html#introduction",
    "title": "6  Wrangling data to create your first table",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nWe can use tables to communicate a lot of information in a compact form while maintaining precision. This advantage is why creating tables is essential for effectively communicating data. We can easily create tables programmatically as part of an R markdown or quarto document. R has many “table generator” packages that translate your draft table to an output format of your choice. A great format to start authoring an analysis in is HTML; however, most “table generators” need to know your output format to be properly formatted and work in the output format. Below we will introduce a new package for the purpose of creating tables. This package, gt has the advantage that it does not require the user to change code when we switch to another output format. The gt package can create tables in HTML, PDF and word format.\nSince we are concerned with reproducibility, we would like to avoid copy-and-paste operations. The strength of writing reports in R markdown or quarto is the ability to combine data, code, and text to produce a formatted output programmatically. Therefore, We will choose a table generator that allows for consistently selecting multiple formats. One such table generator is part of the gt package.\nAs mentioned previously, authoring in R markdown and quarto makes little difference. However, we will now focus on the more modern file format, quarto, knowing that examples and tutorials written in R markdown will translate to quarto with few problems.\nThe basic workflow of creating a table in R markdown or quarto is to transform the data into a nice format and then get this underlying data into the table generator. The table generator is written in a code chunk, and upon rendering the source file, the table generator will create, for example, HTML output. In this chapter, we will introduce some data-wrangling tools since the table we will produce consists of summarized data. The functions we introduce are found in the packages dplyr and tidyr. These packages are loaded as part of the tidyverse package.\n\n6.1.1 Resources\nAll tidyversepackages are well documented and generally well represented in help forums. Google is your friend when looking for help.\nThe gt package is now a mature package for generating tables in R. This chapter is written on the basis of this package. If you are looking for alternatives, the kable function from the knitr package is described in a newly developed book, available online called the R Markdown Cookbook. The package, kableExtra comes with excellent vignettes for both html and pdf outputs. kableExtra provides extra functions to customize your basic knitr table. Note that kabale and kableExtra will only produce output in HTML and pdf-formats. Another package the can create tables in HTML, pdf, and word formats is the flextable package"
  },
  {
    "objectID": "05-first-tables.html#making-table-1",
    "href": "05-first-tables.html#making-table-1",
    "title": "6  Wrangling data to create your first table",
    "section": "6.2 Making “Table 1”",
    "text": "6.2 Making “Table 1”\nThe first table in many reports in sport and exercise studies is the “Participant characteristics” table. This first table summarizes background information on the participants. We will try to create this table based on data from (Hammarström et al. 2020). These data can be found in the exscidata package. To load the data and other required packages run the following code.\n\nlibrary(tidyverse) # for data wrangling\nlibrary(gt) # for creating tables\nlibrary(exscidata) # the dxadata\n\nThe end result of this exercise can be found below in Table 6.1. This summary table contains the average and standard deviation per group for the variables age, body mass and stature (height) and body fat as a percentage of the body mass. This table is a reproduction of the first part of Table 1 from (Hammarström et al. 2020).\n\n\n\n\n\n\nTable 6.1:  Participant characteristics \n  \n    \n    \n       \n      \n        Female\n      \n      \n        Male\n      \n    \n    \n      Included\n      Excluded\n      Included\n      Excluded\n    \n  \n  \n    N\n18\n4\n16\n3\n    Age (years)\n22 (1.3)\n22.9 (1.6)\n23.6 (4.1)\n24.3 (1.5)\n    Mass (kg)\n64.4 (10)\n64.6 (9.7)\n75.8 (11)\n88.2 (22)\n    Stature (cm)\n168 (6.9)\n166 (7.6)\n183 (5.9)\n189 (4.6)\n    Body fat (%)\n34.1 (5.6)\n28.8 (8.7)\n20.4 (6)\n24.3 (15)\n  \n  \n  \n    \n       Values are mean and (SD)\n    \n  \n\n\n\n\n\nWe have to make several operations to re-create this table. First we can select the columns we want to work with further from the data set that also contains a lot of other variables. Let us start by looking at the full data set. Below we use the function glmipse from the dplyr package (which is loaded with tidyverse).\n\ndata(\"dxadata\")\n\nglimpse(dxadata)\n\nRows: 80\nColumns: 59\n$ participant      &lt;chr&gt; \"FP28\", \"FP40\", \"FP21\", \"FP34\", \"FP23\", \"FP26\", \"FP36…\n$ time             &lt;chr&gt; \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre\", \"pre…\n$ multiple         &lt;chr&gt; \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"L\", \"L\"…\n$ single           &lt;chr&gt; \"R\", \"L\", \"L\", \"L\", \"L\", \"L\", \"R\", \"L\", \"L\", \"R\", \"R\"…\n$ sex              &lt;chr&gt; \"female\", \"female\", \"male\", \"female\", \"male\", \"female…\n$ include          &lt;chr&gt; \"incl\", \"incl\", \"incl\", \"incl\", \"incl\", \"excl\", \"incl…\n$ age              &lt;dbl&gt; 24.5, 22.1, 26.8, 23.1, 24.8, 24.2, 20.5, 20.6, 37.4,…\n$ height           &lt;dbl&gt; 170.0, 175.0, 184.0, 164.0, 176.5, 163.0, 158.0, 181.…\n$ weight           &lt;dbl&gt; 66.5, 64.0, 85.0, 53.0, 68.5, 56.0, 60.5, 83.5, 65.0,…\n$ BMD.head         &lt;dbl&gt; 2.477, 1.916, 2.306, 2.163, 2.108, 2.866, 1.849, 2.21…\n$ BMD.arms         &lt;dbl&gt; 0.952, 0.815, 0.980, 0.876, 0.917, 0.973, 0.871, 0.91…\n$ BMD.legs         &lt;dbl&gt; 1.430, 1.218, 1.598, 1.256, 1.402, 1.488, 1.372, 1.42…\n$ BMD.body         &lt;dbl&gt; 1.044, 0.860, 1.060, 0.842, 0.925, 0.984, 0.923, 1.01…\n$ BMD.ribs         &lt;dbl&gt; 0.770, 0.630, 0.765, 0.636, 0.721, 0.737, 0.648, 0.70…\n$ BMD.pelvis       &lt;dbl&gt; 1.252, 1.078, 1.314, 1.044, 1.154, 1.221, 1.194, 1.32…\n$ BMD.spine        &lt;dbl&gt; 1.316, 0.979, 1.293, 0.899, 1.047, 1.089, 1.006, 1.14…\n$ BMD.whole        &lt;dbl&gt; 1.268, 1.082, 1.325, 1.119, 1.181, 1.350, 1.166, 1.24…\n$ fat.left_arm     &lt;dbl&gt; 1168, 715, 871, 610, 788, 372, 932, 1312, 388, 668, 5…\n$ fat.left_leg     &lt;dbl&gt; 4469, 4696, 3467, 3023, 3088, 2100, 4674, 5435, 1873,…\n$ fat.left_body    &lt;dbl&gt; 6280, 4061, 7740, 3638, 6018, 2328, 4896, 9352, 2921,…\n$ fat.left_whole   &lt;dbl&gt; 12365, 9846, 12518, 7565, 10259, 5048, 10736, 16499, …\n$ fat.right_arm    &lt;dbl&gt; 1205, 769, 871, 610, 741, 374, 940, 1292, 413, 716, 5…\n$ fat.right_leg    &lt;dbl&gt; 4497, 4900, 3444, 3017, 3254, 2082, 4756, 5455, 1782,…\n$ fat.right_body   &lt;dbl&gt; 6082, 3923, 8172, 3602, 5699, 2144, 4705, 8674, 2640,…\n$ fat.right_whole  &lt;dbl&gt; 12102, 9862, 12856, 7479, 10020, 4821, 10806, 15876, …\n$ fat.arms         &lt;dbl&gt; 2373, 1484, 1742, 1220, 1529, 747, 1872, 2604, 802, 1…\n$ fat.legs         &lt;dbl&gt; 8965, 9596, 6911, 6040, 6342, 4182, 9430, 10890, 3655…\n$ fat.body         &lt;dbl&gt; 12362, 7984, 15912, 7239, 11717, 4472, 9601, 18026, 5…\n$ fat.android      &lt;dbl&gt; 1880, 963, 2460, 1203, 1933, 527, 1663, 3183, 1240, 1…\n$ fat.gynoid       &lt;dbl&gt; 5064, 5032, 4779, 3739, 4087, 2740, 5217, 6278, 2309,…\n$ fat.whole        &lt;dbl&gt; 24467, 19708, 25374, 15044, 20278, 9869, 21542, 32375…\n$ lean.left_arm    &lt;dbl&gt; 1987, 1931, 2884, 1753, 2652, 2425, 1913, 2266, 3066,…\n$ lean.left_leg    &lt;dbl&gt; 7059, 7190, 10281, 6014, 8242, 7903, 6829, 8889, 9664…\n$ lean.left_body   &lt;dbl&gt; 9516, 10693, 13847, 9736, 11387, 10573, 8954, 11482, …\n$ lean.left_whole  &lt;dbl&gt; 20305, 21778, 29332, 19143, 24185, 22946, 18809, 2431…\n$ lean.right_arm   &lt;dbl&gt; 2049, 2081, 2888, 1754, 2487, 2439, 1930, 2236, 3253,…\n$ lean.right_leg   &lt;dbl&gt; 7104, 7506, 10200, 6009, 8685, 7841, 6950, 8923, 9198…\n$ lean.right_body  &lt;dbl&gt; 9199, 10304, 14593, 9636, 10779, 9733, 8602, 10672, 1…\n$ lean.right_whole &lt;dbl&gt; 19605, 21310, 29643, 18792, 23653, 21837, 19407, 2372…\n$ lean.arms        &lt;dbl&gt; 4036, 4012, 5773, 3508, 5139, 4864, 3843, 4501, 6319,…\n$ lean.legs        &lt;dbl&gt; 14163, 14696, 20482, 12023, 16928, 15744, 13779, 1781…\n$ lean.body        &lt;dbl&gt; 18715, 20998, 28440, 19372, 22166, 20306, 17556, 2215…\n$ lean.android     &lt;dbl&gt; 2669, 2782, 3810, 2455, 2904, 2656, 2297, 3094, 3344,…\n$ lean.gynoid      &lt;dbl&gt; 6219, 7209, 10233, 5866, 7525, 5970, 5825, 8175, 7760…\n$ lean.whole       &lt;dbl&gt; 39910, 43088, 58976, 37934, 47837, 44783, 38216, 4804…\n$ BMC.left_arm     &lt;dbl&gt; 181, 138, 204, 144, 180, 173, 140, 173, 220, 226, 225…\n$ BMC.left_leg     &lt;dbl&gt; 567, 508, 728, 441, 562, 574, 482, 631, 633, 630, 672…\n$ BMC.left_body    &lt;dbl&gt; 622, 414, 696, 367, 526, 465, 370, 629, 473, 629, 509…\n$ BMC.left_whole   &lt;dbl&gt; 1680, 1321, 1945, 1201, 1527, 1580, 1131, 1688, 1544,…\n$ BMC.right_arm    &lt;dbl&gt; 198, 150, 210, 142, 176, 183, 140, 176, 224, 251, 226…\n$ BMC.right_leg    &lt;dbl&gt; 574, 514, 739, 431, 552, 565, 491, 641, 622, 636, 690…\n$ BMC.right_body   &lt;dbl&gt; 592, 428, 730, 351, 502, 409, 358, 582, 420, 616, 483…\n$ BMC.right_whole  &lt;dbl&gt; 1582, 1288, 1958, 1130, 1451, 1466, 1229, 1668, 1478,…\n$ BMC.arms         &lt;dbl&gt; 379, 288, 414, 285, 356, 357, 280, 348, 444, 478, 451…\n$ BMC.legs         &lt;dbl&gt; 1142, 1022, 1467, 872, 1115, 1139, 974, 1272, 1255, 1…\n$ BMC.body         &lt;dbl&gt; 1214, 842, 1426, 718, 1028, 874, 728, 1211, 893, 1245…\n$ BMC.android      &lt;dbl&gt; 80, 57, 90, 44, 56, 54, 43, 77, 52, 72, 59, 60, 65, 5…\n$ BMC.gynoid       &lt;dbl&gt; 314, 285, 427, 245, 299, 262, 241, 379, 335, 378, 332…\n$ BMC.whole        &lt;dbl&gt; 3261, 2609, 3903, 2331, 2978, 3046, 2360, 3356, 3022,…\n\n\nWe can see that we got 80 rows and 59 columns in the data set. The columns of interest to us are:\n\nparticipant\ntime\nsex\ninclude\nage\nheight\nweight\nfat.whole\n\nFor a full description of the data set, you can type ?dxadata in your console. The participant column is good to have to keep track of the data set in the beginning. time is needed to remove some observation that are not needed. This pre-training table only sums up information from before the intervention starts. sex is a grouping variable together with include, Table 1 in (Hammarström et al. 2020) uses Sex and Inclusion in data analysis as grouping for descriptive data. The other variables are used to describe the data sample.\n\n6.2.1 The pipe operator and select\nAs mentioned above, we will start by selecting the variables we want to work further with. Using the select function from dplyr we can select columns that we need. In the code below I will use select as part of a “pipe”. Think of the pipe as doing operations in sequel. Each time you use the pipe operator (%&gt;%) you say “then do”. The code below translates to:\n\nTake the data set dxadata, then do\nselect() the following variables, then do\nprint()\n\nprint, is a function that outputs the results of the operations. In each new function of a pipe, the data that we take with us from the above line ends up as the first argument. A representation of this behavior can be expressed as:\nDATA %&gt;%   FUNCTION(DATA, ARGUMENTS) %&gt;%   FUNCTION(DATA, ARGUMENTS) %&gt;%   FUNCTION(DATA)\nWe do not need to type the data part, instead the pipe operator (%&gt;%) gathers the data from each step and puts it in the subsequent function.\nCopy the code below to your own quarto document and run it. When using quarto you might want to set “Chunk output in console” in the settings menu. In my experience, this makes developing code a bit faster.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  print() # print the output\n\nNotice that I have added short comments after each line to make it clear what I want to accomplish. We will build further on the above code, and this is a common workflow. Using pipes, it is easy to extend the code by adding lines doing certain operations, one at the time. Notice also that the select function uses a list of variable names with include:weight being short for “take all variables from include to weight”.\n\n\n6.2.2 Filter observations\nThe next step will be to filter observations. We need to remove the observations that comes from the post-intervention tests. The time variable contains to values pre and post to remove post-values we need to tell R to remove all observations (rows) containing post. We will use the filter function from dplyr. This will be our first experience with logical operators. Let’s try out two alternatives, copy the code to your console to see the results.\n\n## Alternative 1: ##\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter away all observation with \"post\"\n  filter(time != \"post\") %&gt;%\n  \n  print() # print the output\n\n\n## Alternative 2: ##\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  print() # print the output\n\nThe above code should give the same output. The operator != says “not equal to”, the operator == says “equal to”. Notice that R uses two equal signs to say equal to. A single equal sign is used as an assignment operator in R.\n\n\n6.2.3 Create or change variables\nThe next problem for us is that we need to manipulate or combine information from two variables in order to calculate body fat percentage. The formula that we will use is simply expressing body fat as a percentage of the body weight.\n\\[\\text{Body fat (\\%)} = \\frac{\\text{Body fat (g)}/1000}{\\text{Body weight (kg)}} \\times 100\\] By using the mutate function we can add or manipulate existing variables in a pipe. Mutate takes as arguments a list of new variables:\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  print() # print the output\n\nIn the code above, we overwrite the variable fat.whole with the re-calculated variable.\n\n\n6.2.4 Grouped operations and summary statistics\nIn a pipe, we can group the data set giving us opportunities to calculate summary statistics over one or several grouping variables. In Table 1 in (Hammarström et al. 2020), include and sex are the two grouping variables. Using the group_by() function from dplyr sets the grouping of the data frame. If we use functions that summarizes data, such summaries will be per group. In Table 1 in (Hammarström et al. 2020) the number of participants in each group are specified. We can use the function n() to calculate the number of observations per group in a mutate call.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  print() # print the output\n\nThe new variable n now contains the number of observations in each group. For now we can regard this as a new variable. Each participant belongs to a specified group, and this specific group has n number of members.\nWe can now go further and use the summarise function. Instead of adding variables to the existing data set, summarize reduces the data set using some summarizing function, such as mean() or sd(). These summary statistics are what we are looking for in our data set. Example of other summarizing functions for descriptive data are min() and max() for the minimum and maximum.\nWe can use the summarise() function to calculate the mean and standard deviation for the weight variable.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Summarise weight\n  summarise(weight.m = mean(weight), \n            weight.s = sd(weight)) %&gt;%\n  \n  print() # print the output\n\nTry out the code in your own quarto document. The above example gives us what we want, however, it means that we need to type a lot. Instead of needing to make a summary for each variable, we can combine the variables in a long format. To get to the long format we will use the pivot_longer() function. This function gathers several variables into two columns, one with the variables names as values and a second column with each value from the original variables. In our case we want to gather the variables age, height, weight, fat.whole and n. I will call the new variables that we create variable and value.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  print()\n\nThe cols = age:n part of pivot_longer specifies what columns to gather. The data set is still grouped by include and sex. We may now proceed by summarizing over these groups, however, we need to add another group to specify that we want different values per variable. To do this we re-specify the grouping. After this we add the summarise function.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  print()\n\nIf you run the above code you will notice that the the standard deviation of each variable is larger than zero except for n which has no variability. This is because we created it per group and simply calculated it as the sum of observations.\nTake a look at Table 1 in (Hammarström et al. 2020). The format of the descriptive statistics are mean (SD), this is a preferred way of reporting these statistics. In order to achieve this we need to “manually” convert the numbers. In the example below, I will start by making a new variable by simply pasting the numbers together. I will also add the parentheses.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = paste0(m, \" (\", s, \")\"))\n  print()\n\nIn mutate(ms = paste0(m, \" (\", s, \")\")), the paste0 function simply glues components together to form a string of text. First, the vector of means are being used, then we add a parenthesis, followed by the SD and finally a parenthesis.\nIf you run the above code you will notice that you end up with numbers looking like this:\n167.666666666667 (6.86851298231541)\nThis is neither good or good looking. We have to take care of the decimal places. There are a number of ways to do this but in this case the function signif seems to make the situation better. signif rounds to significant digits. This means that we will get different rounding depending on the “size” of the value. I find signif(m, 3) to be a good starting point.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = paste0(signif(m, 3), # Use signif to round to significant numbers\n                     \" (\",\n                     signif(s, 3), \n                     \")\")) %&gt;%\n  print()\n\nThings are starting to look good. Run the code, what do you think. A problem with the above is that we do not want any variability in the n variable. So if the variable is n we do not want that kind of formatting. It is time to add a conditional statement. In dplyr there are easy-to-use if/else functions. The function if_else sets a condition, if this is met then we can decide what to do, and likewise decide what to do if it is not met.\nThis looks something like this inside a dplyr pipe:\n\n... %&gt;%\n  if_else(IF_THIS_IS_TRUE, THE_DO_THIS, OTHERWISE_DO_THIS) %&gt;%\n  print()\n\nIf variable is n, then we only want to display m otherwise we want the full code as described above: paste0(signif(m, 3), \" (\", signif(s, 3), \")\"). We add this to the code:\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\"))) %&gt;%\n  print()\n\nThe as.character part is needed because the output of if_else must be the same regardless of what the outcome of the test is.\nWe are getting close to something!\nThe next step is to remove variables that we do not longer need. The select function will help us with that. we can remove m and s by select(-m, -s), the minus sign tells R to remove them from the list of variables in the data set. We can then combine the grouping variables into a include_sex variable. Similarly to what we did above, we can simply paste them together. Now we will use the paste (function instead of paste0). In paste we specify a separator, maybe _ is a nice alternative. Selecting away the individual variables from the new combined one leaves us with this code and data set.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  print()\n\nIf ungroup is not used, we cannot select away variables since they are used to group the data set. We will now perform the last operations before we can make it a table. To make it formatted as in Table 1 in (Hammarström et al. 2020), we can make the present data set wider. Each group as its own column in addition to the variable name column. We will use the opposite function to pivot_longer, namely pivot_wider1. pivot_wider takes a variable or “key” column and a “values” column and divide the values based on the “key”.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n  print()\n\nA final step is to format the variable variable(!). The easiest is to make it a factor variable with specified levels and names. In the factor function we use levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\") to specify the order of values contained in the variable. Using labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \"Stature (cm)\", \"Body fat (%)\", we set corresponding labels on each level. After we have added this information to the variable we can use arrange to sort the data set. arrange will sort the data set based on the order we have given to the variable. select will help you sort the columns to match what we want.\n\n#| eval: false\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n    mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  print()\n\n`summarise()` has grouped output by 'include', 'sex'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 5 × 5\n  variable     excl_female excl_male   incl_female incl_male  \n  &lt;fct&gt;        &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n1 N            4           3           18          16         \n2 Age (years)  22.9 (1.57) 24.3 (1.46) 22 (1.25)   23.6 (4.11)\n3 Mass (kg)    64.6 (9.71) 88.2 (22.4) 64.4 (10.4) 75.8 (10.7)\n4 Stature (cm) 166 (7.59)  189 (4.58)  168 (6.87)  183 (5.88) \n5 Body fat (%) 28.8 (8.69) 24.3 (15.3) 34.1 (5.64) 20.4 (5.99)\n\n\n\n\n6.2.5 Starting the table generator - The gt() function.\nThe next step is to “pipe” everything into the table generator.\n\ndxadata %&gt;% # take the dxadata data set\n  select(participant, time, sex, include:weight, fat.whole) %&gt;% \n  # select participant, time, sex, include to height and fat.whole\n  \n  # Filter to keep all observations with pre\n  filter(time == \"pre\") %&gt;%\n  \n  # Calculate body fat\n  # fat.whole in grams, needs to be divided by 1000 to express as kg\n  # Multiply by 100 to get percentage\n  mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;%\n  \n  # Group the data frame and add a variable specifying the number of observations per group\n  group_by(include, sex) %&gt;%\n  mutate(n = n()) %&gt;%\n  # Collect all variables for convenient summarizing\n  pivot_longer(names_to = \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  # Create a new grouping, adding variable\n  group_by(include, sex, variable) %&gt;%\n  # Summarize in two new variables m for mean and s for SD\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  # Add descriptive statistics together for nice formatting\n  mutate(ms = if_else(variable == \"n\", # If the variable is n\n                      as.character(m), # the only display the mean, otherwise:\n                        paste0(signif(m, 3), # Use signif to round to significant numbers\n                        \" (\",\n                        signif(s, 3), \n                        \")\")), \n         # Doing a new grouping variable\n         include_sex = paste(include, sex, sep = \"_\")) %&gt;%\n  # removing unnecessary variables after ungrouping\n  ungroup() %&gt;%\n  select(-sex, -include, -m, -s) %&gt;%\n  # pivot wider to match the desired data\n  pivot_wider(names_from = include_sex, \n              values_from = ms) %&gt;%\n    mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  # Piping into the table generator (gt)\n\n  gt()  \n\nAs per our strategy to first summarize and set up the data table, we already have nice first draft. However, we need to format variable names and add column labels. We can also add a footnote.\nThe gt package has several functions for manipulating the raw tables created with the gt() function. The gt package also use a consistent vocabulary for tables as seen in [this figure]((https://gt.rstudio.com/reference/figures/gt_parts_of_a_table.svg).\nFirst, using tab_footnote() we can add the footnote indicating that “Values are mean and (SD)”. We do this by piping the whole table, created with gt() into tab_footnote(footnote = \"Values are mean and (SD)\"). We have two columns representing females and two representing males, these can be more clearly separated by adding a spanner column label. This column label adds rows to the table. Using tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) we add “Female” above the two columns representing females. We can do the same for males. Using cols_label() we specify new column names to match what we want. The resulting full code can be seen below.\n\ndxadata %&gt;%\n  select(participant, time, sex, include:weight, fat.whole) %&gt;%\n  mutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\n  filter(time == \"pre\") %&gt;%\n  group_by(sex, include) %&gt;%\n  mutate(n = n()) %&gt;%\n\n  pivot_longer(names_to =  \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  group_by(sex, include, variable) %&gt;%\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  ungroup() %&gt;%\n  mutate(m = signif(m, digits = 3), \n         s = signif(s, digits = 2), \n         ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n         sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\n  dplyr::select(-m, -s, - sex, -include) %&gt;%\n\n  pivot_wider(names_from = sex_incl, \n              values_from = ms) %&gt;%\n  select(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\n  mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  gt() %&gt;%\n  tab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\n  tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\n  tab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\n  cols_label(variable = \" \", \n             female_incl = \"Included\", \n             female_excl = \"Excluded\", \n             male_incl = \"Included\", \n             male_excl = \"Excluded\")\n\n\n\n6.2.6 Working with tables in quarto\nIf we where to use this table in a report created with quarto we would like to be able to cross-reference it. This will work if we add information to the code chunk where the table is created. More specifically we need to set a table label and a table caption. Quarto has built in support for cross referencing figures and tables. Adding chunk options that sets a label and table caption will make it possible to reference the table. Note that the label must start with “tbl-” to make quarto identify it as a table.\n```{r}\n#| label: tbl-participant-characteristics\n#| tbl-cap: \"Participant characteristics\"\n\ndxadata %&gt;%\nselect(participant, time, sex, include:weight, fat.whole) %&gt;%\nmutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\nfilter(time == \"pre\") %&gt;%\ngroup_by(sex, include) %&gt;%\nmutate(n = n()) %&gt;%\n\npivot_longer(names_to =  \"variable\", \n             values_to = \"value\", \n             cols = age:n) %&gt;%\ngroup_by(sex, include, variable) %&gt;%\nsummarise(m = mean(value), \n          s = sd(value)) %&gt;%\nungroup() %&gt;%\nmutate(m = signif(m, digits = 3), \n       s = signif(s, digits = 2), \n       ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n       sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\ndplyr::select(-m, -s, - sex, -include) %&gt;%\n\npivot_wider(names_from = sex_incl, \n            values_from = ms) %&gt;%\nselect(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\nmutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                         labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                    \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\narrange(variable) %&gt;%\n\ngt() %&gt;%\ntab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\ntab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\ntab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\ncols_label(variable = \" \", \n           female_incl = \"Included\", \n           female_excl = \"Excluded\", \n           male_incl = \"Included\", \n           male_excl = \"Excluded\")\n           \n```\nThe above code will produce referable table, as seen in Table 6.2!\n\n  dxadata %&gt;%\n  select(participant, time, sex, include:weight, fat.whole) %&gt;%\n  mutate(fat.whole = ((fat.whole / 1000) / weight) * 100) %&gt;%\n  filter(time == \"pre\") %&gt;%\n  group_by(sex, include) %&gt;%\n  mutate(n = n()) %&gt;%\n\n  pivot_longer(names_to =  \"variable\", \n               values_to = \"value\", \n               cols = age:n) %&gt;%\n  group_by(sex, include, variable) %&gt;%\n  summarise(m = mean(value), \n            s = sd(value)) %&gt;%\n  ungroup() %&gt;%\n  mutate(m = signif(m, digits = 3), \n         s = signif(s, digits = 2), \n         ms = if_else(variable == \"n\", as.character(m), paste0(m, \" (\", s, \")\")), \n         sex_incl = paste(sex, include, sep = \"_\")) %&gt;%\n  dplyr::select(-m, -s, - sex, -include) %&gt;%\n\n  pivot_wider(names_from = sex_incl, \n              values_from = ms) %&gt;%\n  select(variable, female_incl, female_excl, male_incl, male_excl) %&gt;%\n  mutate(variable = factor(variable, levels = c(\"n\", \"age\", \"weight\", \"height\", \"fat.whole\"), \n                           labels = c(\"N\", \"Age (years)\", \"Mass (kg)\", \n                                      \"Stature (cm)\", \"Body fat (%)\"))) %&gt;%\n  arrange(variable) %&gt;%\n  \n  gt() %&gt;%\n  tab_footnote(footnote = \"Values are mean and (SD)\") %&gt;%\n  tab_spanner(label = \"Female\", columns = c(\"female_incl\", \"female_excl\")) %&gt;%\n  tab_spanner(label = \"Male\", columns = c(\"male_incl\", \"male_excl\")) %&gt;%\n  cols_label(variable = \" \", \n             female_incl = \"Included\", \n             female_excl = \"Excluded\", \n             male_incl = \"Included\", \n             male_excl = \"Excluded\")\n\n`summarise()` has grouped output by 'sex', 'include'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\nTable 6.2:  Participant characteristics \n  \n    \n    \n       \n      \n        Female\n      \n      \n        Male\n      \n    \n    \n      Included\n      Excluded\n      Included\n      Excluded\n    \n  \n  \n    N\n18\n4\n16\n3\n    Age (years)\n22 (1.3)\n22.9 (1.6)\n23.6 (4.1)\n24.3 (1.5)\n    Mass (kg)\n64.4 (10)\n64.6 (9.7)\n75.8 (11)\n88.2 (22)\n    Stature (cm)\n168 (6.9)\n166 (7.6)\n183 (5.9)\n189 (4.6)\n    Body fat (%)\n34.1 (5.6)\n28.8 (8.7)\n20.4 (6)\n24.3 (15)\n  \n  \n  \n    \n       Values are mean and (SD)"
  },
  {
    "objectID": "05-first-tables.html#an-exercise-in-data-wrangling-and-tables",
    "href": "05-first-tables.html#an-exercise-in-data-wrangling-and-tables",
    "title": "6  Wrangling data to create your first table",
    "section": "6.3 An exercise in data wrangling and tables",
    "text": "6.3 An exercise in data wrangling and tables\nIn total 30 college students performed a heavy-resistance training protocol where training volume was constantly increased over six weeks. In (Haun et al. 2018), a part of the study, focusing on supplementation was reported. In (Haun et al. 2019), participants were divided into two clusters based on training responses and the authors aimed to answer the question what separates high- from low-responders to resistance training.\nIn this exercise we want to reproduce a big part of Table 1 in (Haun et al. 2019). The Table as re-produced here can be seen below. See the original article for explanation of clusters. To select variables, see the data description in the exscidata package, the data set is called hypertrophy.\n\n\n\n\n\nBaseline characteristics at PRE and back squat training volume\nbetween clusters\n  \n    \n    \n       \n      HIGH (n = 10)\n      LOW (n = 10)\n    \n  \n  \n    Age (years)\n20.9 (1.9)\n21.5 (1)\n    Training age (years)\n5.5 (2.3)\n5.5 (2)\n    Body mass (kg)\n78.8 (8)\n83.1 (12.8)\n    DXA LBM (kg)\n62.2 (5.9)\n65.1 (9.7)\n    DXA FM (kg)\n13.5 (4.9)\n14.5 (4.9)\n    Type II fiber (%)\n59.4 (16.9)\n50.2 (13.6)\n    3RM back squat (kg)\n127 (23.3)\n135.4 (14.1)\n    Total back squat training volume (kg)\n106610.2 (18679.4)\n111820.8 (12962.5)\n  \n  \n  \n    \n       Values are mean and (SD)\n    \n  \n\n\n\n\nClick for a possible solution\n\n\n# load the data\ndata(hypertrophy)\n\n\nhypertrophy %&gt;%\n# Select the variables needed to reproduce the table\n  dplyr::select(PARTICIPANT, \n                GROUP, \n                CLUSTER,\n                AGE, \n                BODYMASS_T1, \n                TRAINING_AGE,\n                PERCENT_TYPE_II_T1, \n                SQUAT_3RM, \n                DXA_LBM_T1,\n                DXA_FM_T1, \n                SQUAT_VOLUME) %&gt;%\n  # Pivot longer to gather all variables\n  pivot_longer(cols = AGE:SQUAT_VOLUME, names_to = \"variable\", values_to = \"values\") %&gt;%\n  # Remove participants not belonging to a cluster\n  filter(!is.na(CLUSTER)) %&gt;%\n  # Create a grouping before summarizing\n  group_by(CLUSTER, variable) %&gt;%\n  summarise(m = mean(values), \n            s = sd(values)) %&gt;%\n  # For nice printing, paste mean and SD \n  mutate(m.s = paste0(round(m,1), \" (\", round(s,1), \")\")) %&gt;%\n  # Select only variables needed for the table\n  select(CLUSTER, variable, m.s) %&gt;%\n  # Transform the data set to a wide format based on clusters\n  pivot_wider(names_from = CLUSTER, values_from = m.s) %&gt;%\n  # Re-arrange the \"variable\" variable, correct order with levels, and correct labels\n  mutate(variable = factor(variable, levels = c(\"AGE\",\n                                                \"TRAINING_AGE\", \n                                                \"BODYMASS_T1\", \n                                                \"DXA_LBM_T1\", \n                                                \"DXA_FM_T1\", \n                                                \"PERCENT_TYPE_II_T1\", \n                                                \"SQUAT_3RM\", \n                                                \"SQUAT_VOLUME\"), \n                                      labels = c(\"Age (years)\", \n                                                \"Training age (years)\", \n                                                \"Body mass (kg)\", \n                                                \"DXA LBM (kg)\", \n                                                \"DXA FM (kg)\", \n                                                \"Type II fiber (%)\", \n                                                \"3RM back squat (kg)\", \n                                                \"Total back squat training volume (kg)\"))) %&gt;%\n  # Sort/order the dataset\n  arrange(variable) %&gt;%\n  # Use gt to output the table with appropriate caption and column names. \n  gt() %&gt;%\n  cols_label(variable = \" \", HIGH = \"HIGH (n = 10)\", LOW = \"LOW (n = 10)\") %&gt;%\n    tab_footnote(footnote = \"Values are mean and (SD)\")"
  },
  {
    "objectID": "05-first-tables.html#references-and-footnotes",
    "href": "05-first-tables.html#references-and-footnotes",
    "title": "6  Wrangling data to create your first table",
    "section": "6.4 References and footnotes",
    "text": "6.4 References and footnotes\n\n\n\n\nHammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. “Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis.” Journal Article. The Journal of Physiology 598 (3): 543–65. https://doi.org/10.1113/JP278455.\n\n\nHaun, C. T., C G. Vann, C. Brooks Mobley, Shelby C. Osburn, Petey W. Mumford, Paul A. Roberson, Matthew A. Romero, et al. 2019. “Pre-Training Skeletal Muscle Fiber Size and Predominant Fiber Type Best Predict Hypertrophic Responses to 6 Weeks of Resistance Training in Previously Trained Young Men.” Journal Article. Frontiers in Physiology 10 (297). https://doi.org/10.3389/fphys.2019.00297.\n\n\nHaun, C. T., C. G. Vann, C. B. Mobley, P. A. Roberson, S. C. Osburn, H. M. Holmes, P. M. Mumford, et al. 2018. “Effects of Graded Whey Supplementation During Extreme-Volume Resistance Training.” Journal Article. Front Nutr 5: 84. https://doi.org/10.3389/fnut.2018.00084."
  },
  {
    "objectID": "05-first-tables.html#footnotes",
    "href": "05-first-tables.html#footnotes",
    "title": "6  Wrangling data to create your first table",
    "section": "",
    "text": "All this talk about pivot, take a break and watch this clip from the hit series “Friends”, its about “pivot”!↩︎"
  },
  {
    "objectID": "06-writing-reports.html#rstudio-projects-and-your-reproducible-report",
    "href": "06-writing-reports.html#rstudio-projects-and-your-reproducible-report",
    "title": "7  Writing your first reproducible report",
    "section": "7.1 RStudio projects and your reproducible report",
    "text": "7.1 RStudio projects and your reproducible report\nWhen you build an analysis in a R markdown or quarto file, R will use the folder that the source file is in as the root directory. This directory (or folder) is the top directory in a file system. This means that R will look for data or other files used to generate the report in this folder structure. Think of this folder as ./ (confusing, I know! But bare with me!). Any sub-folders to the root directory can be called things like\n\n./data/ (a folder where you keep data files),\n./figures/ (a folder where you output figures from analyses).\n\nThe R markdown or quarto file, being in the root directory will have the “address” ./my_rmarkdown_file.Rmd.\nThis has several advantages, as long as you stick to one rule: When doing an analysis, always use relative paths (“addresses” to files and folders). Never reference a folder or file by their absolute path. The absolute path for the file I’m writing in now is /Users/Daniel1/Documents/projects/quant-methods/06-writing-reports.qmd. The relative path is ./06-writing-reports.qmd. When working in a “project” you may move the folder containing your project to other locations, but relative paths will not break.\nIf you want to share your analysis, all you need to do is share the folder with all content with your friend. If you use relative paths, everything will work on your friends computer. If you use absolute paths, nothing will work, unless your friends computer uses the same folder structure (highly unlikely).\nRStudio projects makes it easy to jump back and forth between projects. The project menu (top right corner in RStudio) contains all your recent projects. When starting a new project, R will create a .Rproj file that contains the settings for your project. If you start a project and click this file, a settings menu will appear where you can customize settings for your particular project.\nWhat does this have to do with my quarto/RMarkdown file? As mentioned above, the source file is often written in a context where you have data and other files that help you create your desired output. By always working in a project makes it easy to keep every file in the right place."
  },
  {
    "objectID": "06-writing-reports.html#getting-started-with-r-projects",
    "href": "06-writing-reports.html#getting-started-with-r-projects",
    "title": "7  Writing your first reproducible report",
    "section": "7.2 Getting started with R projects",
    "text": "7.2 Getting started with R projects\nTo start a new project in RStudio:\n\nPress the project menu in the upper right corner, choose “Start a project in a brand new working directory”\nIn the next menu, select “New Project” and chose a suitable location on your machine for the project to live.\nUn-check the option of creating a git repository. We will do this later.\nName the project with an informative name. “Project1” is not good enough, “rproject-tutorial” or “rproject-report-workshop” is better as you will be able to track it down afterwards.\n\nWe have now started up a brand new project without version control. The next step is to make sure the setting of the project is up date with our Global settings in RStudio. By clicking the .Rproj file in our files tab, we will open up a settings window. These are the settings for the project. Under General we see that we can set RStudio to handle the workspace and history as default. This means that our global options will be used. The global options regarding workspace should be to never save workspace, do not restore on start up and do not save history.\n\n7.2.1 What folder am I in?\nThe great advantage of an RStudio Project is that it will make it easier to keep everything contained in our folder. To check what folder we are currently in, type getwd() in the console. R should return the full path to our working directory. If this is the case, success. If not, you have probably not succeeded in opening up a project, or you have somehow told R to set another working directory.\nThe working directory is the root directory. It is possible to set the working directory manually. However, we should aim not to do that! The R command setwd() should not be used as it breaks relative paths.\nSee R for Data Science, chapter 7 for more details on RStudio projects."
  },
  {
    "objectID": "06-writing-reports.html#authoring-reports-in-quarto",
    "href": "06-writing-reports.html#authoring-reports-in-quarto",
    "title": "7  Writing your first reproducible report",
    "section": "7.3 Authoring reports in quarto",
    "text": "7.3 Authoring reports in quarto\nSo much fuzz just for writing a report? Yes, it is a bit more work to get started. The upside is that this system is easier to navigate with increasing complexity compared to a system where text, figures, tables and software are located on different locations in your computer and the final report requires copy-paste operations.\nAs mentioned before, we will focus on the more modern format for authoring reports in R, quarto. In this section we will introduce the basic building blocks of a report and how to put them together. We have already covered figures and tables, now its time to put the into context.\n\n7.3.1 The Markdown syntax, and friends\nWe have already mentioned the markup language markdown1. This enables an author like yourself to format your text in a plain text editor. This has the advantage of keeping formatting explicit and available from the keyboard. In a word editor like MS Word, formatting is sometimes not obvious and you need to point and click make changes. The R-markdown style of markdown includes the ability to combine code in code chunks and embedded in text. This makes it possible to include code output in the final report. Another technical achievement that makes RMarkdown and quarto possible is Pandoc, a general document conversion software. Pandoc can convert files from one format to another, this includes the operations that we use, from markdown to HTML, PDF or Word. Both markdown and pandoc are free and open source software that makes life easy for us!\n\n7.3.1.1 Markdown basics\nThe idea of using markdown is that everything is formatted in plain text. This requires a little bit of extra syntax. We can use bold or italic, striketrough and superscript. Lists are also an option as numbered:\n\nItem one\nItem two\n\nAnd, as unordered\n\nItem x\nItem y\n\nWith sub item z\n\n\nLinks can be added like this.\nA table can be added also, like this:\n\n\n\nColumn 1\nColumn2\n\n\n\n\nItem1\nItem 2\n\n\n\nThe whole section above will look like this in your plain text editor:\n\nThe idea of using markdown is that everything is formatted in plain text. \nThis requires a little bit of extra syntax. We can use **bold** or *italic*, \n~~striketrough~~ and ^superscript^. Lists are also an option as numbered:\n\n1. Item one\n2. Item two\n\nAnd, as unordered\n\n* Item x\n* Item y\n  + With sub item z\n  \nLinks can be added [like this](https://rmarkdown.rstudio.com/authoring_basics.html).\n\nA table can be added also, like this:\n\n|Column 1|Column2|\n|---| ---|\n|Item1 | Item 2|\n\n\n\n\n7.3.2 Additional formatting\nIn addition to plain markdown, we can also write HTML or LaTeX in RMarkdown or quarto files.\nHTML is convenient when we want to add formatted text beyond the capabilities of markdown, such as color. Some formatting might be considered more easily remembered such as subscript and superscript. Notice that HTML and markdown syntax can be combined:\nSome Markdown text with some blue text, superscript.\n\n\nSee here for syntax\nHTML is convenient when we want to add formatted text \nbeyond the capabilities of markdown, such as \n&lt;span style=\"color:red\"&gt;color&lt;/span&gt;. Some formatting \nmight be considered more easily remembered such as \n&lt;sub&gt;subscript&lt;/sub&gt; and &lt;sup&gt;superscript&lt;/sup&gt;. \n\nNotice that HTML and markdown syntax can be combined:\n\nSome Markdown text with &lt;span style=\"color:blue\"&gt;some *blue* \n  text, &lt;sup&gt;&lt;span style=\"color:red\"&gt;super&lt;/span&gt;**script**&lt;/sup&gt;&lt;/span&gt;.\n\n\nLaTeX is another plain text formatting system, or markup language, but it far more complex than markdown. Text formatting using LaTeX is probably not needed for simpler documents as markdown and HTML will be enough. The additional advantage of using LaTeX comes with equations.\nEquations can be written inline, such as the standard deviation \\(s = \\sqrt{\\frac{\\sum{(x_i - \\bar{x})^2}}{n-1}}\\). An equation can also be written on the center of the document\n\\[\nF=ma\n\\tag{7.1}\\]\nWe are also able to cross-reference the equation Equation 7.1 for force (\\(F\\)).\nA larger collection of equations is sometimes needed to describe a statistical model, as in Equation 7.2.\n\\[\n\\begin{aligned}\n\\begin{split}\n\\text{y}_i &\\sim \\operatorname{Normal}(\\mu_i, \\sigma)\n\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{x}_i,\n\\end{split}\n\\end{aligned}\n\\tag{7.2}\\]\nThe equation above could look like this in your editor, including the tag ({#eq-model}) used for cross-referencing:\n\n$$\n\\begin{align}\n\\text{y}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{x}_i,\n\\end{align}\n$$ {#eq-model}\n\nSee this wikibook on LaTeX for an overview on mathematics in LaTeX.\n\n\n7.3.3 Code chunks\nUsing RMarkdown syntax we can add a code chunk using the following syntax:\n```{r}\n#| label: fig-simple-plot\n#| message: false\n#| echo: true\n\ndat &lt;- data.frame(a = rnorm(10, 10, 10), \n                  b = runif(10, 1, 20))\n\nplot(dat)\n\n```\nWe recognize the R code inside the code chunk but we have only touched upon code chunk settings. These are settings that tells R (or quarto) how to handle output from the code chunk. message: false indicate that any messages from R code should not be displayed in the output document. echo: true indicates that the code in the code chunk should be displayed in the output document. The label is important as it enables cross-referencing the output. If your code chunk outputs a figure the prefix fig- must be in the label to enable cross-referencing. Likewise, if your code chunk creates an table, the prefix tbl- must be in the label. Possible code chunk settings also include figure and table captions.\nSettings can also be specified in the YAML field in quarto files. We might not want to display our code, messages or warnings in the final output. We would specify this in the YAML field as\n\n---\ntitle: \"A basic quarto report without code\"\nexecute:\n  echo: false\n  message: false\n  warning: false\n---\n\nSee here for documentation on execution options for code chunks in quarto. See also Chapter 29 in R for data science.\n\n\n7.3.4 Cross-referencing, references and footnotes\nWe have mentioned cross-referencing above, this basically means referencing specific parts of your document in the text. A figure might be mentioned in the text, such as Figure 7.1. To insert the cross-reference in text, use the @fig-label syntax where fig- is the required prefix for figures and label is a user defined unique identifier. The label should be included in the code chunk under such as #| label: fig-label. The equivalent prefix for tables is tbl-.\n\n\n\n\n\nFigure 7.1: This is an example of a Figure with a caption.\n\n\n\n\nWe might want to cross-reference a section in our document. This is easily done by inserting a tag at the section header such as {#sec-cross-reference}, this tag can be referenced in text using @sec-cross-reference resulting in Section 7.3.4. The sec- part is the required prefix for a section.\nFor additional details on cross-referencing, see the quarto documentation on cross-referencing.\nCitations are mandatory in academic writing. Be sure to take advantage of the built in support for citations. When writing in quarto (or RMarkdown) we can think of a reference as having three parts. The identifier, the reference and the style. We use the identifier when authoring. For example, let’s cite the R for Data Science book, we do this by using the following syntax (Wickham and Grolemund 2017). The syntax requires that we have linked a bibliography to the document. The bibliography should include the reference, with the same identifier. The bibliography is a collection of reference entries written in bibtext format (see below). It must be included in the document meta data field (YAML field).\n\n@book{r4ds,\n  title={R for data science},\n  author={Wickham, Hadley and {\\c{C}}etinkaya-Rundel, Mine and Grolemund, Garrett},\n  year={2023},\n  publisher={\" O'Reilly Media, Inc.\"}\n}\n\nNotice the identifier. When adding the citation [@r4ds] it will turn out to (Wickham and Grolemund 2017) in the formatted text and added to the bottom of the document as a full reference. If we want another citation style we can specify a file responsible for citation styles. The default is the Chicago style. Specifying a citation style file in YAML will change the style, for example csl: my-citation-style.csl tells quarto to use the file my-citation-style.csl when formatting citations. This file can be edited or copied from a large collection of possible styles located in the citation style language repository. The repository is hosted on GitHub and searchable, click “Go to file” and type “vancouver” to get examples of CSL files that uses a Vancouver-type citation style.\nFootnotes can be handy when writing. In the default mode, these will be included as superscript numbers, like this2, numbered by order of appearance.\nThe syntax for including footnotes is straight forward. Notice that the text for the footnote is included below the paragraph using the identifier created in the text.\n\n\nSee here for footnote syntax\nFootnotes can be handy when writing. In the default mode, \nthese will be included as superscript numbers, like \nthis[^footnote], numbered by order of appearance. \n\n[^footnote]: This is a footnote.\n\n\nSee the quarto documentation on citations and footnotes.\nsee also Chapter 29 in R for data science."
  },
  {
    "objectID": "06-writing-reports.html#additional-files-and-folder-structures-in-a-complete-analysis-project",
    "href": "06-writing-reports.html#additional-files-and-folder-structures-in-a-complete-analysis-project",
    "title": "7  Writing your first reproducible report",
    "section": "7.4 Additional files and folder structures in a complete analysis project",
    "text": "7.4 Additional files and folder structures in a complete analysis project\nAs we starting to notice, a report authored in quarto or R Markdown often requires additional files to render properly. We might have a collection of references, some data sets and possibly some analysis files that are not included in the quarto or R markdown file. To keep everything organized I recommend a general folder structure for every analysis project. This structure might change as the project grows or changes. The parts listed below are what I usually end up with as a common set in the majority of projects I work with3.\n\n7.4.1 The readme-file\nThe README-file can be, or should be an important file for you. When a project is larger than very tiny, it becoms complex and you should include a README-file to tell others and yourself what the project is about and how it is organized. Creating a file called README.md in a GitHub folder automatically renders it on the main page of your repository (more about that later). Here you have the opportunity to outline the project and explain the organization of your projects folder/repository.\nI find it very helpful to work with the README-file continuously as the project evolves. It helps me remember where the project is going.\nA very basic ouline of the README-file can be\n\n# My project\n\nAuthor: \nDate: \n\n## Project description \nA description of what this prject is about, the \npurpose and how to get there. \n\n## Organization of the repository\n\nFiles are organized as...\n\n## Changes and logs\n2023-08-15: Added a description of the project...\n\n\n\n7.4.2 /resources\nI usually include a sub-folder called resources. Here I keep CSL-files, the bibliography, any styling or templates used to render the report. Keeping this in a separate folder keeps the top-folder clean.\n\n\n7.4.3 /data\nThe data folder is an important one. Here I keep all data that exists as e.g., .csv or .xlsx files. If I create data in the project, such as combined data sets that are stored for more convienient use, I keep these in a sub-folder (e.g., data/derived-data/)4. If there is a lot of raw unprocessed data, these might be stored in data/raw-data/ with specific sub-folders.\n\n\n7.4.4 /figures\nIf you want to make figures for presentations or submission to a journal, you might want to save output as .tiff or .pdf files. When doing this it might be a good idea to structure a figure-folder with e.g. figure1.R that renders to e.g. figure1.pdf. If you only include figure output in the quarto, the figure folder might contain R-scripts that produces the figures. The end results are included in the quarto document by sourcing the R-script. This detour might make it easier to find code for a specific figure once your project is large enough.\n\n\n7.4.5 /R\nR-scripts that are not figures but contains analyses or data cleaning or the like can be stored in R scripts in a specific folder. The reason to keep R scripts separate from a quarto file might be that they are large and produces some output, like a data set, that is later used in the report file. It makes it easier to find and work on specific code without breaking other parts of your project. Actually, it is a good idea to “build” the parts of your analysis as smaller parts."
  },
  {
    "objectID": "06-writing-reports.html#quarto-formats",
    "href": "06-writing-reports.html#quarto-formats",
    "title": "7  Writing your first reproducible report",
    "section": "7.5 Quarto formats",
    "text": "7.5 Quarto formats\nQuarto brings many possibilities for authoring data-driven formats, including but not restricted to websites, books, blogs and presentations. In this course we will cover simple reports and books. These formats are easily extended to websites and blogs. The quarto documentation is really good in conveying this message. Se also Chapter 30 in R 4 DS."
  },
  {
    "objectID": "06-writing-reports.html#references-and-footnotes",
    "href": "06-writing-reports.html#references-and-footnotes",
    "title": "7  Writing your first reproducible report",
    "section": "7.6 References and footnotes",
    "text": "7.6 References and footnotes\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "06-writing-reports.html#footnotes",
    "href": "06-writing-reports.html#footnotes",
    "title": "7  Writing your first reproducible report",
    "section": "",
    "text": "Markdown was introduced in 2004 as a syntax to convert plain text to formatted HTML. Markdown is primarily attributed to John Gruber.↩︎\nThis is a footnote.↩︎\nThis organization was initially inspired by Karl Broman’s steps towards reproducible science.↩︎\nAgain, an important note from Karl Broman, “Organize your data and code”↩︎"
  },
  {
    "objectID": "07-version-control-git.html#why-version-control",
    "href": "07-version-control-git.html#why-version-control",
    "title": "8  Version control and collaboration",
    "section": "8.1 Why version control?",
    "text": "8.1 Why version control?\nGithub is a platform for collaborative coding. As we have noted before, collaboration concerns both others and you, in the future! This means that having a formal system for keeping track of your projects is a good thing.\nGithub also provides version control. Version control can help you track changes in your entire analysis or writing project. This is helpful when multiple files make up a complex project, including e.g. scripts, data and manuscript files. It is also helpful when multiple collaborators work together (e.g. writing a report). You will, by using version control, avoid overwriting other peoples work. With multiple changes made to the project, merging will create the latest up-to-date version. When you change a file in your analysis you will be required to describe the changes you have made. Git creates a record of your changes. This also means that we have “backups” of previous versions."
  },
  {
    "objectID": "07-version-control-git.html#three-ways-of-hooking-up-to-github",
    "href": "07-version-control-git.html#three-ways-of-hooking-up-to-github",
    "title": "8  Version control and collaboration",
    "section": "8.2 Three ways of hooking up to GitHub",
    "text": "8.2 Three ways of hooking up to GitHub\n\n8.2.1 Create a new repository on GitHub and clone it\nAccess your personal GitHub account and click New under repositories. This is equivalent to going to www.github.com/new. GitHub will ask for a repository name, a description and whether you want the repository to be public or not. You can also chose to add a Readme-file.\nNames and descriptions are important, a better name and description makes it easier for you and others to find and make use of your repository. Even when making repositories for school assignments, a good name will likely make it more re-usable in the future. The same is true for the readme file. So, name the repository with a descriptive name, write a short description with the purpose of the repository and add a readme-file to the repository.\nA public repository is open for everyone, private repositories have restricted access.\nOnce the repository is created you can clone it. This means that you will copy the content to your local machine (PC/Mac). In RStudio this is most conveniently done by starting a new RStudio project and selecting Version Control in the project menu. You will be asked to copy the address shown under “Code” on GitHub.\n\n\n8.2.2 Create an online repository from a local folder\nLet’s say that we have a local folder that is a RStudio Project, without version control and we want to create a online repository together with version control. We can use GitHub desktop to accomplish this or GitHub CLI.\nUsing the terminal and GitHub desktop:\n\nThe first step is to make the local folder a git repository, in RStudio with the project running go to a terminal and type git init. The terminal will let you know that you have initialized a git repository.\nStart up GitHub desktop, under File choose Add local repository and find the folder on your computer where you have your RStudio project. Once open in GitHub desktop you will see all changes and additions of new files.\nCommit your changes by writing a first commit message, and possibly a longer description of the commit.\nClick “Publish repository”, you will be asked to edit the name and description of the repository and choose whether to have the repository private or not (see above for recommendations).\nGo to GitHub.com and check if the repository is published.\n\nUsing the terminal and GitHub terminal client (CLI):\n\nBe sure to be in your RStudio project and use the terminal in RStudio to initiate a git repository, type git init in the terminal.\nAlso in the terminal type gh repo create, this will guide you through the same process as with GitHub desktop but all selections are done in the terminal.\n\n\n\n8.2.3 Create an online repository from a local git repository\nIf you have already initialized a RStudio project as a git repository you can follow the steps above without the git init command. Using git init on an already initialized git repository will reinitialize it. This will not remove git history of the repository (see here for documentation)."
  },
  {
    "objectID": "07-version-control-git.html#git-commands-and-workflows",
    "href": "07-version-control-git.html#git-commands-and-workflows",
    "title": "8  Version control and collaboration",
    "section": "8.3 Git commands and workflows",
    "text": "8.3 Git commands and workflows\n\n8.3.1 Add, commit and push\nThe day to day workflow when working on a git project involves making changes to your files and saving those changes locally, and in the version control system. By the end of the day you might also want to make sure all changes are synchronized with the online repository.\nThis workflow includes the git commands add, commit and push.\nUsing the terminal git add &lt;filename&gt; or git add -A adds a specific file or all changes to a list of changes to be “committed” into version history. The equivalent operation in GitHub desktop is checking all boxes under changes. This is done automatically and you have to uncheck files or changes that you do not want to commit to history.\nIn the terminal we can commit changes to the git history using the command git commit -m \"a commit description message\" the additional part -m \"a commit... is the required commit message. It is good to be informative if you need to find a specific change to a file. In GitHub desktop this is easily done by writing a commit message under summary in the bottom left corner once you have changes in your repository.\nThe last step, git push, means that you are uploading all changes to the online repository. This will update the repository on GitHub, your version history is now up to date in your online repository. This also means that you have an online backup of your work.\nIn GitHub desktop we can examine the commit history of a project by looking in the History tab. Using the web interface at www.github.com we can get an overview of all commits by clicking Activity or commits in the repository view. Using the command line we can look at the the commit history by using git log. This will list all commits and you can scroll trough them by pressing enter. To exit this list press q.\nFurther descriptions of a certain state of the repository can be added using tags. Using a tag we can make a note of a certain state of the repository, for example when an assignment is ready to exam or when a journal article is ready for submission. In GitHub desktop we might want to put a tag on a specific commit. We can do this by right-clicking on a commit in the history followed by Create tag. Using the command line, tags are added with git tag, see the git documentation for details.\n\n\n8.3.2 Collaboration, pull, clone and fork\nCollaboration is most often done with yourself in the future. The git pull command (using the terminal) downloads all changes to your working directory. You want to do this when you have changes in the online repository that is not synchronized with the local repository. This might be the case if you have made changes to your repository on GitHub, like added a readme file. Or if you are collaborating with someone who have made changes to the repository. I work on multiple computers, sometimes on the same repository, the online repository is where a keep the most up to date version of my project.\nUsing GitHub desktop, we can click Fetch origin to get the latest changes from the online repository. GitHub desktop will suggest to pull these changes to the working directory after you have done this operation.\nWe have already covered git clone, this essentially means downloading an online repository to your local machine. This is most easily done while initializing a new RStudio project.\nA fork is a copy of someones online repository that is created as a new repository under your user. You now have access to this repository and can make changes. The repository can have its own life or be used to create changes that later are suggested as changes to the “parent repository”.\nIn this course you can fork a template for the portfolio exam. This is an example where your fork will have its own life.\nIf a fork is used to suggest changes this is done through a pull request. Using the web client (GitHub), we can click create pull request when inside a forked repository. This will take you a few steps where you are expected to describe changes to the repository. The original author will get a notification to review the pull request and can chose to incorporate the changes into the parent repository.\n\n\n8.3.3 Branches\nMuch like a fork, we can create copies of our own repository. These are called branches. A branch might contain changes that we want to try out before we make it the “official” version of our repository. These changes can include experiments that might mess up things or break code.\nUsing GitHub desktop we can create a new branch by clicking Current branch in the upper left and then Create branch. In GitHub desktop it is easy to switch between branches.\nUsing the command line we can create a branch by typing git branch &lt;new-branch-name&gt; where &lt;new-branch-name&gt; is a name of the branch that you choose. Using the command git checkout &lt;new-branch-name&gt; we switch to the new branch from the current branch, which is often called master. We can use git checkout master to get back to the original branch. If you switch between branches while working in both your work can be saved using a commit.\n\n\n8.3.4 Conflicts\nA conflict emerges when two versions of a files cannot be merged into one. The web client will check if, for example, two branches will be possible to merge. If you try to merge two versions of a file that have different changes made to the same line you will get a message from GitHub desktop or on the command line tool. A conflict needs to be resolved manually.\nFind the file that is affected by the conflict. This is possible to do by using git status on the command line. Next, find the lines that are affected by the conflict and edit them to what should be correct content. A conflict can be seen in the file as\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:filename.file\n\ncontent\n\n=======\n\n\nother content \n\n&gt;&gt;&gt;&gt;&gt; branch:filename.file\n\nIn the above example, “content” and “other content” is the content of the two conflicting versions of the file (filename.file). You have to pick one or write something else instead. You also need to remove markers (&lt;&lt;&lt;, ==== and &gt;&gt;&gt;). When you are satisfied with the changes commit the changes. This will resolve the conflict."
  },
  {
    "objectID": "07-version-control-git.html#additional-great-things-about-github",
    "href": "07-version-control-git.html#additional-great-things-about-github",
    "title": "8  Version control and collaboration",
    "section": "8.4 Additional great things about GitHub",
    "text": "8.4 Additional great things about GitHub\nGitHub has great capabilities for managing projects. You can for example:\n\nPost issues that are suggestions or questions regarding a repository. Issues can be categorized with labels and assigned.\nYou can create to-do lists in the Projects tab (in the web interface). This could be a nice way of sharing and tracking the progress of a project.\nYou can build a wiki. This is simply a collection of pages that can be used to document the repository or a project (in a wider sense) that you are working on.\nAll of the above can be private and public. You can choose whom have access to your repository. This makes it easy to work on a project even if you need to keep things a secret."
  },
  {
    "objectID": "07-version-control-git.html#when-will-this-knowledge-be-handy",
    "href": "07-version-control-git.html#when-will-this-knowledge-be-handy",
    "title": "8  Version control and collaboration",
    "section": "8.5 When will this knowledge be handy?",
    "text": "8.5 When will this knowledge be handy?\nWhen writing your master thesis, it will be extremely easy to share your code with your supervisor or other students, whit whom you collaborate. You can just invite someone to make changes in your repository and then download them. As noted several times before, your most frequent collaborator is you. Using git makes it easy to keep track of changes in your project and it keeps your most frequent collaborator from messing up your work.\nVersion control workflows are part of almost all technology companies, and will most certainly be part of many more types of businesses, institutions and workplaces in the future as we need to collaborate on large, complex projects. Knowing about these systems is in that sense quite handy!"
  },
  {
    "objectID": "07-version-control-git.html#resources",
    "href": "07-version-control-git.html#resources",
    "title": "8  Version control and collaboration",
    "section": "8.6 Resources",
    "text": "8.6 Resources\nThere are of course more functions in git, here are some resources for deeper understanding.\n\nExtensive resources can be found on Happy Git and GitHub for the useR\nKarl Broman provides a “minimal tutorial”\nGitHub hosts resources for learning Git\nGit documentation, here you will find all git commands"
  },
  {
    "objectID": "07-version-control-git.html#footnotes-and-references",
    "href": "07-version-control-git.html#footnotes-and-references",
    "title": "8  Version control and collaboration",
    "section": "8.7 Footnotes and references",
    "text": "8.7 Footnotes and references"
  },
  {
    "objectID": "08-phys-lab-reliability.html#smallest-worthwhile-change-or-effect",
    "href": "08-phys-lab-reliability.html#smallest-worthwhile-change-or-effect",
    "title": "9  Reliability in the physiology lab",
    "section": "9.1 Smallest worthwhile change or effect",
    "text": "9.1 Smallest worthwhile change or effect\nWhen we do not have any measures of reliability, nor clinically relevant thresholds for a test, the smallest worthwhile effect may provide an indication of important changes in a test. An arbitrary number of the smallest effect of interest in an average change score has been defined as 20% of the between participant standard deviation (\\(0.2\\times SD\\)). This tells you nothing about the reliability of your test, it simply gives a proportion of the expected population variation.\n\n\n\n\nHopkins, W. G. 2000. “Measures of Reliability in Sports Medicine and Science.” Journal Article. Sports Med 30 (1): 1–15. http://www.ncbi.nlm.nih.gov/pubmed/10907753."
  },
  {
    "objectID": "10-mol-lab-intro.html#health-and-safety-in-the-lab",
    "href": "10-mol-lab-intro.html#health-and-safety-in-the-lab",
    "title": "10  Introduction to the molecular exercise physiology lab",
    "section": "10.1 Health and safety in the lab",
    "text": "10.1 Health and safety in the lab"
  },
  {
    "objectID": "10-mol-lab-intro.html#good-laboratory-practice",
    "href": "10-mol-lab-intro.html#good-laboratory-practice",
    "title": "10  Introduction to the molecular exercise physiology lab",
    "section": "10.2 Good laboratory practice",
    "text": "10.2 Good laboratory practice"
  },
  {
    "objectID": "10-mol-lab-intro.html#the-laboratory-journal",
    "href": "10-mol-lab-intro.html#the-laboratory-journal",
    "title": "10  Introduction to the molecular exercise physiology lab",
    "section": "10.3 The laboratory journal",
    "text": "10.3 The laboratory journal\nYour laboratory journal fills several purposes. It helps you in keeping track of what you have done, why you did it, what were the results of your experiments and what were your conclusions. The journal also helps your collaborators (fellow students, supervisors, laboratory managers) to understand what you have done. In a broader perspective the laboratory journal can be viewed as a primary source of data for your prospective manuscript, and when the manuscript has been published, the journal is source data kept for records at your department."
  },
  {
    "objectID": "10-mol-lab-intro.html#keeping-it-digital---using-our-electronic-lab-journal",
    "href": "10-mol-lab-intro.html#keeping-it-digital---using-our-electronic-lab-journal",
    "title": "10  Introduction to the molecular exercise physiology lab",
    "section": "10.4 Keeping it digital - Using our electronic lab journal",
    "text": "10.4 Keeping it digital - Using our electronic lab journal\nAfter a couple of years running our lab in Lillehammer, we have arrived at the conclusion that we need to organize the molecular lab using a electronic laboratory journal system. This will solve several issues with organizing laboratory work like storage of data, collaborating, running multiple projects in parallel, having guests and students contributing to projects etc.\nTraditionally, laboratory journals are stored in a fire safe cabinets in the lab, in notebooks. In some labs, each page must be “read and understood” and subsequently signed by a lab administrator or principle investigator. People working in laboratories typically have strong feeling about their laboratory journal system, regardless of digital or analog alternatives. The above observations indicate that, as a new lab member you should take care to learn the laboratory journal system and culture of the lab. This will make your work more important to the lab, as it will contribute to the collective knowledge based on the groups work.\nOur electronic laboratory journal system can be found at elab.inn.no. As a student at inn you may log in using your student username and password. The system has link to the documentation of the software which can also be found through this web address: doc.elabftw.net/. As a student you will be added to the group related to this course.\n\n10.4.1 What to write in the journal\nA laboratory journal should consist of all necessary information needed to replicate your experiments. The tricky thing is that you do not always know what information is needed to do a replication. This means that you will need to include all relevant information about your experiment, and a bit more, within reasonable limits.\n\n\n10.4.2 Structure of an entry\nIn the electronic journal (elab.inn.no), entries are recorded as experiments. An experiment starts on a specific date, it has a status, a title and tags can be added. This will make it easy to keep track of the entry. eLabFTW will keep a specific record of your entry and time stamp it. The actual entry has three parts, namely purpose, methods and results. These are suggested in the template when you start a new entry.\nThe purpose of an experiments should give a clear description of why you are performing the experiment. This description could be “To extract and analyze DNA for ACTN3 genotyping”. Further descriptions could be necessary if you are performing more complex experiments. Maybe you found something out in previous experiment that needs to be validated in this experiemnt. If you spend some time to write this up, your subsequent report will be easier to write and your collaborators will understand what you did.\nThe methods in an experiment “need to include all relevant information about your experiment, and a bit more, within reasonable limits”, as already mentioned above. A detailed protocol should be included. If you are using pre-written protocols, this entry should refer to this protocol and include any changes made to the protocol. You should also note batch numbers on chemicals and reagents etc. If an experiment fail, a complete record of reagents and machines etc. will make it easier to track potential methodological issues.\nThe method describes the experiment, e.g. by a step-by-step approach. This section includes recipes of solutions, what samples were used, in what order different steps of the protocol was done, if any problems occurred and so on. Remember to write explicit, you might remember tomorrow, but in one week or one year, you have no idea. A method section (as the other sections) can refer back in the laboratory journal system. For example, maybe you prepared and validated the lysis buffer in a previous experiment, write this and refer to the experiment describing the buffer. A part of a method section can look something like this:\n\nOverview: Freeze dried muscle samples are homogenized in lysis buffer (Refer to experiment: Lysis buffer test) and protein concentrations are determined in the plate reader (raw data included below). Based on protein concentrations, supernatant is normalized to a common 2 μg/μl. Step-by-step: 1. Freeze dry muscle over-night (at least 20 h). 2. Dissect away fat, connective tissue and blood 3. Move ~2 mg to a new tube and add 80 μl of ice-cold lysis buffer per mg of tissue. 4. …”\n\nFinally, the results should be a description of the actual results, and a description of what this means. If you would pick up your experiment later this will help you understand the results. This also applies to collaborations. In eLabFTW you can include raw data files. This will be the primary way of storing raw data from machines and instruments. Importantly, eLabFTW will not give you a way of structuring large amount of data. This step could instead be done outside eLabFTW.\n\n\n10.4.3 Relationship between the laboratory journal and your samples, solutions, tubes etc.\nWhen you do work in the laboratory, you will notice that you accumulate a lot of micro-centrifuge tubes, boxes full of intermediate sample preparations, solutions etc. To keep track of all this you need to “connect” the content of your laboratory journal entries to the place where you keep all your stuff. For example, when marking a new cryo-box, this should be done with information about what is the content of the box, the date when the box was “started”, your name, and the experiment. Your name, date and experiment can be tracked back to your laboratory journal entries. This means that when your collaborator finds a mystical “Sample X1 2017 10 10, experiment Y”, she can go back to your journal and find out what you did. In summary: label everything!"
  },
  {
    "objectID": "10-mol-lab-intro.html#protocols-and-experiments-in-the-course",
    "href": "10-mol-lab-intro.html#protocols-and-experiments-in-the-course",
    "title": "10  Introduction to the molecular exercise physiology lab",
    "section": "10.5 Protocols and experiments in the course",
    "text": "10.5 Protocols and experiments in the course\nProtocols used in the course are as for now available at trainome.github.io."
  },
  {
    "objectID": "11-the-linear-model.html#resources",
    "href": "11-the-linear-model.html#resources",
    "title": "11  The linear model",
    "section": "11.1 Resources",
    "text": "11.1 Resources\n\nChapter 5 in Spiegelhalter serves as a very good introduction to regression.\nThere are several texts on Regression, Navarro provides a nice introduction with references to R in Learning statistics with R, Chapter 15."
  },
  {
    "objectID": "11-the-linear-model.html#straight-lines",
    "href": "11-the-linear-model.html#straight-lines",
    "title": "11  The linear model",
    "section": "11.2 Straight lines",
    "text": "11.2 Straight lines\nA straight line can be used to describe a relationship between two variables. This relationship can also be described with a formula:\n\\[y = \\beta_0 + \\beta_1x\\]\nWhere \\(y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope and \\(x\\) is the predictor.\n\n\n\n\n\n\n\n\n\nIn the model shown above, \\(y\\) increases two units for every unit increase in \\(x\\). If we measure something in nature and find such a fit (every point on the line) we should check our calculations as perfect relationships are seldom found in measured variables. This because of measurement error and other non measured variables that affect the relationship. We could also add some details to the notation presented above. For a specific observation (\\(y_i\\)), we quantify unmeasured sources of variation in an additional term in the formula\n\\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\]\n\\(\\epsilon\\) is the error-term. Here we quantify the distance from the best fit line, or the estimated value for each observation (\\(\\hat{y}_i\\)) to the observed value (\\(y_i\\)). The best fit line is the line that minimizes the sum of the squared residual error, or the differences between estimated and observed values:\n\\[\\sum\\limits_{i=1}^{n}\\epsilon_i^2 = \\sum\\limits_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nA more realistic regression model contains some error.\n\n\n\n\n\n\n\n\n\nIn the above figure, the errors or residuals are highlighted as the distance (red lines) from the predicted (blue points) to the observed (black points).\nA two variable relationship can be positive (increase in \\(y\\) as \\(x\\) increases) or negative (\\(y\\) decreases as \\(x\\) increases)."
  },
  {
    "objectID": "11-the-linear-model.html#fitting-regression-models-in-r",
    "href": "11-the-linear-model.html#fitting-regression-models-in-r",
    "title": "11  The linear model",
    "section": "11.3 Fitting regression models in R",
    "text": "11.3 Fitting regression models in R\nThe data above can be fitted to a regression model in R using the lm() function. We will get far by specifying a formula and data where the variables used in the formula are stored. We can store a model as an object and inspect the results by using the summary() function.\n\ndf &lt;- data.frame(x = c(5.461851, 6.110910, 6.952707, 5.321775, 5.951849),\n                 y = c(9.168992,  8.273749,  5.926797, 10.745583,  7.999151))\n\nfit &lt;- lm(y ~ x, data = df)\n\nsummary(fit)\n\n\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n      1       2       3       4       5 \n-0.5561  0.2460  0.1005  0.6542 -0.4445 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  24.0085     2.6873   8.934  0.00296 **\nx            -2.6151     0.4488  -5.827  0.01007 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5789 on 3 degrees of freedom\nMultiple R-squared:  0.9188,    Adjusted R-squared:  0.8917 \nF-statistic: 33.95 on 1 and 3 DF,  p-value: 0.01007\n\n\nThe summary that you get from the model above will show the value of the coefficient (estimates, standard error, t-value and a p-value), we will get some information about the spread of the residuals and values that tells us the overall fit of the model.\nThe estimates from a summary in a two variable situation tells the value of \\(y\\) when x is zero (the intercept) and the increase in \\(y\\) for every unit increase in \\(x\\) (the slope). Can you identify these from the output above?\nTwo-variable regression (univariate regression) is closely related to the correlation. Try out the code cor.test(df$x, df$y) and see what similarities you find between the outputs.\nIn the model we use in the example, the intercept is quite “far away” from the rest of the data (see figure below). This is, as noted above, because the intercept is the value of \\(y\\) when \\(x\\) is set to zero.\n\n\n\n\n\n\n\n\n\nLet’s fit some real data. We might wonder if there are some characteristic that is related to VO2max. For example, do taller individuals have greater VO2max? It is always a good idea to start with a plot before we do the modeling.\n\nlibrary(tidyverse)\nlibrary(exscidata)\n\n\n\nexscidata::cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n         ggplot(aes(height.T1, VO2.max)) + \n  geom_point(size = 3, \n             fill = \"lightblue\", \n             shape = 21) +\n         labs(x = \"Height (cm)\", \n              y = expression(\"VO\"[\"2max\"]~(ml^-1~min^-1))) +\n         theme_minimal()\n\n\n\n\nHeight and VO2max from the cycling study data set\n\n\n\n\nThere might be a positive relationship, what do you think? You might get a clearer picture if you use geom_smooth(method = \"lm\") in your ggplot command, try it out (or see below)!\nTo quantify the relationship between Height (height.T1) and VO2max (VO2.max) we can fit a linear model. Below I store the model in an object called m1. Before we look at the results of the regression model, we should think about the data and inspect the fit to see if it matches with our assumptions. Assumptions that generally needs to be met in order to get a valid regression model are:\n\nIndependent observations. This is an assumption about the design of the study and the data at hand. If we have observations that are related, the ordinary linear model will give us biased conclusions. As an example, if we collect data from the same participants over time we will not have independent observations and this will lead to pseudo-replication, lower standard errors and biased confidence intervals. Another way to see it is that non-independent observations will give non-independence of the residuals which is the mechanism that creates bad inference (as the residuals are used to estimate the sampling distribution of parameters).\nLinear relationship. In the basic case, we expect a linear trend that can be described with a straight line. If the relationship is curve-linear, we may adjust the fit using e.g. polynomials. The relationship between Height and VO2max is plotted in Figure 11.1 A and highlighted with a best fit line.\nNormal residuals. This condition might be violated when there is an outlier. Residuals from a model of the relationship Height and VO2max are plotted in Figure 11.1 B and C.\nConstant variance. This assumption says that we want to be equally wrong all along the explanatory variable. If we predict \\(y\\) with greater error at large \\(x\\) we have heteroscedasticity (unequal variance), if we are “equally wrong” we have homoscedasticity (equal variance). Residuals from a model of the relationship Height and VO2max are plotted in Figure 11.1 D against the predictor variable (Height).\n\n\n11.3.0.1 Code for fitting a preliminary model\n\n\nFitting the model and plotting the data\nlibrary(exscidata)\nlibrary(ggtext)\nlibrary(cowplot)\n\n\ncyc_select &lt;-  cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) \n\nm1 &lt;- lm(VO2.max ~ height.T1, data = cyc_select)\n\n\n## Plotting the raw data together with a best-fit line\n\nfiga &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n  ggplot(aes(height.T1, VO2.max)) +\n    geom_point(size = 3, \n             fill = \"lightblue\", \n             shape = 21) +\n  \n  geom_smooth(method = \"lm\", se = FALSE) +\n  \n         labs(x = \"Height (cm)\", \n              y = expression(\"VO\"[\"2max\"]~(ml^-1~min^-1))) +\n         theme_minimal()\n\n## Extracting and plotting residuals\n\nfigb &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n  mutate(yhat = fitted(m1)) %&gt;%\n  \n  ggplot(aes(height.T1, VO2.max, group = subject)) +\n    \n  geom_segment(aes(y = yhat, yend = VO2.max, x = height.T1, xend = height.T1), \n               color = \"red\") +\n  \n  geom_point(size = 3, \n             fill = \"lightblue\", \n             shape = 21) +\n  \n  geom_point(aes(height.T1, yhat), \n             size = 3, fill = \"orange\", shape = 21) + \n  \n  \n  \n  geom_smooth(method = \"lm\", se = FALSE) +\n  \n         labs(x = \"Height (cm)\", \n              y = expression(\"VO\"[\"2max\"]~(ml^-1~min^-1))) +\n         theme_minimal()\n  \n figc &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n  mutate(yhat = fitted(m1), \n         resid = resid(m1)) %&gt;%\n \n   \n   ggplot(aes(resid)) + \n   \n      \n   geom_density(aes(resid),\n              \n                color = \"orange\") + \n     geom_rug(color = \"orange\") +\n   \n   scale_x_continuous(limits = c(-1200, 1200)) +\n   \n   stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 425), \n                 color = \"steelblue\") +\n   \n   labs(x = \"Residuals\", \n        subtitle = \"The &lt;span style = 'color: orange;'&gt;observed residual distribution&lt;/span&gt; &lt;br&gt; and a &lt;span style = 'color: #4682b4;'&gt;Normal distribution&lt;/span&gt; with mean 0 and SD of 425\") +\n   \n   theme_minimal() + \n   theme(axis.text.y = element_blank(), \n         axis.ticks.y = element_blank(), \n         axis.title.y = element_blank(), \n         plot.subtitle = element_markdown())\n\n   \n \n   \nfigd &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n  mutate(yhat = fitted(m1), \n         resid = resid(m1)) %&gt;%\n  ggplot(aes(height.T1, resid)) + geom_point() + theme_minimal() + \n  labs(y = \"Residuals\", x = \"Height (cm)\")\n\n\nplot_grid(figa, figb, figc, figd, ncol = 2, labs = c(\"A\", \"B\", \"C\", \"D\"))\n\n\n\n\n\nFigure 11.1: An analysis of the model fit. In A we do have a pattern indicating a linear relationship between Height and VO2max. In B we visualize each observation together with its estimated value and the distance between the two (residuals). The residuals (c) are approximately normal, although we do not have a lot of data to really be confident. Finnaly we plot the distance between observed and estimated values against the predictor, we are approximately equally wrong along all values of Height (although we do not have a lot of data to really be confident)\n\n\n\n\n\n\n11.3.1 Linear relationship\nA plot (e.g. Figure 11.1) can be used to see if the relationship is generally linear. We do not have that many data points, but a curve-linear relationship is not evident.\n\n\n11.3.2 Normal residuals\nTo check if the residuals are normal (like Figure 11.1 C suggests), we can create a plot that plot every observed residual against its theoretical position in a normal distribution. This is a quantile-quantile plot. To show the concept we may sample data from a normal distribution and plot it against the theoretical quantile (Figure 11.2).\n\n\nShow the code\nset.seed(1)\nggplot(data.frame(y = rnorm(100, 0, 1)), aes(sample = y)) + \n  stat_qq(size = 3, fill = \"lightblue\", shape = 21) + \n  stat_qq_line() + \n  theme_minimal()\n\n\n\n\n\nFigure 11.2: An example of a quantile-quantile plot (qq-plot)\n\n\n\n\nThe code above samples 100 observations. They are plotted against their “theoretical values”. If the values (points) follows the straight line, we have data that follows a normal distribution. The same can be assessed from our fitted model (Figure 11.3).\n\n\nShow the code\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1)  %&gt;%\n        mutate(resid = resid(m1), \n               st.resid = resid/sd(resid)) %&gt;%\n        ggplot(aes(sample = st.resid)) +\n         stat_qq(size = 3, fill = \"lightblue\", shape = 21) + \n                 stat_qq_line() +\n                 theme_minimal()\n\n\n\n\n\nFigure 11.3: A qq-plot constructed from the residuals of our model\n\n\n\n\nThe resulting plot looks nice. Except from one or two observation, the residuals follows the normal distribution. This corresponds to our overlay in Figure 11.1 C. The qq-plot is however a more formal way of assessing the assumption of normally distributed errors.\n\n\n11.3.3 Constant variance\nThis assumption can be checked by creating a residual plot (e.g. Figure 11.1 D). We will do a variation of this plot below by hand to see how it works. The model is fitted and stored in the object m1. From this object we can use the residuals() function to get every residual. We can add this data to the data set by creating a new variable called resid. It is common practice to plot the residuals against the fitted values. We can get the fitted values using the fitted(), these are the predicted values from the model.\nWe will plot the fitted values against the residuals. If the model is equally wrong all along the fitted values (or the predictor values as in Figure 11.1 D), we have homoscedasticity. The residual plot should not show any obvious patterns.\n\n\nShow the code\ndata.frame(resid = resid(m1), \n           fitted = fitted(m1)) %&gt;%\n        ggplot(aes(fitted, resid)) + \n        geom_hline(yintercept = 0) +\n        geom_point(size = 3, fill = \"lightblue\", shape = 21) +\n        theme_minimal()\n\n\n\n\n\nFigure 11.4: Residuals plotted against the fitted values from our model of VO2max against Height\n\n\n\n\nSometimes you will see standardized residuals. This is the residual divided by the standard deviation of the residual. We can create this standardization like this:\n\n\nShow the code\ndata.frame(resid = resid(m1), \n           fitted = fitted(m1)) %&gt;%\n        mutate(st.resid = resid/sd(resid)) %&gt;%\n        ggplot(aes(fitted, st.resid)) + \n        geom_hline(yintercept = 0) +\n        geom_point(size = 3, fill = \"lightblue\", shape = 21) +\n        theme_minimal()\n\n\nLooking at the plot (?fig-resid-standard) tells us that observation with the largest error is about 2.5 standard deviation away from its predicted value. We are suffering a bit from having a small amount of data here. But the residual plot does not invalidate the regression model."
  },
  {
    "objectID": "11-the-linear-model.html#check-the-results",
    "href": "11-the-linear-model.html#check-the-results",
    "title": "11  The linear model",
    "section": "11.4 Check the results",
    "text": "11.4 Check the results\nTo examine the results of the analysis we can use the summary() function.\n\nsummary(m1)\n\n\nCall:\nlm(formula = VO2.max ~ height.T1, data = cyc_select)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-619.99 -279.54  -32.56  181.82 1109.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -2596.26    2936.51  -0.884   0.3883  \nheight.T1      41.10      16.37   2.511   0.0218 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 436.8 on 18 degrees of freedom\nMultiple R-squared:  0.2594,    Adjusted R-squared:  0.2183 \nF-statistic: 6.306 on 1 and 18 DF,  p-value: 0.02179\n\n\nThe output (see above) will show you the following things: - Call this summary contains your instructions to R. - Residuals which contains the minimum, maximum, median and quartiles of the residuals. The tails should be approximately similar above and below the median. - Coefficients contains the estimates and their standard errors. As we have fitted a univariate model, we only see the increase in VO2max with every unit increase of height.T1 and the intercept. - R-squared and additional statistics: shows the general fit of the model, the R squared value is a value between 0 and 1 where 1 shows if the data fits perfectly.\n\n11.4.1 A note about printing the regression tables\nWe might want to print the regression table (coefficients) in our reports. To do this in a nice way we might want to format the output a bit. This can be done using a package called broom. broom is not part of the tidyverse so you might need to install it. The package has a function called tidy that takes model objects and formats it into nice data frames that are more easy to work with. Together with the gt package we can create tables for use in the report. The function gt makes nice tables with some arguments to format the table.\n\nlibrary(gt); library(broom)\n\ntidy(m1) %&gt;%\n  gt() %&gt;%\n  fmt_auto()\n\n\n\n\n\nTable 11.1:  A partly formatted regression table \n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−2,596.261\n2,936.508\n−0.884\n0.388\n    height.T1\n    41.105\n   16.369\n 2.511\n0.022"
  },
  {
    "objectID": "11-the-linear-model.html#interpreting-the-results",
    "href": "11-the-linear-model.html#interpreting-the-results",
    "title": "11  The linear model",
    "section": "11.5 Interpreting the results",
    "text": "11.5 Interpreting the results\nFrom our model we can predict that a participant with a height of 175 cm will have a VO2max of 4597 ml min-1. We can do this prediction by combining the intercept and the slope multiplied with 175 as x-value, as we remember the equation for our model:\n\\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_i \\\\\nVO_{2max} &= -2596.3 + 41.1 \\times 175 \\\\\nVO_{2max} &= 4597\n\\end{align}\\]\nActual values from the regression table can be accessed from a tidy table created with broom. But we can also use coef() to get the coefficients. Using confint() we will get confidence intervals for all parameters in a linear model.\n\n# Coefficients\ncoef(m1)\n\n# Confidence intervals\nconfint(m1)\n\nWe will talk more about confidence intervals, t-values and p-values in later chapters. For now, a small introduction may be enough. The confidence interval can be used for hypothesis testing, so can also p-values from the summary table. The p-values tests against the null-hypothesis that the intercept and slope are 0. What does that mean in the case of the intercept in our model? The estimated intercept is -2596 meaning that when height is 0 the VO2max is -2596. We are very uncertain about this estimate as the confidence interval goes from -8766 to 3573. We cannot reject the null. Think a minute about what information this test may give in this situation.\nThe slope estimate has a confidence interval that goes from round(confint(m1)[2], 1) to 75.5 which means that we may reject the null-hypothesis at the 5% level. The test of the slope similarly test against the null-hypothesis that VO2max does not increase with height. Since our best guess (the confidence interval) does not contain zero, we can reject the null hypothesis."
  },
  {
    "objectID": "11-the-linear-model.html#do-problematic-observations-matter",
    "href": "11-the-linear-model.html#do-problematic-observations-matter",
    "title": "11  The linear model",
    "section": "11.6 Do problematic observations matter?",
    "text": "11.6 Do problematic observations matter?\nIn the residual plot we could identify at least one potentially problematic observation. We can label observations in the residual plot to find out what observation is problematic.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1)  %&gt;%\n        mutate(st.resid = resid(m1)/sd(resid(m1)), \n               fitted = fitted(m1)) %&gt;% \n        ggplot(aes(fitted, st.resid, label = subject)) + \n        geom_hline(yintercept = 0) +\n        geom_point(size = 3, fill = \"lightblue\", shape = 21) +\n        geom_label(nudge_x = 25, nudge_y = 0) +\n        theme_minimal()\n\nThe plot shows that participant 5 has the largest distance between observed and predicted values. If we would fit the model without the potentially problematic observation we can see if this changes the conclusion of the analysis.\n\ncyclingStudy_reduced &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\", \n               subject != 5) %&gt;%\n        select(subject, group, VO2.max, height.T1) \n\nm1_reduced &lt;- lm(VO2.max ~ height.T1, data = cyclingStudy_reduced)\n\n\ndelta_beta &lt;- 100 * (coef(m1_reduced)[2]/coef(m1)[2] - 1)\n\nThe delta_beta above calculates the percentage change in the slope as a consequence of removing the observation with the greatest residual. The slope changes 13% when we remove the potentially problematic observation. This might be of importance in your analysis. Another way to look for potential influential data points would be to check the scatter plot.\n\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1) %&gt;%\n         ggplot(aes(height.T1, VO2.max, label = subject)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(size = 3, fill = \"lightblue\", shape = 21) +\n         labs(x = \"Height (cm)\", \n              y = expression(\"VO\"[\"2max\"]~(ml^-1~min^-1))) +\n                geom_label(nudge_x = 1, nudge_y = 0) +\n        \n        \n         theme_minimal()\n\nThe plot will show participant 5 has not got a lot of “weight” in the slope. If an equally big residual would have been present in the far end of the range of the height variable, removing it would have made more difference. Since the observation is in the middle of the x’s, it wont be that influential.\nThere are many ways of doing diagnostics for the ordinary linear model in R. The simplest way is to write plot(m1), this will produce four graphs.\n\nResiduals vs. Fitted shows the fitted (or predicted) values against the residuals. If we would have tried to fit a linear trend to curve linear data, we would have catch it here. We want equal spread all along the fitted values. We test the assumption of homoscedasticity and linear trend.\nNormal Q-Q shows residual theoretical quantiles against the observed quantile. The points should to a large degree be on, or close to the line. We test the assumption of normality in the residuals.\nScale location similarly to the residual plot, we can assess assumptions of heteroscedasticity and if we find the trend in the data. We are looking for a straight, flat line and points equally scattered around it.\nResidual vs. Leverage is good to find influential data points. If a point is outside the dashed line it changes the conclusion of the regression to a large degree. Remember that we identified participant 5 as a potential problematic case. The Residual vs. leverage shows that number 5 has a large residual value but no leverage, meaning that it does not change the slope of the regression line."
  },
  {
    "objectID": "11-the-linear-model.html#a-more-intepretable-model",
    "href": "11-the-linear-model.html#a-more-intepretable-model",
    "title": "11  The linear model",
    "section": "11.7 A more intepretable model",
    "text": "11.7 A more intepretable model\nThe intercept in model m1 is interpreted as the VO2max when height is zero. We do not have any participants with height zero nor will we ever have. A nice modification to the model would be if could get the intercept to tell us something useful. We could get the model to tell us the VO2max in the tallest or shortest participant by setting them to zero. Even more interesting would be to get the VO2max at the average height.\nWe accomplish this by mean centering the height variable. We remove the mean from all observations, this will put the intercept at the mean of heights as the mean will be zero.\n\ncyc_select &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height.T1)  %&gt;%\n        mutate(height.mc = height.T1 - mean(height.T1)) # mean centering the height variable\n\nm2 &lt;- lm(VO2.max ~ height.mc, data = cyc_select)\n\nExamine the fit, what happens to the coefficients?"
  },
  {
    "objectID": "11-the-linear-model.html#an-exercise",
    "href": "11-the-linear-model.html#an-exercise",
    "title": "11  The linear model",
    "section": "11.8 An exercise",
    "text": "11.8 An exercise\nWe think that body dimensions influence physiological characteristics. To test if if the stature (height.T1) influence maximum ventilatory capacity (VE.max) fit a regression model, check model assumptions and interpret the results.\n\nHere is a possible solution\n\n\n## Load data\n\ncyc_select &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VE.max, height.T1)  %&gt;%\n        mutate(height.mc = height.T1 - mean(height.T1)) # mean centering the height variable\n\n# fitting the model\nm1_ve &lt;- lm(VE.max ~ height.mc, data = cyc_select)\n\n# Check assumptions\nplot(m1_ve)\n\n# Check coefficients\nsummary(m1_ve)\n\n# Get confidence intervals\nconfint(m1_ve)"
  },
  {
    "objectID": "12-linear-and-curvlinear-relationships.html#predicting-from-data",
    "href": "12-linear-and-curvlinear-relationships.html#predicting-from-data",
    "title": "12  Linear and curve-linear relationships, and predictions",
    "section": "12.1 Predicting from data",
    "text": "12.1 Predicting from data\nBecause of the relationship between inner dimensions (such as the heart chambers) and our height, we might expect to see a relationship between body height and VO2max. The idea is that we will build a model and use this model to make predictions of our outcome with new data. Later we will hopefully see that this is one of the meany benefits of the powerful regression model technique.\nAs a first step, it is a good idea to get a visual representation of the prospective model (Figure 12.1). In the code chunk below we load the cyclingstudy data set from the exscidata package together with loading tidyverse. We then plot the relationship between height and VO2.max. In the ggplot call, a good starting point is to use geom_point together with geom_smooth which will produce a scatter plot with a best fit line. Notice that method = \"lm\" and se = FALSE are being used to make sure you get a straight line (method = \"lm\") and no confidence bands (se = FALSE).\n\n\nCode\nlibrary(tidyverse)\nlibrary(exscidata)\n\n\n# A simple plot of the association\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height = height.T1)  %&gt;%\n\n  ggplot(aes(height, VO2.max)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  theme_minimal()\n\n\n\n\n\nFigure 12.1: Assessing a linear reliationship between variables\n\n\n\n\nWe will now construct a model object containing the same straight line that we have visualized above. The lm function (for linear model) takes a formula and a data set as its arguments. We have to save the output of the model in an object to be able to work with it down the line. In the code below I suggest storing the model object as m1.\n\n\nCode\n# Store the data set needed in the model fitting \ndat &lt;- cyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height = height.T1) \n\n# Fit the model \nm1 &lt;- lm(VO2.max ~ height, data = dat)\n\n\nThe above code will store an object in your environment. Using this object, we may now make predictions. The manual way of making a prediction would be to get the coefficients from the model and use them to calculate predicted values based. In the code chunk below, I retrieve coefficients from the model representing the intercept and slope of the model. Remembering the basic mathematics of this simple model tells us that we can predict VO2max using the estimates from the model. These estimates can be retrieved using coef(). The intercept will be the first coefficient (coef(m1)[1]), and the slope will be the second (coef(m1)[2]). Adding them together and multiplying the slope with some new data representing height will get us the prediction.\n\n\nCode\nnew_height &lt;- 179\n\nprediction &lt;- coef(m1)[1] + coef(m1)[2] * new_height \n\n\nOur model predicts that an individual that is 179 cm tall will have a VO2max of 4761 ml min-1.\nR has some built-in functions for this kind of operation. We can use the predict function to calculate what each observation would look like if it were “on the regression line.” Using predict on the model without new data will give you the same values as the fitted function (Figure 12.2).\n\n\nCode\n# Store output\npred &lt;-  predict(m1)\nfit &lt;- fitted(m1)\n\n# Plot the values\ndata.frame(pred, fit) %&gt;%\n  ggplot(aes(pred, fit)) + geom_point()\n\n\n\n\n\nFigure 12.2: The function fitted(m1) returns the same predicted values as when we use predict(m1).\n\n\n\n\npredict has an argument called newdata; here, we can use a new data frame with the same predictors as in the data set used to fit the model. We may use this new data set to make several predictions from our model.\n\n\nCode\n# New Data Frame containing data we want to predict with \nndf &lt;- data.frame(height = c(160, 170, 180, 190))\n\npredictions &lt;- predict(m1, newdata = ndf)\n\n\nIf you inspect predictions you will see that an increased height gives us higher predictions of VO2max. What would be your VO2max given this model?"
  },
  {
    "objectID": "12-linear-and-curvlinear-relationships.html#uncertanties-in-predictions",
    "href": "12-linear-and-curvlinear-relationships.html#uncertanties-in-predictions",
    "title": "12  Linear and curve-linear relationships, and predictions",
    "section": "12.2 Uncertanties in predictions",
    "text": "12.2 Uncertanties in predictions\nWhen we are interested in a model’s ability to predict values from new data, we might also be interested in a range of plausible values for our prediction. A prediction interval can be constructed based on the model to tell us where we can expect future observations with a specified level of certainty. The prediction interval has a definition that might be difficult to understand. In short, if we construct infinitely many models and 95% prediction intervals, 95% of the prediction intervals will contain the true value for a future predicted observation.\nIf we relax the mathematical and philosophical rigor, we can regard the prediction interval as an interval of plausible values for individual observations based on the model. The prediction interval accounts for uncertainty in the predicted mean and the uncertainty associated with individual observations.\nLet us visualize the prediction interval (Figure 12.3). The predict function can help us again. We will create a data frame with predicted values over the whole range of observed values in the data set. seq(from = min(dat$height), to = max(dat$height)) creates a sequence of values that goes from the minimum in the data to the maximum. We will then use geom_ribbon to plot them together with the observed data. Notice that we must transform the predicted values into a data frame and include the variables to match our original ggplot2 call.\n\n\nCode\n# Create predictions based on min to max observed height values\npred.vals &lt;- predict(m1, \n                     newdata = data.frame(height = seq(from = min(dat$height), \n                                                       to = max(dat$height))), \n                                          interval = \"predict\") %&gt;%\n  ## Transform to a data frame and add variables to correspond to our data set `dat`\n  data.frame() %&gt;%\n  mutate(VO2.max = fit, \n         height = seq(from = min(dat$height), to = max(dat$height)))\n\n# Plot the data and prediction intervals\ncyclingstudy %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        select(subject, group, VO2.max, height = height.T1)  %&gt;%\n\n  ggplot(aes(height, VO2.max)) +\n  geom_ribbon(data = pred.vals, # We need new data for this layer\n              aes(ymin = lwr, ymax = upr),  # Add lower and upper bounds\n              fill = \"steelblue\", # Fill with a nice color\n              alpha = 0.2) +  # Make the fill \"transparent\"\n  geom_point() +  # Add observed points from the original data set \n  geom_smooth(method = \"lm\", se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 12.3: Our model with 95% prediction intervals.\n\n\n\n\nIs the model any good? Well, a true value might as well be about 20% higher or lower than the prediction based on body height. It is up to us to consider if this is a good model for predicting VO2max.\nTo express the boundaries of our 95% prediction interval as a ratio to the predicted values we could use the code below.\n\npred.vals %&gt;%\n  mutate(upr = upr/fit , \n         lwr = lwr/fit) %&gt;%\n  print()"
  },
  {
    "objectID": "12-linear-and-curvlinear-relationships.html#the-workload-lactate-relationship",
    "href": "12-linear-and-curvlinear-relationships.html#the-workload-lactate-relationship",
    "title": "12  Linear and curve-linear relationships, and predictions",
    "section": "12.3 The workload-lactate relationship",
    "text": "12.3 The workload-lactate relationship\nIn the cyclingstudy data set, data from lactate threshold tests are recorded for each participant. We need to “wrangle” the data set a bit to get the data in a format more suitable for analysis. In the code below I will first select columns needed for the analyses and the filter to retain one participant and one time-point. These data are then converted from a wide to long (tidy) format using the pivot_longer function. Notice that each of the lactate columns starts with lac., this information can be used in pivot_longer when rearranging the data. In pivot_longer we also convert the values to numeric values using as.numeric. Finally, we plot the data.\nThe resulting plot (Figure 12.4), shows a point for every lactate measurement. We have also connected the dots with geom_line which draws straight lines between each point. The straight line can be used to interpolate values between the observed lactate values. This is a common technique to calculate a lactate threshold, often defined as the intensity at 4 mmol L-1.\n\n\nCode\nlibrary(tidyverse)\nlibrary(exscidata)\n\n\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Filter NA's from the data \n  filter(!is.na(lactate)) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) \n\n\n\n\n\nFigure 12.4: A workload and lactate relationship\n\n\n\n\nOur next figure shows the value of x (watt, intensity) when y (lactate) is set to 4 (Figure 12.5). The lines are added by eyeballing1 the expected value. This is all fine, we have an approximate lactate threshold.\n\n\nCode\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%  \n  # Filter NA's from the data \n  filter(!is.na(lactate)) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  # Adding straight lines at specific values\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\")\n\n\n\n\n\nFigure 12.5: Interpolation to estimate the lactate threshold, exercise intensity at 4 mmol L-1\n\n\n\n\nTo get a better approximation, we could make use of the curve-linear relationship between exercise intensity and lactate accumulation. The “curvier” the relationship, the more wrong the above approximation would be (as Yoda say, would)2. We can add a curve-linear model on top of our plot using the geom_smooth function in our ggplot call. In the code below, we will actually use several polynomial models together with a straight line to assess their fit (Figure 12.6).\n\n\nCode\nlibrary(ggtext)\n\ncyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n    # Filter NA's from the data \n  filter(!is.na(lactate)) %&gt;%\n  # Plot the data, group = subject needed to connect the points\n  ggplot(aes(watt, lactate, group = subject))  + \n  geom_line(lty = 2) +\n  geom_point(shape = 21, fill = \"lightblue\", size = 2.5) +\n  geom_hline(yintercept = 4, color = \"red\") +\n  geom_vline(xintercept = 341.5, color = \"blue\") +\n  # Adding a straight line from a linear model\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, color = \"#e41a1c\") +\n  # Adding a polynomial linear model to the plot\n  # poly(x, 2) add a second degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2), color = \"#377eb8\") +\n  # poly(x, 3) add a third degree polynomial model.\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 3), color = \"#4daf4a\") +\n\n  labs(subtitle = \"Modelling the lactate-workload relationship as&lt;br&gt;\n       &lt;span style = 'color: #377eb8;'&gt;A second degree polynomial&lt;/span&gt;&lt;br&gt;\n       &lt;span style = 'color: #4daf4a;'&gt;A third degree polynomial&lt;/span&gt;&lt;br&gt;\n       &lt;span style = 'color: #e41a1c;'&gt;A straight line&lt;/span&gt;\") +\n  theme(plot.subtitle = element_markdown())\n  \n  # #fee090 represent color codes in th HEX format, palettes for different color can be found \n  # here: https://colorbrewer2.org/\n\n\n\n\n\nFigure 12.6: Several linear models fitted to the same data, some captures the data better than others.\n\n\n\n\nA polynomial model effectively models our data as a curve-linear relationship. In the figure above, the third degree polynomial model seems to capture the data best. In this case, watt is added to the regression equation in the form of a third degree polynomial:\n\\[\\text{lactate} = \\beta_0 + \\beta_1\\text{watt} + \\beta_2\\text{watt}^2 + \\beta_3\\text{watt}^3\\]\nAs we have already seen above (Figure 12.6), this model captures the curve-linear relationship between workload and lactate that occurs when we perform an incremental exercise test.\nThis type of model is in many cases good enough for the purpose of determining a lactate threshold. It may however be terribly bad in other situations with other kinds of data as the curvature of the model will make useless prediction outside the data.\nAs we can see in Figure 12.6, the different models are not that different around the 4 mmol L-1 mark. However, the linear model is just wrong at around 300 watts, the second degree polynomial model is wrong at 275 watts. The third degree polynomial model does capture the data pretty well.\nWe may fit these models formally using lm and check their residuals. First we will store the data set in an object called lactate and the use this data set in several lm calls. The same formula can be used as in geom_smooth, but we must use the actual variable names.\n\n\nCode\nlactate &lt;- cyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n  # Remove NA (missing) values to avoid warning/error messages.\n  filter(!is.na(lactate))\n\n# fit \"straight line\" model\nm1 &lt;- lm(lactate ~ watt, data = lactate)\n\n# fit second degree polynomial\nm2 &lt;- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate)\n\n# fit third degree polynomial\nm3 &lt;- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate)\n\n\n\n# Store all residuals as new variables\nlactate$resid.m1 &lt;- resid(m1)\nlactate$resid.m2 &lt;- resid(m2)\nlactate$resid.m3 &lt;- resid(m3)\n\n\nlactate %&gt;%\n  # gather all the data from the models\n  pivot_longer(names_to = \"model\", \n               values_to = \"residual\", \n               names_prefix = \"resid.\", \n               names_transform = list(residual = as.numeric), \n               cols = resid.m1:resid.m3) %&gt;%\n  # Plot values with the observed watt on x axis and residual values at the y\n  ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) +\n  \n  geom_hline(yintercept = 0, lty = 2) +\n  \n  # To set the same colors/fills as above we use scale fill manual\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\"))\n\n\n\n\n\nFigure 12.7: Assessing the fit f different linear models on a exercise intensity to lactate accumulation relationship.\n\n\n\n\nThe third degree polynomial model finds the observed values best, followed by the second degree model. This is not strange as the polynomial model with increased degrees has more flexibility to fit to the data. A problem with polynomial models is that you cannot fit e.g. a forth degree polynomial model with only four data points. You may also encounter some over-fitting to for example, a bad measurement. Let’s settle for the third degree model.\nThe next step will be to predict x from y. Remember that we have modeled the effect of x on y, i.e. the effect of exercise intensity on lactate. Using predict we may easily predict a lactate value for a specific value of watt. Since we want the inverse prediction we have to use some tricks in our prediction. The code below creates a data set of intensity values watt using the seq function which basically creates a vector of number with a specific distance between them. We can then use this vector of numbers to predict lactate values and find the value closest to 4 mmol L-1.\n\n\nCode\n# new data data frame\nndf &lt;- data.frame(watt = seq(from = 225, to = 350, by = 0.1)) # high resolution, we can find the nearest10:th a watt\n\nndf$predictions &lt;- predict(m3, newdata = ndf)\n\n# Which value of the predictions comes closest to our value of 4 mmol L-1?\n# abs finds the absolute value, makes all values positive, \n# predictions - 4 givs an exact prediction of 4 mmol the value zero\n# filter the row which has the prediction - 4 equal to the minimal absolut difference between prediction and 4 mmol\nlactate_threshold &lt;- ndf %&gt;%\n  filter(abs(predictions - 4) == min(abs(predictions - 4)))\n\n\nOur best estimate of the lacatate threshold is 343. We have approximated the exercise intensity at a specific value of lactate.\n\n12.3.1 More on lactate thresholds\nA fixed lactate threshold might not be the most informative marker of exercise performance. If we instead determine the exercise intensity when lactate has increased a certain amount from a baseline value it turns out that we will be able to predict real-world performance more precisely (Carlsson et al. 2012). Similarly, assessing the point of a lactate curve with the largest perpendicular distance to a straight line between the first and last observation also captures a relative increase rather than a fixed point (Machado, Nakamura, and Moraes 2012).\nMachodo (2012) use a third degree polynomial model to calculate lactate thresholds as the graphically shown with the red arrow in Figure 12.8.\n\n\nCode\ndat &lt;- cyclingstudy %&gt;%\n  # Select columns needed for analysis\n  select(subject, group, timepoint, lac.225:lac.375) %&gt;%\n  # Only one participant and time-point\n  filter(timepoint == \"pre\", subject == 10) %&gt;%\n  # Pivot to long format data using the lactate columns\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\",\n               names_transform = list(watt = as.numeric),\n               cols = lac.225:lac.375) %&gt;%\n    # Filter NA's from the data \n  filter(!is.na(lactate)) %&gt;%\n  data.frame()\n\n# Fit a third degree polynomial model (with raw orthogonal)\npolymodel &lt;- lm(lactate ~ poly(watt, 3, raw = TRUE), data = dat)\n\ndelta_mod &lt;- lm(lactate ~ watt, data = filter(dat, watt %in% c(min(watt),  max(watt)) ))\n\n\n# Calculate the point with the largest perpendicular distance between a straight line \n# and the 3rd degree polynomial. \nd1 &lt;- coef(polymodel)[2]\nd2 &lt;- coef(polymodel)[3]\nd3 &lt;- coef(polymodel)[4]\ndelta &lt;- coef(delta_mod)[2]\n\n# Equation from Machodo\npoly_threshold &lt;- (-d2 + sqrt((d2^2 - 3 * d3 * (d1 - delta)) )) / (3 * d3)\n\n\n# Plotting the results\ndat %&gt;%\n  ggplot(aes(watt, lactate))  + \n  \n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), \n              se = FALSE, \n              color = \"steelblue\") + \n  \n  geom_segment(aes(y = pull(filter(dat, watt == min(watt)), lactate), \n               yend = pull(filter(dat, watt == max(watt)), lactate), \n               x = min(watt), \n               xend = max(watt)), \n               color = \"steelblue\", \n               linewidth = 1) +\n  \n  \n  geom_segment(aes(y = predict(polymodel, newdata = data.frame(watt = poly_threshold)), \n                   yend = 0, \n                   x = poly_threshold, \n                   xend = poly_threshold), \n               color = \"red\", lty = 2, \n               arrow = arrow(type = \"closed\", length = unit(4, \"mm\"))) +\n\n  \n  \n  geom_point()\n\n\n\n\n\nFigure 12.8: Using a polynomial model to determine the lactate threshold.\n\n\n\n\n\nThere a several ways of doing such calculations, and many other concepts of lactate thresholds exists. Newell (Newell et al. 2007) has developed R code for calculating several of these concepts. Others have implemented R code in applications to calculate lactate thresholds, for example lactate dashboard.\nMost techniques and concepts rely on an underlying regression model."
  },
  {
    "objectID": "12-linear-and-curvlinear-relationships.html#references",
    "href": "12-linear-and-curvlinear-relationships.html#references",
    "title": "12  Linear and curve-linear relationships, and predictions",
    "section": "12.4 References",
    "text": "12.4 References\n\n\n\n\nCarlsson, Magnus, Tomas Carlsson, Daniel Hammarström, Toomas Tiivel, Christer Malm, and Michail Tonkonogi. 2012. “Validation of Physiological Tests in Relation to Competitive Performances in Elite Male Distance Cross-Country Skiing.” Journal of Strength and Conditioning Research 26 (6): 1496–1504. https://doi.org/10.1519/JSC.0b013e318231a799.\n\n\nMachado, Fabiana Andrade, Fábio Yuzo Nakamura, and Solange Marta Franzói De Moraes. 2012. “Influence of Regression Model and Incremental Test Protocol on the Relationship Between Lactate Threshold Using the Maximal-Deviation Method and Performance in Female Runners.” Journal of Sports Sciences 30 (12): 1267–74. https://doi.org/10.1080/02640414.2012.702424.\n\n\nNewell, J., D. Higgins, N. Madden, J. Cruickshank, J. Einbeck, K. McMillan, and R. McDonald. 2007. “Software for Calculating Blood Lactate Endurance Markers.” Journal Article. Journal of Sports Sciences 25 (12): 1403–9. https://doi.org/10.1080/02640410601128922."
  },
  {
    "objectID": "12-linear-and-curvlinear-relationships.html#footnotes",
    "href": "12-linear-and-curvlinear-relationships.html#footnotes",
    "title": "12  Linear and curve-linear relationships, and predictions",
    "section": "",
    "text": "“the act of eyeballing something is to measure or weigh something without any tools”↩︎\nIf you follow this link, take a cup of coffe before continuing with the chapter.↩︎"
  },
  {
    "objectID": "13-categorical-predictors-and-multiple-regression.html#linear-models-can-be-used-instead-of-t-tests",
    "href": "13-categorical-predictors-and-multiple-regression.html#linear-models-can-be-used-instead-of-t-tests",
    "title": "13  Categorical predictors and multiple regression",
    "section": "13.1 Linear models can be used instead of t-tests",
    "text": "13.1 Linear models can be used instead of t-tests\nt-test are designed to compare means. A question you might want to answer using a t-test is how unlikely the results from your test is if there were no true differences between values from two groups (independent t-test), or two samples from the same individuals (paired sample t-test) or comparing a group to a specific value (one sample t-test). Another way of describing these tests are; tests of differences from zero in a one-sample case or differences between groups with paired or unpaired observations.\nUsing the cyclingstudy data set we can perform t-tests. In the code below we will select the variable squat jump and filter it from two time-points. pivot_wider is used to put squat jump performance from the two time-points in separate columns. A change score is then calculated.\n\nlibrary(tidyverse)\nlibrary(exscidata)\n\n\ncyc_select &lt;- cyclingstudy %&gt;%\n  # select a subset of variables, squat jump max is the outcome of interest.\n        select(subject, timepoint, sj.max) %&gt;%\n  # time-points of interest\n        filter(timepoint %in% c(\"pre\", \"meso3\")) %&gt;%\n  # spread the data based on time-points\n        pivot_wider(names_from = timepoint, \n                    values_from = sj.max) %&gt;%\n  # create a change score\n        mutate(change = meso3 - pre) \n\nThe data above may be used to perform the paired sample t-test, or one sample t-test. These are basically equivalent. In the first case we use both vectors of numbers and test if the difference between them are different from zero. In the other case we calculate the differences first and then test if the mean change-score is different from zero. In the paired sample t-test, the argument paired = TRUE must be added to the t.testcall to make sure you do a paired comparison. In the one-sample case we have to set the mean we want to compare to, in this case zero (mu = 0).\n\npaired &lt;- t.test(cyc_select$meso3, cyc_select$pre, paired = TRUE)\none_sample &lt;- t.test(cyc_select$change, mu = 0)\n\nThese tests are equal as we can see from the comparison below. They are also equivalent to a linear model where we simply model the mean of the change score. When fitting the change variable without any predictors we estimate a single parameter in our model, the intercept. The intercept coefficient can be used to test against the same null-hypothesis (change not different from zero), the same results will appear. We can add all result to the same table (Table 13.1).\n\nlinear_model &lt;- lm(change ~ 1, data = cyc_select)\n\n \nlibrary(gt)\n\ndata.frame(test = c(\"Paired sample t-test\", \"One sample t-test\", \"Linear model\"), \n           t.value = c(paired$statistic, one_sample$statistic, coef(summary(linear_model))[1, 3]), \n           p.value = c(paired$p.value, one_sample$p.value, coef(summary(linear_model))[1, 4]), \n           estimate = c(paired$estimate, one_sample$estimate, coef(summary(linear_model))[1, 1]), \n           lwr.ci = c(paired$conf.int[1], one_sample$conf.int[1], confint(linear_model)[1]),\n           upr.ci = c(paired$conf.int[2], one_sample$conf.int[2], confint(linear_model)[2]),\n           se = c(paired$stderr, one_sample$stderr, coef(summary(linear_model))[1, 2])) %&gt;%\n  tibble() %&gt;%\n  \n  gt() %&gt;%\n  fmt_auto() %&gt;%\n  cols_label(test = \"Test\", t.value = md(\"*t*-value\"), p.value = md(\"*p*-value\"), estimate = \"Estimate\", lwr.ci = \"Lower CI\", upr.ci = \"Upper CI\", se = \"Standard error\")\n\n\n\n\n\nTable 13.1:  Comparing a paired sample t-test, one sample t-testa and a linear\nregression model \n  \n    \n    \n      Test\n      t-value\n      p-value\n      Estimate\n      Lower CI\n      Upper CI\n      Standard error\n    \n  \n  \n    Paired sample t-test\n−1.788\n0.091\n−0.891\n−1.938\n0.156\n0.498\n    One sample t-test\n−1.788\n0.091\n−0.891\n−1.938\n0.156\n0.498\n    Linear model\n−1.788\n0.091\n−0.891\n−1.938\n0.156\n0.498\n  \n  \n  \n\n\n\n\n\nWe can also test if there is a true difference in VO2.max change between group INCRand DECR using a t-test.\n\ncyc_select &lt;- cyclingstudy %&gt;%\n  # Select appropriate variables and filter time-points\n        select(subject,group, timepoint, VO2.max) %&gt;%\n        filter(timepoint %in% c(\"pre\", \"meso3\"), \n               group != \"MIX\") %&gt;%\n  # make the data set wider and calculate a change score (%-change).\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = meso3-pre) %&gt;%\n        print()\n\n# A tibble: 14 × 5\n   subject group   pre meso3 change\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1       1 INCR  5629   5330 -299  \n 2       2 DECR  4471   4886  415  \n 3       3 INCR  5598.  6380  782. \n 4       4 DECR  4944.  5171  227. \n 5       5 DECR  5748     NA   NA  \n 6       6 INCR  4633.  4864  231. \n 7      10 INCR  5226   5755  529  \n 8      13 DECR  4981.  5045   63.6\n 9      15 INCR  4486   4991  505  \n10      16 INCR  4604   5087  483  \n11      18 DECR  5135.  5315  180. \n12      19 DECR  4018.  4404  386. \n13      20 INCR  4739   5119  380  \n14      21 DECR  4753.  4730  -22.5\n\n# Perform a two-sample t-test\nt.test(change ~ group, data = cyc_select, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  change by group\nt = -1.0703, df = 11, p-value = 0.3074\nalternative hypothesis: true difference in means between group DECR and group INCR is not equal to 0\n95 percent confidence interval:\n -503.6842  174.0833\nsample estimates:\nmean in group DECR mean in group INCR \n          208.2119           373.0123 \n\n\nAbove we use the formula method to specify the t-test (see ?t.test). var.equal = TRUE tells the t.test function to assume equal variances between groups.\nThe same result will appear when we fit the data to a linear model. The summary function is a generic function, meaning that many type of R objects has summary methods. From summary we get a regression table of estimates. The first row is the intercept, we can interpret this as the mean change in one of the groups (DECR). This rows has all the statistics associated with this estimate including the average (Estimate), standard error, t-value and a p-value.\nThe second row represents the difference between groups. The INCR group has a change score that is 164.8 units larger than the DECR group. The associated statistics can be used to assess if this difference is large enough to declare surprisingly large if the null hypothesis is actually true.\n\nlin_mod &lt;- lm(change ~ group, data = cyc_select)\n\nsummary(lin_mod)\n\n\nCall:\nlm(formula = change ~ group, data = cyc_select)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-672.01 -141.94   18.76  155.99  409.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    208.2      113.0   1.843   0.0924 .\ngroupINCR      164.8      154.0   1.070   0.3074  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 276.7 on 11 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09433,   Adjusted R-squared:  0.01199 \nF-statistic: 1.146 on 1 and 11 DF,  p-value: 0.3074\n\n\nIf you compare the two tests, do they tell you the same?\nUsing var.equal = TRUE in the unpaired t-test we assumed that the variation was similar in both groups. This might not be the case, and R uses the Welch two-sample t-test by default which does not assum equal variance between groups. Even the Welch two sample t-test can be replicated using a linear model. However, we have to specify it in a slightly different framework using the gls() function from the nlme package.\n\nlibrary(nlme)\n\nwelch_twosample &lt;- t.test(change ~ group, data = cyc_select, var.equal = FALSE)\n\nlin_mod_var &lt;- gls(change ~ group, data = cyc_select, weights = varIdent(form = ~1|group), na.action = na.exclude, method = \"ML\")\n\n\nwelch_twosample\nsummary(lin_mod_var)\n\nYou are not required to master gls at this time. It however shows that the linear model frame work is very flexible as it in this case also can be adjusted to take care of heteroscedasticity."
  },
  {
    "objectID": "13-categorical-predictors-and-multiple-regression.html#dummy-variables",
    "href": "13-categorical-predictors-and-multiple-regression.html#dummy-variables",
    "title": "13  Categorical predictors and multiple regression",
    "section": "13.2 Dummy variables",
    "text": "13.2 Dummy variables\nThe group variable that we used in the code above introduces something new to the linear model, namely dummy variables. When we put a categorical variable in the lm command, R will code it as a dummy variable. This variable will be zero if the group corresponds to the first level of the categorical variable (coded as a factor variable) and it will be 1 if it is the second level.\nIn the simplest case (as above) we will get a linear model looking like this:\n\\[Y = \\beta_0 + \\beta_1X\\]\nWhere the \\(X\\) is the grouping variable, remember, 0 if first (reference) group and 1 if the second level group. The coefficient \\(\\beta_1\\) only kicks in if the group is 1. Meaning that when group = 0 we have only the intercept. If group = 1 we have the intercept + the slope. The slope represents the difference between the intercept (group = 0) and group = 1.\nIf the grouping variable would have more groups more dummy-variables would have been added.\nUsing all groups in the data set, fit a model and interpret the results.\nHere is a possible solution\n\n\ncyc_subset &lt;- cyclingstudy %&gt;%\n        select(subject,group, timepoint, VO2.max) %&gt;%\n        filter(timepoint %in% c(\"pre\", \"meso3\")) %&gt;%\n        pivot_wider(names_from = timepoint, \n                    values_from = VO2.max) %&gt;%\n        mutate(change = 100 * (meso3-pre)/pre) %&gt;%\n        print()\n\nmod &lt;- lm(change ~ group, data = cyc_subset)\n\nsummary(mod)\n\nThe DECR group is the reference group, the intercept shows the mean of this group. Each parameter shows the difference from the reference.\n\n\nThe same assumptions are made with these kinds of models and they can be checked with the same methods as described above."
  },
  {
    "objectID": "13-categorical-predictors-and-multiple-regression.html#multiple-regression",
    "href": "13-categorical-predictors-and-multiple-regression.html#multiple-regression",
    "title": "13  Categorical predictors and multiple regression",
    "section": "13.3 Multiple regression",
    "text": "13.3 Multiple regression\nContrary to the t-tests used above, the linear model can be extended by adding predicting variables (independent variables). In a situation where multiple independent variables are included in the model, we control for their relationship to the dependent variable when we evaluate the other variables. Similarly with univariate regression we can examine each individual parameter from the summary.\nIn a previous example we used height.T1 to predict VO2.max. We might want to add information to the model. We might wonder if the age (age) of participants have a relationship with VO2max. To fit this model, use the code below.\n\ncycling_data &lt;-  cyclingstudy %&gt;%\n       # select(subject, timepoint, VO2.max, weight.T1, height.T1) %&gt;%\n        filter(timepoint == \"pre\") %&gt;%\n        print()  \n          \n\nmod1 &lt;- lm(VO2.max ~  height.T1 + age, data = cycling_data)\n\nsummary(mod1)\n\nFrom the output we can see that there is a negative relationship, when age increases VO2max decrease. We can compare this model to the simpler model by looking at the \\(R^2\\) value. We fit the simpler model.\n\nmod0 &lt;- lm(VO2.max ~  height.T1, data = cycling_data)\n\nsummary(mod0)\n\nWe can interpret \\(R^2\\) as the percentage of the variation explained by the model. When we added more variables to the model we add information that collectively explain a larger portion of the observed data. When adding variables we face the risk of over-fitting our model. With enough variables the model will explain the observed data with less and less uncertainty, however, new data will probably not validate the model.\nThe same assumptions apply to the multiple regression model as with the univariate regression model. We have to take care that we have homoscedasticity, independent observations and normally distributed errors."
  },
  {
    "objectID": "14-correlations.html#always-plot-the-data",
    "href": "14-correlations.html#always-plot-the-data",
    "title": "14  Correlations",
    "section": "14.1 Always plot the data!",
    "text": "14.1 Always plot the data!\nWhen doing a correlation analysis you are at risk of drawing conclusions based on wonky data. A single data point can for example inflate a correlation in a small data set. Lets look at the data we are using now (Figure 14.1).\n\n\nCode\ndat %&gt;%\n  ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal()\n\n\n\n\n\nFigure 14.1: A figure showing the relationship between training volume and lean mass\n\n\n\n\nThe figure displays no apparent curve-linear relationship, there are no obvious outliers, both variables are evenly distributed (normally distributed). These are assumptions concerning the correlation analysis. Since they are reasonably met, our test above still holds."
  },
  {
    "objectID": "14-correlations.html#extending-the-correlation-to-the-regression",
    "href": "14-correlations.html#extending-the-correlation-to-the-regression",
    "title": "14  Correlations",
    "section": "14.2 Extending the correlation to the regression",
    "text": "14.2 Extending the correlation to the regression\nAll good! We have a test that tells us a measure of the strength of relationship between two variables. If we want more detailed information we need to move to a regression analysis.\nFirst some similarities. Notice that the p-value for the regression coefficient for squat volume is (almost) precisely the same as the p-value for the correlation analysis!\n\n# Store the correlation analysis in an object \nc &lt;- cor.test(dat$DXA_LBM_T1, dat$SQUAT_VOLUME)\n# store the regression model\nm &lt;- lm(DXA_LBM_T1  ~ SQUAT_VOLUME, data = dat)\n\n# Display the p-value for the regression coefficient\ncoef(summary(m))[2, 4] \n\n[1] 0.003154061\n\n# Display the p-value for the correlation coefficient\nc$p.value\n\n[1] 0.003154061\n\n\nAlso notice that the \\(R^2\\) value in the regression model is the same as the squared correlation coefficient. Remember that the \\(R^2\\) in the regression model is the degree to which the model account for the data (Navarro 2020), also see here.\n\nsummary(m)$r.squared\n\n[1] 0.2714845\n\nc$estimate^2\n\n      cor \n0.2714845 \n\n\nThese similarities arise from the fact that they are the same analysis. The degree to which the two variables co-varies.\nThe additional benefit of using a regression analysis comes from the interpretation of the regression coefficient estimates. In our example we can see that increasing the weekly volume with one ton increases percentage lean mass by 0.283%-points. The confidence interval is given on the same scale and can be retrieved by using the code below:\n\nconfint(m)\n\nThis shows that the true value could be as low as 0.1 and as high as 0.46. Something that again indicates that the two variables vary together."
  },
  {
    "objectID": "14-correlations.html#correlation-comes-in-many-forms",
    "href": "14-correlations.html#correlation-comes-in-many-forms",
    "title": "14  Correlations",
    "section": "14.3 Correlation comes in many forms",
    "text": "14.3 Correlation comes in many forms\nIf you look at the help pages for cor (?cor) you will see that you may specify the type of correlation used for analysis. Commonly used are Pearson’s (default) and Spearman’s correlation coefficient. The difference between these two is that the Spearman’s correlation coefficient does not assume normally distributed data. This is basically a correlation of ranks. The highest number in a series of numbers will have the highest rank and the smallest will be given the lowest ( = 1).\nWe can prove this! The rank function gives a ranking to each number. We first panel of our figure (Figure 14.2) the data as raw continuous values and the second transfomed to ranks.\n\n\nCode\nlibrary(cowplot)\n\nraw &lt;- dat %&gt;%\n  ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal()\n\n\nrank &lt;- dat %&gt;%\n  ggplot(aes(rank(SQUAT_VOLUME),\n             rank(DXA_LBM_T1))) + geom_point() + theme_minimal()\n\nplot_grid(raw, rank, labels = c(\"A\", \"B\"))\n\n\n\n\n\nFigure 14.2: Two figure showing the relationship between training volume and lean mass either in raw units (A) or transformed to ranks (B)\n\n\n\n\nWe can see in the figure that the relationship persist after rank transformation.\nTo use the Spearman’s correlation coefficient we specify \"spearman\" in the cor.test function.\n\ncor.test(dat$SQUAT_VOLUME, dat$DXA_LBM_T1, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  dat$SQUAT_VOLUME and dat$DXA_LBM_T1\nS = 2167.4, p-value = 0.00338\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.5178259 \n\n\nTo see that this is similar to using Pearson’s correlation coefficient with ranked data.\n\ncor.test(rank(dat$SQUAT_VOLUME, na.last = NA), rank(dat$DXA_LBM_T1, na.last = NA), \n         method = \"pearson\")\n\nThe p-values are identical. Success! Another statistical mystery unlocked!\nIn this case the interpretation of tests using ranked data and un-transformed data are very similar. When do we use the rank based correlation? In cases when assumptions for the Pearson correlation are not met, a rank based correlation will protect us from making bad decisions. When, for example, a single data point “drives” a correlation, the the rank-based correlation (Spearman’s) will be less likely to suggest a strong relationship in the population from where we drew our sample."
  },
  {
    "objectID": "14-correlations.html#statistical-and-subject-matter-interpretations",
    "href": "14-correlations.html#statistical-and-subject-matter-interpretations",
    "title": "14  Correlations",
    "section": "14.4 Statistical and subject-matter interpretations",
    "text": "14.4 Statistical and subject-matter interpretations\nIt is now very important to stop and think about the estimates that we arrived to above. We have concluded that lean body mass correlate quite well with squat volume calculated as the amount of weight lifted per week. Does this mean that individuals that exercise with higher volumes have more muscle? Perhaps, but could it also mean that individuals that are taller and heavier work out with greater resistance? Perhaps. In any case, the correlation (and regression) analysis of snap-shot observational data can trick us into believing that the mathematical relationship also indicate a causal relationship. We have to thread carefully when interpreting associations (Spiegelhalter 2019, chaps. 2, 4 and 5), this is were your subject-matter knowledge is handy."
  },
  {
    "objectID": "14-correlations.html#summary",
    "href": "14-correlations.html#summary",
    "title": "14  Correlations",
    "section": "14.5 Summary",
    "text": "14.5 Summary\nThe correlation coefficient has many similarities with a univariate regression model. Correlations measures strength of association, but the regression model comes with benefits in terms of interpretation. The correlation only takes two variables but we can extend the regression model. When we think that data do not match our assumptions we can do correlation analysis using Spearman’s rank correlation to avoid biased estimates of estimation."
  },
  {
    "objectID": "14-correlations.html#references",
    "href": "14-correlations.html#references",
    "title": "14  Correlations",
    "section": "14.6 References",
    "text": "14.6 References\n\n\n\n\nNavarro, D. 2020. Learning Statistics with r.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "16-special-topic-functions-and-iterations.html#building-a-simple-function",
    "href": "16-special-topic-functions-and-iterations.html#building-a-simple-function",
    "title": "15  Special topics: Iterations and functions",
    "section": "15.1 Building a simple function",
    "text": "15.1 Building a simple function\nA function in R is a defined set of operations, put together and often stored as an object to perform a specific task. In R, the user may define his/her own functions. Defining a function usually involves storing the function as an object in your environment. When you have gathered multiple functions relating to common tasks these can be organized as a package.\nLet’s say we want to create a function that returns the sum of two values. The function sum_two_values that is defined below, contains the basics of a typical function. In the function() call we define what arguments the function will accept. In our case, a and b represents the two numbers we want to add together. The body of the function is where we define the operations that we want the function to perform. We add a and b together and assign their sum to an object called result. Finally, we pass the result of the function to return(result). Using return makes it explicit what part of the results you want to “get from the function”.\n\nsum_two_values &lt;- function(a, b) {\n  \n  result &lt;- a + b\n  \n  return(result)\n}\n\nsum_two_values(3, 6)\n\n[1] 9\n\n\nThe name of a function is defined by saving the function in an object. When the function is defined in a script or RMarkdown file it is available in the R environment. The difference from packages is that functions defined as part of packages are available when you load the package using library()."
  },
  {
    "objectID": "16-special-topic-functions-and-iterations.html#an-applied-problem-where-a-function-might-help",
    "href": "16-special-topic-functions-and-iterations.html#an-applied-problem-where-a-function-might-help",
    "title": "15  Special topics: Iterations and functions",
    "section": "15.2 An applied problem where a function might help",
    "text": "15.2 An applied problem where a function might help\nLactate threshold values can be calculated from the exercise-intensity and blood lactate value relationship gathered from a graded exercise test. In the simple case, the exercise intensity at a fixed blood lactate concentration can be used to evaluate training progressions in e.g. athletes. To find the exercise intensity at a fixed blood lactate concentration, a polynomial regression model can be used to predict lactate values at different intensities and the find the exercise intensity value closest to our desiered lactate value.\nThe above description can be broken down into these steps:\n\nFit a third degree polynomial model to exercise-intensity and lactate data from a single individual.\nPredict lactate values over the range of the observed exercise intensity values.\nFind the exercise intensity value closest to the lactate value of interest (e.g. 4 mmol L-1).\n\nUsing the cyclingstudy data we can perform these steps. We will do so using the pre time-point in participant 10.\n\nlibrary(tidyverse); library(exscidata); data(\"cyclingstudy\")\n\n# Save a data set of lactate values from participant 10, time-point pre\ndat &lt;- cyclingstudy %&gt;%\n  select(subject, group, timepoint, lac.125:lac.375) %&gt;%\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\", \n              names_transform = list(watt = as.numeric), \n              cols = lac.125:lac.375) %&gt;%\n  filter(subject == 10, \n         timepoint == \"pre\", \n         !is.na(lactate)) %&gt;% # Remove NA values\n  print()\n\n# Fit the model \nmodel &lt;- lm(lactate ~ watt + I(watt^2) + I(watt^3), data = dat)\n\n\n# Predict lactate values over all observed watt values\n# calculate the smallest distance from the fixed lactate value \n\nnew_data &lt;- data.frame(watt = seq(from = min(dat$watt), to = max(dat$watt), by = 0.1))\n\nnew_data$dist &lt;- abs(predict(model, newdata = new_data) - 4)\n\n# Find the smallest value of predicted - fixed lacate value\nnew_data %&gt;%\n  filter(dist == min(dist)) # Where the dist value equals the minimum dist value\n\nIf we were to do this operation for more participants and across more time-points, we should formalize the operation into a function. In the simplest form, the prospective function would only need an argument defining the input data. We can then settle for a single fixed lactate value. We will define the function and call it lt for lactate threshold. (This might not be the best name! (Wickham and Grolemund 2017)).\nIf the function returns a data frame, we can use it more efficiently later som we will make sure the result of the function is a data frame.\n\nlt &lt;- function(data) {\n  \n  # Fit a 3 degree polynomial model\n  m &lt;- lm(lactate ~ watt + I(watt^2) + I(watt^3), data = data)\n  \n  # Store a data frame with exercise intensities\n  new_data &lt;- data.frame(watt = seq(from = min(data$watt), to = max(data$watt), by = 0.01))\n  \n  # Predict using the new data, predicting lactate values at each \n  new_data$pred &lt;- predict(m, newdata = new_data)\n  \n  # calculate deviation from the lactate value of interest\n  new_data$watt.4mmol &lt;- abs(new_data$pred - 4)\n\n  # Create a results data frame\n  results &lt;- data.frame(watt.4mmol = new_data[which.min(new_data$watt.4mmol),1])\n  \n  # Return the data frame\n  return(results)\n  \n}\n\nThe body of the function contains all operations needed to get the results we are after. It finally returns a data frame containing a single column named watt.4mmol. We can now test the function on some data.\n\nlibrary(tidyverse); library(exscidata); data(\"cyclingstudy\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata &lt;- cyclingstudy %&gt;%\n  select(subject, group, timepoint, lac.125:lac.375) %&gt;%\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\", \n              names_transform = list(watt = as.numeric), \n              cols = lac.125:lac.375) %&gt;%\n  filter(!is.na(lactate), \n         subject == 10, \n         timepoint == \"pre\") %&gt;%\n  print()\n\n# A tibble: 8 × 5\n  subject group timepoint  watt lactate\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      10 INCR  pre         125    0.93\n2      10 INCR  pre         175    0.87\n3      10 INCR  pre         225    0.86\n4      10 INCR  pre         250    0.92\n5      10 INCR  pre         275    1.2 \n6      10 INCR  pre         300    1.69\n7      10 INCR  pre         325    2.6 \n8      10 INCR  pre         350    4.69\n\nlt(data)\n\n  watt.4mmol\n1     342.96\n\n\nNice, since we defined the function with higher resolution, we get a exercise intensity value with two decimal points. This value represent the watt value where the predicted lactate value is the closest to 4 mmol L-1.\nWe still need to incorporate the function into code that dose the calculation for each individual, and time-point without the need of manually filtering values for each new data set. This can be accomplished by using the function in a pipe together with group_by and group_modify.\nSince we are using a third degree polynomial we need to make sure we have enough data to fit such a model to each participant/time-point. We will add a variable counting the number of observations and then filter tests with less then 4 observations.\nThe complete pipe could look something like this.\n\n# Extract lactate values\ncyclingstudy %&gt;%\n  select(subject, group, timepoint, lac.125:lac.375) %&gt;%\n  pivot_longer(names_to = \"watt\", \n               values_to = \"lactate\", \n               names_prefix = \"lac.\", \n              names_transform = list(watt = as.numeric), \n              cols = lac.125:lac.375) %&gt;%\n  # Filter missing lactate values\n  filter(!is.na(lactate)) %&gt;%\n  # Group the data set, keeping group to have this information in the final results\n  group_by(timepoint, subject, group) %&gt;%\n  # create a new variable counting the number of observations and filter away tests with less than 4 obs.\n  mutate(n = n()) %&gt;%\n  filter(n &gt;= 4) %&gt;%\n  # Use grouup modify to apply the function to all participants per time-point (and group)\n  group_modify(~ lt(.)) %&gt;%\n  print()\n\n# A tibble: 74 × 4\n# Groups:   timepoint, subject, group [74]\n   timepoint subject group watt.4mmol\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 meso1           1 INCR        293.\n 2 meso1           2 DECR        286.\n 3 meso1           3 INCR        335.\n 4 meso1           4 DECR        244.\n 5 meso1           5 DECR        332.\n 6 meso1           6 INCR        252.\n 7 meso1           7 MIX         268.\n 8 meso1           8 MIX         294.\n 9 meso1           9 MIX         300.\n10 meso1          10 INCR        340.\n# ℹ 64 more rows\n\n\nGreat sucess! We have now calculated lactate threshold values for each participant, belonging to specific time-points and groups. The group_modify function does the iterative work of applying the function for all cases, you have save yourself a lot of code, and time!\n\n15.2.1 Exercises\n\nModify the lt function to return two fixed lactate values.\nModify the lt function to include an extra argument specifying the level of the lactate value you are interested in."
  },
  {
    "objectID": "16-special-topic-functions-and-iterations.html#references",
    "href": "16-special-topic-functions-and-iterations.html#references",
    "title": "15  Special topics: Iterations and functions",
    "section": "15.3 References",
    "text": "15.3 References\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "17-sample-and-population.html#reading-instructions",
    "href": "17-sample-and-population.html#reading-instructions",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.1 Reading instructions",
    "text": "16.1 Reading instructions\nBefore going further, I suggest reading Chapters 7-10 in Spiegelhalter (Spiegelhalter 2019). These chapters are a great start for understanding the concepts discussed below."
  },
  {
    "objectID": "17-sample-and-population.html#descriptive-statistics-in-r",
    "href": "17-sample-and-population.html#descriptive-statistics-in-r",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.2 Descriptive statistics in R",
    "text": "16.2 Descriptive statistics in R\n\n16.2.1 Simulating data in R\nR is great because you can create data! In the examples below we will generate data, you can copy and paste the code into your R session to run it and answer questions in this lesson.\nWhen we generate data in R we need to set the seed to make the random number generator create the same numbers every time. Basically, R generate numbers and if we want R to generate the same numbers every time we have to tell R where to start.\nThis means that before each simulation I will include:\n\nset.seed(1)\n\nThe number within set.seed is important as it defines where R starts generating numbers."
  },
  {
    "objectID": "17-sample-and-population.html#a-simple-example",
    "href": "17-sample-and-population.html#a-simple-example",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.3 A simple example",
    "text": "16.3 A simple example\nLet’s say that we collect data on VO2max values in trained cyclists (ml kg-1 min-1). We are interested in the average. First we simulate all possible values:\n\nset.seed(1)\nvo2max &lt;- rnorm(100000, 70, 5)\n\nAll possible values?? Yes, we create a distribution of values of VO2max in trained cyclists based on the rnorm-function. rnorm simulates random numbers (100000 in this case) based on an average (in this case 75) and standard deviation (in this case 5). This population is now stored in the object vo2max.\nWe conduct our study and collect data on 20 participants. This represents only 2% of all possible numbers!\nBelow we use the sample function, this function draws a sample of a fixed size from our collection of random numbers. We store it in an object called samp\n\nset.seed(1)\nsamp &lt;- sample(vo2max, 20, replace = FALSE)\n\nreplace = FALSE makes sure that we do not sample the same numbers more than once. This way our sampling resembles what a real life study would do.\nThe samp object now contain numbers from a possible study. The study has recruited 20 randomly chosen cyclists and measured their VO2max. Let’s describe the sample.\nIn R we can describe the data using multiple methods, first we will calculate summary statistics.\n\nm &lt;- mean(samp)\ns &lt;- sd(samp)\n\nWe can also use the summary function.\n\nsummary(samp)\n\nLet’s make a graph of the sample. We can represent the sample using points, the code below may be a bit complicated, it is mostly cosmetics.\n\nlibrary(tidyverse) # Needed for making the plot!\ndf &lt;- data.frame(samp = samp)\n\ndf %&gt;%\n  # Plots our samples on the x axis, and sets all y to 1.\n  ggplot(aes(samp, y = 1)) + \n  # Adds points and \"jitter\" on the y axis\n  geom_point(position = position_jitter(height = 0.2)) + \n  # Scales the y axis to better plot the data\n  scale_y_continuous(limits = c(0,2)) +\n  # Set the name of the x axis\n  labs(x = \"VO2max\") +\n  # The code below modifies the theme of the plot, removing \n  # the y axis\n  theme(axis.title.y = element_blank(), \n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(), \n        axis.line.y = element_blank())\n\nAnother plot can be the box-plot, similar to the above we can do:\n\ndf %&gt;%\n  # Plots our samples on the y axis, and sets all x to 1.\n  ggplot(aes(x = 1, samp)) + \n  # Adds the boxplot\n  geom_boxplot(width = 0.5) +\n  # Scales the x axis to better plot the data\n  scale_x_continuous(limits = c(0,2)) +\n  # Set the name of the y axis\n  labs(y = \"VO2max\") +\n  # The code below modifies the theme of the plot, removing \n  # the x axis\n  theme(axis.title.x = element_blank(), \n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(), \n        axis.line.x = element_blank())\n\nThe boxplot (or box-and-whiskers plot (Spiegelhalter 2019, Ch. 2)) summarizes the data and plots the median, the inter-quartile range minimum and maximum, and any outliers.\nWe can also plot the data as a histogram.\n\ndf %&gt;%\n  # Plots our samples on the y axis, and sets all x to 1.\n  ggplot(aes(samp)) + \n  # Adds the histogram\n  geom_histogram(binwidth = 3, color = \"blue\", fill = \"lightblue\") +\n  # Set the name of the y axis\n  labs(x = \"VO2max\", y = \"Count\") \n\nBoth the statistical summaries (created with mean, sd and summary) and the plots are descriptive analyses of the sample. We still have not made any claims about the population."
  },
  {
    "objectID": "17-sample-and-population.html#inference-about-the-population",
    "href": "17-sample-and-population.html#inference-about-the-population",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.4 Inference about the population",
    "text": "16.4 Inference about the population\nWhen doing a study, we are really interested in what we can say about the population (all possible values). In other words, how we can draw conclusions about the unknown, based on our data. This is were inferential statistics comes in.\nAs we have an estimate of the mean in the population, we may say that based on our sample we believe the mean is close to mean(samp). It is very likely the the mean in the population is not exactly that. Let’s try another sample:\n\nset.seed(2)\nsamp2 &lt;- sample(vo2max, 20, replace = FALSE)\n\n# Calculate the mean\nm2 &lt;- mean(samp2)\n\nAs we can see there is going to be some differences due to the fact that every time we draw a new sample (or conduct another study), we will get a slightly different estimate of the mean. How about the variation:\n\ns2 &lt;- sd(samp2)\n\n# Print the results from calculation of SD\nc(s, s2)\n\nIndeed, slightly different.\nWe could continue to sample in this way and record every outcome to build a distribution of means. A smarter way is to create a for-loop. This is a basic building block in programming, we tell the computer to do a task multiple times and store the results in a nice format. We will sample size = 20 and calculate the mean. The results will be stored in a data frame.\n\n# set the seed\nset.seed(123)\n\n# create the data frame\nresults &lt;- data.frame(mean = rep(NA, 10000)) \n# The rep function creates a vector full of NA \n# each NA can be replaced with a mean\n\n# Second we build the for loop\nfor(i in 1:10000){\n  \n  results[i, 1] &lt;- mean(sample(x = vo2max, size = 20, replace = FALSE))\n  \n}\n\nThe results from this process can be plotted using ggplot2\n\nggplot(data = results, aes(mean)) +\n  geom_histogram(fill = \"lightblue\", color = \"gray40\", binwidth = 1) + \n  theme_minimal()\n\nWhat did just happened? We basically conducted 1000 studies, from each study we calculated the mean and stored them. Most of the means were very close to 70 as we can see in the graph. The distribution of the means is bell-shaped, actually the distribution looks like something that we can call a Normal distribution.\n\nDistributions can be described in different ways depending on what distribution we are takling about. The most commonly used, the normal (or Gaussion) distribution can be described by the mean and the standard deviation.\n\nAs the distribution of means is approximately normal we can determine where most of the means are. Let’s say that we are interested in determining a range where 95% of the means can be found. To do this we can use a theoretical distribution created by estimates from the distribution (the mean and standard deviation).\n\n\n\n\n\n\n\n\n\n95% of all means are under the shaded area! This corresponds to a range of values that can be calculated in R:\n\nlower &lt;- mean(results[,1]) - 1.96 * sd(results[,1])\nupper &lt;- mean(results[,1]) + 1.96 * sd(results[,1])\n\nWhat does this mean? Well, we have drawn 1000 samples from our bag of numbers (representing all possible outcomes). We then calculated the mean of each sample and created a distribution of means. The mean of means is very, very close to the true mean.\n\nmean(vo2max)\nmean(results[,1])\n\nWe have also calculated a range were 95% of all means are located, we did this by approximating the actual values using the normal distribution."
  },
  {
    "objectID": "17-sample-and-population.html#estimation-of-the-sampling-distribution",
    "href": "17-sample-and-population.html#estimation-of-the-sampling-distribution",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.5 Estimation of the sampling distribution",
    "text": "16.5 Estimation of the sampling distribution\nWhat we have done above is a very theoretical example as we never do 1000 studies in real life. We will never get a distribution of means from many studies. However we can estimate this theoretical distribution of means using a sample!\n\nThis is one of the most important concepts in statistics! This means that by doing one study we can estimate the results of doing many studies. This is the basis of frequentist statistics.\n\n\n16.5.0.1 The standard error of a sample is an approximation of the standard deviation of the sampling distribution\nThe headline says it all. Basically, using the sample we can calculate the standard deviation, the standard deviation in turn can be used to calculate the standard error. The standard error is an estimate of the standard deviation of the theoretical distribution of means. Let’s see how it works out!\nUsing R we can simulate this concept. We will create a new set of random samples (or studies) and calculate statistics from them.\n\n# Copy and paste the code if you want the results\n\n# set the seed\nset.seed(123)\n\n# create the data frame\nresults &lt;- data.frame(mean = rep(NA, 1000), \n                      sd = rep(NA, 1000)) \n# The rep function creates a vector full of NA \n# each NA can be replaced with a mean\n\n# Second we build the for loop\nfor(i in 1:10000){\n  \n  samp &lt;- sample(x = vo2max, size = 20, replace = FALSE)\n  \n  results[i, 1] &lt;- mean(samp)\n  results[i, 2] &lt;- sd(samp)\n}\n\n\nresults %&gt;%\n  # Calculate the standard error of each sample\n  mutate(se = sd/sqrt(20)) %&gt;%\n  # Make a graph containing estimates and empirical values\n  ggplot(aes(mean)) + geom_histogram(binwidth = 0.5) + \n  \n  # Add a line representing the standard deviation of the distribution of means\n  geom_segment(aes(y = 350, \n                   yend = 350, \n                   x = mean(mean), \n                   xend = mean(mean) + sd(mean)), \n               lty = 1, color = \"green\", size = 2) +\n  # Add text to discribe the line\n  geom_text(aes(x = mean(mean),\n                y = 300, \n                label = paste(\"Empirical: Mean \", \n                              round(mean(mean), 0), \n                              \" + SD: \", \n                               round(sd(mean), 1) )), \n            color = \"green\") +\n  \n   # Add a line representing the average standard error of each sample\n  geom_segment(aes(y = 160, \n                   yend = 160, \n                   x = mean(mean), \n                   xend = mean(mean) + mean(se)), \n               lty = 1, color = \"blue\", size = 2) +\n  # Add text to describe the above\n  geom_text(aes(x = mean(mean),\n                y = 220, \n                label = paste(\"Estimate: Mean \", \n                              round(mean(mean), 0), \n                              \" + average SE: \", \n                               round(mean(se), 1) )), \n            color = \"blue\") \n\nIn the graph (you will see it you run the code), the blue line represents the average of what we estimate with each sample and the green line represent the actual values of the sampling distribution.\nThe variation (spread) of the sampling distribution corresponds to the standard error of the mean in each sample. At least, in the long run, the standard error of the mean is a pretty good estimate of the variation in the distributions of means. From a mathematical point of view, the standard error of the mean is calculated as:\n\\[SE = \\frac{s}{\\sqrt{n}}\\] where \\(s\\) is the sample standard deviation, \\(n\\) is the number of observations.\nRemember that we can use the standard deviation to calculate a range of values containing 95% of all values in a normal distribution. This can be done using a single sample! When calculating this using a sample we create a confidence interval!\n\n\n16.5.1 A confidence interval for the mean\nA confidence interval for the mean can be calculated as:\n\\[lower~limit=Mean - 1.96 * SE\\] \\[upper~limit=Mean+1.96 * SE\\]\n(This assumes that we are using the normal distribution).\nThe interpretation of the confidence interval is that 95% of confidence intervals, created using repeated sampling will contain the population mean. But unfortunately, we do not know if our specific interval do so.\nThe interpretation follows from the fact that we estimate the variation in the theoretical sampling distribution. Five percent of the time we will be wrong.\nTo test if the theory is correct, lets calculate confidence intervals from our simulated data and see how many times we catch the true mean.\n\n# Creat new variables with upper and lower limits of the confidence interval  \ncis &lt;- results %&gt;%\n  # Using the normal distribution\n  mutate(lower.limit = mean - 1.96 * sd/sqrt(20), \n         upper.limit = mean + 1.96 * sd/sqrt(20)) %&gt;%\n  # Test if the true mean is within the limits\n  # If the true mean is above the lower limit and below the upper limit then TRUE\n  # otherwise FALSE\n    mutate(true.mean = if_else(mean(vo2max) &gt; lower.limit & mean(vo2max) &lt; upper.limit, \n                             TRUE, FALSE)) \n\n# Plot the data, only plotting 200 data points to make it suitable for every computer\n\ncis[1:200, ] %&gt;%\n  ggplot(aes(seq(1:nrow(.)), \n             y = mean, \n             color = true.mean, # set a specific color \n             alpha = true.mean)) + # and transparency to \n  # intervals containing and not containing the true mean\n  \n  # add a line showing the true mean\n  geom_hline(yintercept = mean(vo2max)) +\n  # add errorbars showing each interval\n  geom_errorbar(aes(ymin = lower.limit, ymax = upper.limit)) + \n  # scale the colors and transparency. Intervals not containing \n  # the true mean are red.\n  scale_color_manual(values = c(\"red\", \"black\")) + \n  scale_alpha_manual(values = c(1, 0.2)) + \n  # Set label texts\n  labs(x = \"Sample\", \n       y = \"Mean\", \n       alpha = \"Contains\\nthe true mean\", \n       color = \"Contains\\nthe true mean\") + \n  # Change the theme\n  theme_minimal()\n  \n\n# Calculate the proportions of intervals not containing the true mean\nsum(cis$true.mean == FALSE) / sum(cis$true.mean == TRUE)\n# Almost 5%!\n\nOk so we are really close. This means that if we do a study 10000 times and calculate the average and its confidence interval, in about 5% of the studies we will be missing the true mean. Seen from the other side, about 95% of the times, our interval will contain the true mean.\n\n16.5.1.1 Why isn’t it 5%?\nIt may be due to the fact that the normal distribution is not a good distribution when sample sizes are low. We can instead use the \\(t\\)-distribution. (This distribution has something to do with beer!)\n\n\n\n\n\nThe t-distribution has something to do with beer!\n\n\n\n\nThe t-distribution changes it shape depending on how many data points we have in our sample. This means that a smaller sample size will be reflected in the estimated sampling distribution through a wider interval. In the code below we have changed the calculation of the 95% confidence interval to using the t-distribution instead.\n\n# Creat new variables with upper and lower limits of the confidence interval  \ncis &lt;- results %&gt;%\n  # Using the t-distribution\n  mutate(lower.limit = mean - qt(0.975, 20-1) * sd/sqrt(20), \n         upper.limit = mean + qt(0.975, 20-1) * sd/sqrt(20)) %&gt;%\n  # Test if the true mean is within the limits\n  # If the true mean is above the lower limit and below the upper limit then TRUE\n  # otherwise FALSE\n    mutate(true.mean = if_else(mean(vo2max) &gt; lower.limit & mean(vo2max) &lt; upper.limit, \n                             TRUE, FALSE)) \n\n# Plot the data, only plotting 100 data points to make it suitable for every computer\n\ncis[1:200, ] %&gt;%\n  ggplot(aes(seq(1:nrow(.)), \n             y = mean, \n             color = true.mean, # set a specific color \n             alpha = true.mean)) + # and transparency to \n  # intervals containing and not containing the true mean\n  \n  # add a line showing the true mean\n  geom_hline(yintercept = mean(vo2max)) +\n  # add errorbars showing each interval\n  geom_errorbar(aes(ymin = lower.limit, ymax = upper.limit)) + \n  # scale the colors and transparency. Intervals not containing \n  # the true mean are red.\n  scale_color_manual(values = c(\"red\", \"black\")) + \n  scale_alpha_manual(values = c(1, 0.2)) + \n  # Set label texts\n  labs(x = \"Sample\", \n       y = \"Mean\", \n       alpha = \"Contains\\nthe true mean\", \n       color = \"Contains\\nthe true mean\") + \n  # Change the theme\n  theme_minimal()\n  \n\n# Calculate the proportions of intervals not containing the true mean\nsum(cis$true.mean == FALSE) / sum(cis$true.mean == TRUE)\n# Almost 5%!\n\nAgain, almost!\nThe two distributions (normal and t) are very similar when we are getting closer to sample sizes of \\(n=30\\)."
  },
  {
    "objectID": "17-sample-and-population.html#sample-size-and-confidence-intervals",
    "href": "17-sample-and-population.html#sample-size-and-confidence-intervals",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.6 Sample size and confidence intervals",
    "text": "16.6 Sample size and confidence intervals\nThe width of confidence intervals are determined by the mean, standard deviation and the sample size. If the sample size gets lower the width will increase. This means that we will have less precision. We will still cover the true mean 95% of the time (if we repeat our study) but the range of possible values of the true mean will be wider."
  },
  {
    "objectID": "17-sample-and-population.html#sampling-distribution-of-iq",
    "href": "17-sample-and-population.html#sampling-distribution-of-iq",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.7 Sampling distribution of IQ",
    "text": "16.7 Sampling distribution of IQ\nIQ values are normally distributed with median 100 (since the distribution is normal, this should be very close the the mean) and standard deviation of 15. Using a sample from the population we can calculate a 95% confidence interval. We will do this with n=5, n=25 and n=100. You will have to execute the code. A 95% confidence interval based on the normal distribution can be calculated as\n\n# set the seed\nset.seed(1)\n\n# A population of numbers\npop &lt;- rnorm(100000, mean = 100, sd = 15)\n\n# Sampling from the distribution\nn5 &lt;- sample(pop, 5, replace = FALSE)\nn25 &lt;- sample(pop, 25, replace = FALSE)\nn100 &lt;- sample(pop, 100, replace = FALSE)\n\n\n# n = 10\nmean_n5 &lt;- mean(n5)\ns_n5 &lt;- sd(n5)\n\nerror_n5 &lt;- qnorm(0.975) * s_n5/sqrt(5)\n\n# n = 25\nmean_n25 &lt;- mean(n25)\ns_n25 &lt;- sd(n25)\n\nerror_n25 &lt;- qnorm(0.975) * s_n25/sqrt(25)\n\n# n = 100\nmean_n100 &lt;- mean(n100)\ns_n100 &lt;- sd(n100)\n\nerror_n100 &lt;- qnorm(0.975) * s_n100/sqrt(100)\n\nAbove we used the qnorm function. Test what the result is if you only run round(qnorm(0.975), 2). Above I used a commonly used rounded value, do they correspond?\nWe can collect the pieces and create a plot using this code:\n\ndf &lt;- data.frame(sample.size = c(5, 25, 100),\n                 mean = c(mean_n5, mean_n25, mean_n100), \n                 error = c(error_n5, error_n25, error_n100))\n\ndf %&gt;%\n  ggplot(aes(as.factor(sample.size), mean)) +\n  geom_errorbar(aes(ymin = mean-error, ymax = mean + error), width = 0.2) +\n  geom_point(size = 3) + \n  theme_minimal()\n\nWhat can you say about the effect of sample size on the confidence of an estimate?\nAbove we used the normal distribution to calculate the confidence interval. It would be more correct to use the t-distribution.\nThe corresponding code for calculating the error based on the t-distribution would be\nqt(0.975, df = n - 1) * s/sqrt(n)\nAdopt the code above and graph the different confidence intervals beside each other.\nHere is a possible solution\n\n\n# set the seed\n\nset.seed(123)\n\n\nn5 &lt;- sample(pop, 5, replace = FALSE)\nn25 &lt;- sample(pop, 25, replace = FALSE)\nn100 &lt;- sample(pop, 100, replace = FALSE)\n# n = 5\nmean_n5 &lt;- mean(n5)\ns_n5 &lt;- sd(n5)\n\nerror_n5 &lt;- qnorm(0.975) * s_n5/sqrt(5)\nerrort_n5 &lt;- qt(0.975, df = 5 - 1) * s_n5/sqrt(5)\n\n\n# n = 25\nmean_n25 &lt;- mean(n25)\ns_n25 &lt;- sd(n25)\n\nerror_n25 &lt;- qnorm(0.975) * s_n25/sqrt(25)\nerrort_n25 &lt;- qt(0.975, df = 25-1) * s_n25/sqrt(25)\n\n# n = 100\nmean_n100 &lt;- mean(n100)\ns_n100 &lt;- sd(n100)\n\nerror_n100 &lt;- qnorm(0.975) * s_n100/sqrt(100)\nerrort_n100 &lt;- qt(0.975, df = 100-1) * s_n100/sqrt(100)\n\nWe can collect the pieces and create a plot using this code:\n\ndf &lt;- data.frame(sample.size = c(5, 25, 100,  5, 25, 100),\n                 mean = c(mean_n5, mean_n25, mean_n100, mean_n5, mean_n25, mean_n100), \n                 error = c(error_n5, error_n25, error_n100, errort_n5, errort_n25, errort_n100), \n                 error.type = c(\"Normal\", \"Normal\", \"Normal\", \"t\", \"t\", \"t\"))\n\ndf %&gt;%\n  ggplot(aes(as.factor(sample.size), mean, color = error.type)) +\n  geom_errorbar(aes(ymin = mean-error, ymax = mean + error), \n                width = 0.2, \n                position = position_dodge(width = 0.2)) +\n  geom_point(size = 3, position = position_dodge(width = 0.2)) + \n  labs(x = \"Sample size\", \n       y = \"Estimated mean (95% CI)\", \n       color = \"Type of distribution\") +\n  theme_minimal()\n\nWhat can you say about using the t- vs. the z-distribution?"
  },
  {
    "objectID": "17-sample-and-population.html#a-hypothesis-test",
    "href": "17-sample-and-population.html#a-hypothesis-test",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.8 A hypothesis test",
    "text": "16.8 A hypothesis test\nWe know that a random sample will have a mean close to the center of the population distribution (100 in the case above). We want to know if chess players (\\(Chess\\)) have higher IQ scores than average people \\(Average\\). We can create an alternative hypothesis stating that\n\\[H_A: Chess \\neq Average\\]\nThe null hypothesis that we are comparing to is\n\\[H_0: Chess = Average\\]\nWe collect data from chess players (\\(n=24\\)). Use the data below to test \\(H_0\\).\n\n\n\nChess player\nIQ\n\n\n\n\n1\n129\n\n\n2\n101\n\n\n3\n98\n\n\n4\n89\n\n\n5\n103\n\n\n6\n107\n\n\n7\n123\n\n\n8\n117\n\n\n9\n114\n\n\n10\n109\n\n\n11\n110\n\n\n12\n99\n\n\n13\n101\n\n\n14\n102\n\n\n15\n130\n\n\n16\n121\n\n\n17\n129\n\n\n18\n115\n\n\n19\n107\n\n\n20\n109\n\n\n21\n107\n\n\n22\n96\n\n\n23\n98\n\n\n24\n102\n\n\n\nTry to calculate the sample average and a confidence interval and answer these questions:\n\nWhat is the average IQ among chess players?\nWhat is your best guess about the population average of chess players?\nWhat does the confidence interval say about your hypothesis?\n\nHere is a possible solution\n\n\nchess.players &lt;- c(129, 101,98 ,89 ,103,107,123,117,114,\n                   109,110,99 ,101,102,130,121,129,115,\n                   107,109,107,96 ,98,102)\n\n\nchess.mean &lt;- mean(chess.players)\n\nchess.error &lt;- qt(0.975, df = 24-1) * sd(chess.players)/sqrt(24)\n\n\nc(chess.mean - chess.error, chess.mean + chess.error)\n\n[1] 104.265 113.735"
  },
  {
    "objectID": "17-sample-and-population.html#using-a-confidence-interval-when-planning-a-study",
    "href": "17-sample-and-population.html#using-a-confidence-interval-when-planning-a-study",
    "title": "16  Samples, populations estimates and their uncertainty",
    "section": "16.9 Using a confidence interval when planning a study",
    "text": "16.9 Using a confidence interval when planning a study\nWe can calculate the mean change from pre- to post-training in the cycling study for \\(\\dot{V}O2_{max}\\).\nFor this exercise, use the data set cyclingStudy.xlsx, you will find it in the exscidata package. The variables of interest are subject, group, timepoint and VO2.max. In the time-point variable, meso3 are the post-training values and pre are the pre-training values.\nCalculate the mean change in percentage for the whole data set together with the sample SD. Then calculate a confidence interval.\nHow do you interpret the confidence interval?\nHere is a possible solution\n\n\nlibrary(tidyverse); library(exscidata)\ndata(\"cyclingstudy\")\n\n cyclingstudy %&gt;%\n        select(subject, group, timepoint, VO2.max) %&gt;%\n        filter(timepoint %in%  c(\"pre\", \"meso3\")) %&gt;%\n  pivot_wider(names_from = timepoint, values_from = VO2.max) %&gt;%\n  mutate(change = 100 * (meso3-pre)/pre) %&gt;%\n  group_by() %&gt;%\n  summarise(m = mean(change, na.rm = TRUE), \n            s = sd(change, na.rm = TRUE), \n            n = sum(!is.na(change)), \n            error = qt(0.975, df = n -1) * s/sqrt(n), \n            lower = m - error, \n            upper = m + error) %&gt;%\n  print()\n\nThe confidence interval can be interpreted as: in 95% percent of confidence intervals created from repeated sampling we will have the true mean. The confidence interval we have created does not contain 0.\n\n\nLet’s say that we are designing a new study. We want to be able to show a difference between pre- to post-training in \\(\\dot{V}O2_{max}\\) of 2% as this might be an important difference. Given the standard deviation that you have calculated above, how many participants should be recruit to the study to be able to detect a difference of 2%?\nHere you can try to calculate the lower limit of a 95% confidence interval given a standard deviation equal to what you calculated above and a mean change of interest of 2% using many different alternatives for the sample size.\nHere is a possible solution\n\n\nerror &lt;- qt(0.975, df = seq(from = 10, to = 100, by = 2) - 1) * 4.79 / sqrt(seq(from = 10,\n                                                                                to = 100, \n                                                                                by = 2))\n\nggplot(data.frame(n = seq(from = 10, to = 100, by = 2), \n                  lower.bound = 2-error), aes(n, lower.bound)) + geom_point() +\n  geom_hline(yintercept = 0)\n\n\n\nTry to answer the following questions:\n\nHow could the above information help you when designing a study?\nWhy is there a relationship between sample size and the lower bound of the confidence interval?\n\n\n\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "18-p-values.html#p-values-from-two-sample-comparisons",
    "href": "18-p-values.html#p-values-from-two-sample-comparisons",
    "title": "17  What is the p-value?",
    "section": "17.1 p-values from two sample comparisons",
    "text": "17.1 p-values from two sample comparisons\nIn a two sample scenario, we can model the null-hypothesis using re-shuffling of the data.\nWe sample two groups, one group has done resistance-training, the other group endurance training. We want to know if you get stronger if you do strength training as compared to endurance training. We measure the groups after an intervention. They were similar prior to training so we think that it is OK to measure the post-training values of 1RM bench press.\nThese are:\n\nstrength.group &lt;- c(75, 82.5, 87.5, 90, 67.5, 55, 87.5, 67.5, 72.5, 95)\nendurance.group &lt;- c(67.5, 75, 52.5, 85, 55, 45, 47.5, 85, 90, 37.5)\n\nWe can calculate the difference between the groups as:\n\nmean.difference &lt;- mean(strength.group) - mean(endurance.group)\n\nWe can simulate the null-hypothesis (\\(H_0\\)) by removing the grouping and sample at random to two groups. Read the code below and try to figure out what it does before you run it:\n\nset.seed(123)\n\nresults &lt;- data.frame(m = rep(NA, 1000))\n\nfor(i in 1:1000){\n     \n# Here we combine all observations\n        all.observations &lt;- data.frame(obs = c(strength.group, endurance.group)) %&gt;%\n          # based on a random process each iteration of the for-loop assign either endurance or strength to each individual\n          mutate(group = sample(c(rep(\"strength\", 10), \n                                  rep(\"endurance\", 10)), \n                                size = 20, \n                                replace = FALSE))  %&gt;%\n          group_by(group) %&gt;%\n          summarise(m = mean(obs)) \n        \n\n        # calculate the difference in means and store in the results data frame\n        results[i, 1] &lt;- all.observations[all.observations$group == \"strength\", 2] - all.observations[all.observations$group == \"endurance\", 2]\n\n        }\n\nresults %&gt;%\n  ggplot(aes(m)) +  geom_histogram(fill = \"lightblue\", color = \"gray50\", binwidth = 1) +\n  geom_vline(xintercept = mean.difference) +\n  labs(x = \"Mean difference\", \n       y = \"Count\") + \n  theme_minimal()\n\nWhat does the graph above show? As the reshuffle process was done 1000 times we can count the number of means more extreme than the mean that we did observe.\n\nlength(results[results$m &gt; mean.difference,]) / 1000\n\nThe above code calculates the proportion of mean differences greater than the observed difference that occurred when we re-shuffled the groups by chance.\n\n# An illustration of the above\n\nresults %&gt;%\n  mutate(extreme = if_else(m &gt; mean.difference, \"More extreme\", \"Less extreme\")) %&gt;%\n  ggplot(aes(m, fill = extreme)) +  geom_histogram(color = \"gray50\", binwidth = 1) +\n  scale_fill_manual(values = c(\"lightblue\", \"purple\")) +\n  geom_vline(xintercept = mean.difference) +\n  labs(x = \"Mean difference\", \n       y = \"Count\") + \n  theme_minimal()\n\nWe have now calculated the proportion of values more extreme than the observed. This would represent a directional hypothesis\n\\[H_A: Strength &gt; Endurance\\] We can account for the fact that the endurance group could be stronger than the strength group with the code below. Try to figure out what the code does and what has changed from above.\n\nresults %&gt;%\n  mutate(extreme = if_else(abs(m) &gt; mean.difference, \"More extreme\", \"Less extreme\")) %&gt;%\n  ggplot(aes(m, fill = extreme)) +  geom_histogram(color = \"gray50\", binwidth = 1) +\n  scale_fill_manual(values = c(\"lightblue\", \"purple\")) +\n  geom_vline(xintercept = mean.difference) +\n  labs(x = \"Mean difference\", \n       y = \"Count\") + \n  theme_minimal()\n\n\nlength(results[abs(results$m) &gt; mean.difference,]) / 1000\n\nTry to think about these questions:\n\nAt what level of p are you comfortable rejecting the null-hypothesis?\nWhen planning a study we decide on a level of evidence needed to reject the null. If you would have planned a study, how do you motivate a certain level of evidence?"
  },
  {
    "objectID": "18-p-values.html#more-p-values",
    "href": "18-p-values.html#more-p-values",
    "title": "17  What is the p-value?",
    "section": "17.2 More p-values",
    "text": "17.2 More p-values\nIn the example above where we used the reshuffling technique (also called a permutation test), we are limited by the number of iterations in the for-loop and the size of the groups to produce a valid p-value. As long as the data are approximately normally distributed, we can use a t-test instead. As outlined above, this test uses an imaginary sampling distribution and compare our results to a scenario where the null-hypothesis in true.\nTo test against the null hypothesis that the means of the groups described above are equal we would use a two-sample t-test.\n\nt.test(strength.group, endurance.group, paired = FALSE, var.equal = FALSE)\n\nRun the code above and inspect the results. The output of the two-sample t-test contains information on the test statistic t, the degrees of freedom for the test and a p-value. It even has a statement about the alternative hypothesis. We get a confidence interval of the mean difference between groups.\nDoes the 95% confidence interval say anything about the level of evidence needed to reject the null? Has somebody already decided for you? See in the help-file for the t-test (?t.test) and see if you can change the confidence interval to correspond to your level of evidence. What is the relationship between p-values and confidence intervals?\nThe t-test is quite flexible, we can do one-sample and two-sample t-tests. We can account for paired observation, as when the same participant is measured twice under different conditions.\nIn the one-sample case, we test against the null-hypothesis that a mean is equal to \\(\\mu\\) where we specify the mu in the function.\nIf we have paired observations we set paired = TRUE, each row in such case must correspond to the same individual.\nWe can also assume that the groups have equal variance (var.equal = TRUE) this corresponds to the classical two-sample case. A more appropriate setting is to assume that the groups do not have equal variance (spread), this is the Welch two-sample t-test.\nBy saving a t-test we can access different results from it. To see what parts we can access we may use the names function which gives the names of the different parts of the results.\n\nttest &lt;- t.test(strength.group, endurance.group, paired = FALSE, var.equal = FALSE)\n\nnames(ttest)\n\nTo access the confidence interval, we would use ttest$conf.int."
  },
  {
    "objectID": "18-p-values.html#t-test-examples",
    "href": "18-p-values.html#t-test-examples",
    "title": "17  What is the p-value?",
    "section": "17.3 t-test examples",
    "text": "17.3 t-test examples\nIn the cycling data set we might want to know if the difference between pre- and post-data in squat jump is more or less than 0. Assuming that we use the same participants, this is a paired t-test.\n\nlibrary(tidyverse); library(exscidata)\n\ndata(\"cyclingstudy\")\n# Prepare the data\nsj.max &lt;- cyclingstudy %&gt;%\n  select(subject, timepoint, sj.max) %&gt;%\n  filter(timepoint %in% c(\"pre\", \"meso3\")) %&gt;%\n  pivot_wider(names_from = timepoint, values_from = sj.max) \n\n# calculate the t-test, paired data\nsj.ttest &lt;- t.test(sj.max$pre, sj.max$meso3, paired = TRUE)\n\n# plot the data to see corresponding data\nsj.figure &lt;- sj.max %&gt;%\n  pivot_longer(names_to = \"timepoint\",\n               values_to = \"sj.max\", \n               cols = pre:meso3) %&gt;%\n  mutate(timepoint = factor(timepoint, levels = c(\"pre\", \"meso3\"))) %&gt;%\n  ggplot(aes(timepoint, sj.max)) + geom_boxplot()\n\n# create a summary statistic\nsj.summary &lt;- sj.max %&gt;%\n  pivot_longer(names_to = \"timepoint\",\n               values_to = \"sj.max\", \n               cols = pre:meso3) %&gt;%\n  group_by(timepoint) %&gt;%\n  summarise(m = mean(sj.max, na.rm = TRUE), \n          s = sd(sj.max, na.rm = TRUE))\n\nThe above can be interpreted as: “Squat jump height was on average higher in the pre- compared to the meso3 time-point (30.8 (SD: 3.2) vs. 29.9 (3.3)), this did not lead to rejection of the null-hypothesis of no difference between time-points (t(18)=1.79, p=0.091, 95% CI: -0.16, 1.94)”.\nYou might want to plot the results also, use the figure in the code above to do this.\nThe interpretation above has “two levels”, first I describe that there actually is a difference in means between the time points, this is a statement about the data. Then we use the t-test to make a statement about the population. Notice that we use the standard deviation when describing the data and the t-statistic and confidence intervals when describing the our best guess about the population.\nTry to do the same with VO2max. Calculate descriptive statistics and draw inference using a paired samples t-test."
  },
  {
    "objectID": "18-p-values.html#error-rates",
    "href": "18-p-values.html#error-rates",
    "title": "17  What is the p-value?",
    "section": "17.4 Error rates",
    "text": "17.4 Error rates\nUp to now we have not explicitly talked about the level of evidence needed to reject the null. The level of evidence needed is related to the nature of the problem. We can make two types of errors in classical frequentist statistics. A type I error is when we reject the null when it is actually true. A type II error is when we fail to reject the null and it is actually false.\nIf we do not care that much about making a type I error we might be in a situation where a research project might conclude that a intervention/device/nutritional strategy is beneficial and if not there is no harm done. The side effects are not that serious. Another scenario is when we do not want to make a type I error. For example if a treatment is non-effective, the potential side effects are unacceptable. In the first scenario we might accept higher error rates, in the second example we are not prepared to have a high false positive error rate as we do not want to recommend a treatment with side effects that is also ineffective.\nA type II error might be serious if an effect is beneficial but we fail to detect it, we do not reject the null when it is false. As we will discuss later, error rates are connected with sample sizes. In a study where we have a large type II error because of a small sample size we risk not detecting an important effect.\nType I error might be thought of as the long run false positive rate. If we would have drawn samples from a population with a mean of 0 and test against the \\(H_0: \\mu = 0\\). A pre-specified error rate of 5% will protect us from drawing the wrong conclusion but only to the extent that we specify. We can show this in a simulation by running the code below.\n\nset.seed(1)\npopulation &lt;- rnorm(100000, mean = 0, sd = 1)\n\nresults &lt;- data.frame(p.values = rep(NA, 10000))\n\nfor(i in 1:10000){\n  \n  results[i, 1] &lt;- t.test(sample(population, 20, replace = FALSE), mu = 0)$p.value\n  \n}\n\n# filter the data frame to only select rows with p &lt; 0.05\nfalse.positive &lt;- results %&gt;%\n  filter(p.values &lt; 0.05) %&gt;%\n  nrow()\n  \nfalse.positive / 10000 # should be ~ 5%\n\nTry to explain what the code above do before running it."
  },
  {
    "objectID": "18-p-values.html#p-values-and-confidence-intervals",
    "href": "18-p-values.html#p-values-and-confidence-intervals",
    "title": "17  What is the p-value?",
    "section": "17.5 P-values and confidence intervals",
    "text": "17.5 P-values and confidence intervals\nTransparent reporting of a statistical test include estimates and “the probability of obtaining the result, or a more extreme if the null was true” (the p-value). Estimates may be given together with a confidence interval. The confidence interval gives more information as we can interpret the magnitude of an effect and a range of plausible values of the population effect. Sometimes the confidence interval is large, we may interpret this as having more uncertainty.\nIn the cycling study we may test against the null-hypothesis that there is no effect of training on tte (time to exhaustion). Try to answer these questions by preparing and doing the test.\n\nWhat kind of t-test is suitable for testing pre vs. meso3 data in the tte variable?\nWhat level of evidence do you think is suitable for the test, what is your level of statistical significance?\nHow do you interpret the p-value and confidence interval.\n\nHere is a possible solution\n\nBelow is example code to set up the test\n\n# Prepare the data\ntte.data &lt;- cyclingstudy %&gt;%\n  select(subject, timepoint, tte) %&gt;%\n  pivot_wider(names_from = timepoint, values_from = tte) %&gt;%\n  print()\n\ntte.ttest &lt;- t.test(tte.data$meso3, tte.data$pre,  paired = TRUE)\n\n\n# The point estimate \ntte.ttest$estimate\n\n# Get the confidence interval \ntte.ttest$conf.int\n\n# Get the p-value\ntte.ttest$p.value"
  },
  {
    "objectID": "19-statistical-power.html#defining-the-alternative-hypothesis",
    "href": "19-statistical-power.html#defining-the-alternative-hypothesis",
    "title": "18  Hypothesis tests and statistical power",
    "section": "18.1 Defining the alternative hypothesis",
    "text": "18.1 Defining the alternative hypothesis\nAccording to the Neyman-Pearson approach to statistical testing we should plan our studies with two known (long-run) error-rates. First we should specify what percentage of studies would reject the null-hypothesis if it is actually true. This error-rate is referred to as \\(\\alpha\\). Second, we should specify a rate at which we are willing to miss rejecting the null-hypothesis (or finding evidence for an effect/difference of a certain size) if the alternative hypothesis actually exists in the population. This error rate is called \\(\\beta\\) but often expressed as \\(1-\\beta\\) (statistical power). If we want to have a power (\\(1-\\beta\\)) of 90% we simply state that if the effect exists in the population we will find it with our test in 90% of studies given repeated sampling.\nWhen designing a study we want to balance the two types of errors. As previously noted, we might be in a situation where a Type II error is relatively more serious than a Type I error. If we are trying to determine if a training method without any negative side effects is beneficial on performance, a Type I error might not be so serious as we would recommend using a method without any beneficial effect but also without harmfull effects. If there is a cost of implementing a treatment or training method on the other hand, we want to be more carefull with recommending it as if it has no positive effect the cost of implementation would mean negative consequences for e.g. an athlete.\nWhen designing a study we are often interested in the number of participants needed to obtain certain error-rates. The power of a test, defined as the long-run chance of “detecting” a true alternative hypothesis depends on the size of the effect in the alternative hypothesis, the number of participants in the study, and the \\(\\alpha\\) level. In the figure below we have plotted power values as a function of the sample size, given a fixed size of the effect of 0.4 (see below) and different values of the \\(\\alpha\\) error-rate of 0.001-20%.\n\n\n\n\n\nPower as a function of sample size\n\n\n\n\nThis means that: If there is a true effect in the population, we will be more likely to reject the null-hypothesis if we\n\nIncrease the sample size, or\nIncrease the rate at which we would reject the null-hypothesis if it was actually true (the \\(\\alpha\\)-level)"
  },
  {
    "objectID": "19-statistical-power.html#the-size-of-the-effect",
    "href": "19-statistical-power.html#the-size-of-the-effect",
    "title": "18  Hypothesis tests and statistical power",
    "section": "18.2 The size of the effect",
    "text": "18.2 The size of the effect\nAbove we used a fixed standardized effect size of 0.4 to obtain power values. In the case of comparing two groups, an appropriate standardized effect-size (ES) is the ratio of a meaningful difference (MD) to the estimated population standard deviation (SD)\n\\[ES = \\frac{MD}{SD}\\] Finding a meaningful difference is the tricky part in power calculations. In a clinical setting for patient populations, this could mean a difference induced by the treatment that leads to a perceived beneficial effect for an individual. As the outcome of a study could be a reduced pain-score or blood pressure, the design of the study needs to address what difference is of importance.\nIn elite sports, an improvement in an athletes performance of 0.3-0.4 times the within-athlete performance variability was considered important as it would increase the chance of winning a competition from 38 to 48% for a top athlete (Hopkins, Hawley, and Burke 1999)."
  },
  {
    "objectID": "19-statistical-power.html#power-and-study-designs",
    "href": "19-statistical-power.html#power-and-study-designs",
    "title": "18  Hypothesis tests and statistical power",
    "section": "18.3 Power and study designs",
    "text": "18.3 Power and study designs\nIn healthy Norwegian women between between 30 and 39 years of age, the average VO2max has been estimated to 40 with a standard deviation of 6.8 (Loe et al. 2013). From the age of 30, VO2max decreases by about 3 ml kg-1 min-1 per every ten years. Let’s say that we want to design a study to investigate the effect of a new training method. We might then think that an improvement of VO2max corresponding to a five year decline (half of the average ten year decline) would be important to detect. We compare two training methods in two parallel groups, one group gets to train with traditional method, the other group trains with the our new method. We plan to measure VO2max in both groups, after the intervention. The null hypothesis says that the groups are not different after the intervention. The alternative hypothesis states that the new method leads to a difference between training methods of at least 1.5 ml kg-1 min-1 between groups. We will accept an \\(\\alpha\\) error rate of 5% and \\(\\beta\\) error rate of 20% as we aim for a 80% chance of detecting a true effect (of at least 1.5 ml kg-1 min-1), if it is actually true.\nWe now have all the numbers we need for a sample size estimation. In R the pwr package provide functions for calculation of sample size, power or effect-sizes in commonly used statistical tests. The effect-size (\\(d\\)) can be calculated as the smallest effect of interest divided by the population standard deviation \\(d = \\frac{1.5}{6.8} = 0.22\\). We can input our numbers in the function pwr.t.test.\n\nlibrary(pwr)\n\npwr.t.test(d = 1.5/6.8, sig.level = 0.05, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 323.5688\n              d = 0.2205882\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nFrom the output we can see that we need more than 324 participants in each group to get a power of 80% given an significance-level (\\(\\alpha\\)) of 5%. This is a BIG study!\nHowever, the power of a study is also influence by the study design. In the example above we used a single test after the intervention to determine the number of participants. This is an inefficient design as we need more participants to account for the uncertainty in the sampling process. If we instead rephrase the question to concern the change of VO2max between groups and measure each participant before and after the intervention we are designing a more efficient study, fewer participants are needed to reach the same power. We can calculate an approximate standard deviation of the change score (difference between pre- and post-treatment measurements) by combining an expected SD of pre- and post-treatment scores and the correlation between them (Higgins, Li, and Deeks 2019). We get these numbers from previous studies (Loe et al. 2013; Astorino et al. 2013).\nWe are still interested in a difference between groups of 1.5 ml kg-1 min-1, i.e. the alternative hypothesis is that the two training methods we are examining will differ 1.5 ml kg-1 min-1 in their average responses. The SD of the change score is determined to be 4.7, the resulting standardized effect size is therefore about 0.32. Given the same power and significance level we get the resulting sample size estimate:\n\nlibrary(pwr)\n\npwr.t.test(d = 1.5/4.7, sig.level = 0.05, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 155.083\n              d = 0.3191489\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nA change in the design of the study resulted in a sample size less than half of the more inefficient study.\n\n\n\n\nAstorino, T. A., M. M. Schubert, E. Palumbo, D. Stirling, D. W. McMillan, C. Cooper, J. Godinez, D. Martinez, and R. Gallant. 2013. “Magnitude and Time Course of Changes in Maximal Oxygen Uptake in Response to Distinct Regimens of Chronic Interval Training in Sedentary Women.” Journal Article. Eur J Appl Physiol 113 (9): 2361–69. https://doi.org/10.1007/s00421-013-2672-1.\n\n\nHiggins, Julian PT, Tianjing Li, and Jonathan J Deeks. 2019. “Choosing Effect Measures and Computing Estimates of Effect.” Book Section. In Cochrane Handbook for Systematic Reviews of Interventions, 143–76. https://doi.org/https://doi.org/10.1002/9781119536604.ch6.\n\n\nHopkins, W. G., J. A. Hawley, and L. M. Burke. 1999. “Design and Analysis of Research on Sport Performance Enhancement.” Journal Article. Med Sci Sports Exerc 31 (3): 472–85. https://doi.org/10.1097/00005768-199903000-00018.\n\n\nLoe, H., Ø Rognmo, B. Saltin, and U. Wisløff. 2013. “Aerobic Capacity Reference Data in 3816 Healthy Men and Women 20-90 Years.” Journal Article. PLoS One 8 (5): e64319. https://doi.org/10.1371/journal.pone.0064319.\n\n\nSpiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books."
  },
  {
    "objectID": "22-analyzing-trials.html#background",
    "href": "22-analyzing-trials.html#background",
    "title": "19  Pre- to post-treatment analysis",
    "section": "19.1 Background",
    "text": "19.1 Background\nIn sport science (and e.g. medical-, nutritional-, psychological-sciences), intervention-studies are common. We are interested in the effect of e.g. a training method, nutritional supplement or drug. The outcome in these studies could be physical performance, degree of symptoms, muscle size or some other measure that we are interested in studying. These studies are often called Randomized Controlled Trials (RCT).\nThe choice of study design relates to the research-question and dictates what statistical methods can be applied. The study design affects the ability of the study to detect an effect (the power). A common case of a RCT is the parallel-groups design. In a parallel-group design participants are allocated to two or more “treatment groups”, at random, one group gets a treatment, the other group acts as the control. Usually, a measurement is made prior to the intervention (Baseline) and after the intervention. This is a common design when wash-out period is not possible and thus, the two treatment can not be compared within the same individual.\nIn a design where we have a Treatment group and a control group for comparison hypothesized outcomes can look something like in Figure @ref(fig:trial-design-fig).\n\n\n\n\n\nHypothesized values from a simple pre-post parallel-groups design\n\n\n\n\nAnother common scenario is that we expect progress in two different treatments groups as in Figure @ref(fig:trial-design-fig2).\n\n\n\n\n\nHypothesized values from a simple pre-post parallel-groups design including to different treatments\n\n\n\n\nIn both scenarios we are interested in the treatment effect (or the difference in effect of two different treatments). This means that we want to establish if\n\\[ \\Delta Y_A-\\Delta Y_B \\neq 0 \\]\nmeaning that the null hypothesis is that the change (\\(\\Delta\\)) in group A is not different to the change in group B. To evaluate these studies we could do a t-test on the change score between groups. This is equivalent to a regression model where we estimate the difference between groups:\n\\[outcome = \\beta_0 + \\beta_1 Group_B\\] In R, these to alternatives can be easily fitted using the code presented in code chunk (CC) 1:\n\n# A t-test example\nwith(data, t.test(outcome_A, outcome_B, paired = FALSE)\n\n# The same using a regression model\nlm(change ~ group, data = data)\n\nThis seemingly simple solution has some benefits but does not take into account that baseline values can affect the interpretation of a pre- to post-intervention study through regression towards the mean.\nIf we analyze change scores (\\(post-pre\\)), regression towards the mean will give an overestimation of the effect if there is, by chance, a difference in baseline values between groups (lower values in treatment group) (Vickers and Altman 2001). If we analyze follow up scores (differences in post-scores between groups), the pattern will be reversed. To fix this problem we could control for the relationship between baseline values and the change scores. This technique is called Analysis of Co-Variance (ANCOVA), where the baseline is considered the added co-variance. This is an extension of the simple linear regression model (CC2).\n\n# Extending the linear regression equation\nlm(change ~ baseline + group, data = data)\n\nWe prefer the ANCOVA model over the ordinary regression model as the ANCOVA model has better power (Senn 2006). The ANCOVA model also gives unbiased estimates of differences between groups (Vickers and Altman 2001). We can use the ANCOVA model when the allocation of participants have been done at random (e.g. RCTs) meaning that differences at baseline should be due to random variation."
  },
  {
    "objectID": "22-analyzing-trials.html#exercises-ten-vs-thirty-rm-study",
    "href": "22-analyzing-trials.html#exercises-ten-vs-thirty-rm-study",
    "title": "19  Pre- to post-treatment analysis",
    "section": "19.2 Exercises: Ten vs thirty RM-study",
    "text": "19.2 Exercises: Ten vs thirty RM-study\nThirty-one participants were assigned to one of two groups training with either 10 repetition maximum (RM) or 30RM, 27 participants completed the trial to the mid-study evaluation and 24 participants completed the full study. The main question in the study was whether these two different treatments resulted in different magnitudes of strength development or muscle hypertrophy (we are interested in strength).\nThe data is available in the exscidata package and contains the following variables:\n\nparticipant: ID of participant\ntime: prior to pre or after the intervention post\ngroup: The intervention group\nexercise: The exercise that was tested, legpress, benchpress or bicepscurl\nload: The load lifted in 1RM test (kg)\n\nAn example of loading the data and plotting the data can be seen in CC3:\n\nlibrary(tidyverse); library(exscidata)\ndata(\"tenthirty\")\n\n\n tenthirty %&gt;%\n        mutate(time = factor(time, \n                                  levels = c(\"pre\", \"mid\", \"post\"))) %&gt;%\n        ggplot(aes(time, load, fill = group)) + \n        geom_boxplot() +\n        facet_wrap(~ exercise, scales = \"free\") + \n  theme_minimal()\n\nThe main purpose of our analysis is to answer the question: what training method can we recommend for improving maximal strength? To try to answer this question we will (1) select a suitable 1RM test exercise, (2) choose the most appropriate statistical model. To illustrate differences between models we will compare different models (lm on the change-score, lm with baseline as a covariate, lm on post-values with baseline as a covariate).\n\n19.2.1 Reducing the data set\nFor this exercise we will focus on the pre- to post-analysis of leg-press data. To filter the data set we can use the code in CC4. We will have to re-shape the data for the calculation of change scores. We do this and add a simple calculation of change scores \\(post-pre\\).\n\nlibrary(exscidata); data(\"tenthirty\")\n\n\ntenthirty_reduced &lt;- tenthirty %&gt;%\n  filter(time != \"mid\", \n         exercise == \"legpress\") %&gt;%\n  # To get the variables in the correct order we need...\n        mutate(time = factor(time, \n                                  levels = c(\"pre\",  \"post\"))) %&gt;% \n  pivot_wider(names_from = time, \n              values_from = load) %&gt;%\n  mutate(change = post - pre) %&gt;%\n  filter(!is.na(change))\n\nWe are now ready to fit some models, these are outlined in CC5.\nBefore we look at the models, a word of caution: We should not select the model that best suit our hypothesis or belief. Instead we should formulate a model that fits our question and interpret the results from a model that meets the assumptions of the analysis (in the case of regression analysis: homoscedasticity, normally distributed residuals etc.).\nIn this study it is reasonable to account for the baseline difference between groups. These differences are there because of the randomization. We may account for them by including them in our analysis (as in m2 and m4). To check if the addition of the baseline helps explain the data we can perform analysis of variance when comparing two models using the anova()-function.\nThe null hypothesis is that the addition of the pre variable does not help explain the observed variation in the data. Comparing model 1 and 2, and 3 and 4 (these have the same dependent variable) we see that there is fairly strong evidence against the null hypothesis (CC6).1\nTo check if the models meet the assumptions of regression models we can use the plot function. Let’s first look at m1 comparing change score between groups.\n\n\n\n\n\nDiagnostic plots of Model 1\n\n\n\n\nThe plots in Figure @ref(fig:m1-diagnostics-plot) suggests that there is evidence of violation of the assumption of homoscedasticity (Residuals vs fitted, larger spread in higher fitted values also evident in the scale location plot). We can also see that the residuals are not normally distributed (Normal Q-Q plot). This model is not that good.\nLet’s check the model with change between groups controlling for baseline values (Model 2). To create a similar grouped plot as above, use the code in CC7\nThis is not a perfect model either, but the residuals looks a bit better. In fact the only model that would (almost) fail a more formal test is Model 1. The Brusch-Pagan test (see CC8) tests for heteroscedasticity.2\n\n\n19.2.2 Inference\nSuccess! Our models are somewhat OK. We may draw inference from them. It turns out that model estimating the change score or the post score does not matter when we control for the baseline. The difference between groups are exactly the same in Model 2 and 4 (CC9).\nThe pre variable changes as the relationship to the change is different to the relationship to post scores but the model does the same thing! This is now a question of how we would like to frame our question. If the question is “do people that train with 10RM increase their strength more than people that train with 30RM (assuming a similar baseline strength level)?” then model 2 is preferred. If the question is “are people that trained with 10RM stronger than people who have trained with 30RM (assuming a similar baseline strength level)?”, model 4 is preferred. The differences are basically semantics as the models tests the same thing, the differences between groups when controlling for baseline differences.\nAs we control for the baseline, we removes some of the unexplained error from the model, this will lead to a more powerful model. This is reflected in the stronger evidence3 against the null-hypothesis of no difference between groups.\n\n\n19.2.3 What if the model diagnostics says the models are no good?\nBiological data and performance data often exhibit larger variation at larger values. This may lead to heteroscedasticity. A common way of dealing with this is the log transformation. Transforming the data to the log scale changes the interpretation of the regression coefficients.\n\n# A linear model with the dependent variable on the linear scale!\nm.linear &lt;- lm(post ~ pre + group,  data = tenthirty_reduced)\n\n# A linear model with the dependent variable on the log scale!\nm.log &lt;- lm(log(post) ~ pre + group,  data = tenthirty_reduced)\n\nAs we interpret the regression coefficients as differences the laws of the log are important to remember:\n\\[log(\\frac{A}{B}) = log(A) - log(B)\\] This means that the difference between two variables on the log scale is the same as the log of their ratio. When we back-transform values from the log scale we get the fold differences.\nLet’s say that we have a mean in group A of 40 and a mean in group B of 20. The difference is 20. If we estimate the difference on the log scale however we will get (CC9):\n\na &lt;- log(40/20)  \n\nb &lt;- log(40) - log(20)\n\nc &lt;- 40/20\n\nexp(a)\nexp(b)\nc\n\nThe exp function back-transforms data from the log scale. Back-transforming a difference between two groups (as estimated in our model) will yield the fold-difference, this can be calculated as a percentage difference. In the code chunk below the log-difference between groups is transformed to percentage differences using:\n\\[Percentage~difference = (1-exp(estimate)) \\times 100\\]\nIf we want to express the 30RM group as a percentage of the 10RM group we could remove 1 from the equation:\n\\[Percentage~of~10RM = exp(estimate) \\times 100\\]\nThe function tidy from the broom package is used to access the model output.\nThe 30RM group is 11.3% weaker than the 10RM group. Alternatively we can express the values as a percentage of the 10RM group. The 30RM group has a strength level that is 88.7% of the 10RM group.\nSimilarly to the point estimate, a confidence interval may also be back-transformed."
  },
  {
    "objectID": "22-analyzing-trials.html#case-study-single-vs.-multiple-sets-of-training-and-muscle-mass",
    "href": "22-analyzing-trials.html#case-study-single-vs.-multiple-sets-of-training-and-muscle-mass",
    "title": "19  Pre- to post-treatment analysis",
    "section": "19.3 Case study: Single vs. multiple sets of training and muscle mass",
    "text": "19.3 Case study: Single vs. multiple sets of training and muscle mass\nIn this study, n = 34 participants completed a resistance training intervention with multiple-set and single-set randomized to either leg. Muscle mass was measured through the use of regional estimation of lean mass with a DXA machine. In this case study we will analyze data were participants have been selected either to belong to the single- or multiple-set group. This means we will only analyze one leg per participant!\n\n19.3.1 Prepare the data set\nThe data can be found in the exscidata package as the dxadata data set, use ?dxadata to inspect the different variables. The data set is quite complex, use the code below to get the correct, reduced data set.\nWe will randomly select participants left of right leg. To get the same estimates as in these examples you need to set the seed to 22 before the randomization (set.seed(22)).\n\n# Copy this code to get the correct data set. \n\nlibrary(exscidata); library(tidyverse)\ndata(\"dxadata\")\n\n# Set the random number generator  \nset.seed(85)\n\n  # Create a data frame with \"selected\" legs\n  # this data frame will help us \"randomize\" participants to either group.\nlegs &lt;- dxadata %&gt;%\n filter(include == \"incl\") %&gt;%\n distinct(participant) %&gt;%\n mutate(selected.leg = sample(c(\"L\", \"R\"), size = nrow(.), replace = TRUE)) \n\n\n\n\ndxadata_reduced &lt;- dxadata %&gt;%\n  # Filter only participants completing at least 85% of the prescribed sessions\n  filter(include == \"incl\") %&gt;%\n  # Select relevant columns\n  select(participant:sex, lean.left_leg, lean.right_leg) %&gt;%\n  # Using pivot longer we gather the lean mass data\n  pivot_longer(names_to = \"leanleg\", \n               values_to = \"mass\", \n               cols = lean.left_leg:lean.right_leg) %&gt;%\n  # Change the leg identification from the DXA machine\n  mutate(leanleg = if_else(leanleg == \"lean.left_leg\", \"L\", \"R\")) %&gt;%\n  # Gather data from the training volume variable to \"sets\"\n  pivot_longer(names_to = \"sets\", \n               values_to = \"leg\", \n               cols = multiple:single) %&gt;%\n  # Filter observations to correspond to the training volume variable\n  filter(leg == leanleg) %&gt;%\n  # Join the data set with the selected legs data set\n  inner_join(legs) %&gt;%\n  # Filter to keep only \"selected\" legs, these are the legs that we picked randomly above\n  filter(leg == selected.leg) %&gt;%\n  # Select relevant variables\n  select(participant, time, sex, sets, mass) \n\n\n\n19.3.2 Exploratory analysis\nUse descriptive methods (summary statistics and figures to describe results from the trial). What are the mean and standard deviations of the mass variable for each time and training volume (sets). Use tables and figures to show the results.\n\nHere is a possible solution for a table\n\nWe want to summarize data per volume condition, sex and time-point and we want the means, standard deviations and the number of observations.\n\n\nHere is a possible solution for a figure\n\nUsing the same summary statistics, we can create a figure. We might want to sort variables so that time-points are in the right order. In the figure we can use geom_errorbar to display SD.\n\n\nWhat can you say about the effect of single- vs. multiple-sets training on muscle strength using these observations?\n\n\n19.3.3 Change from baseline\nCalculate the average change from baseline (pre) in each group and create a graph of the results. Make a plan of your code before you start writing it!\nHere is a possible solution for a figure\n\nHere we want to perform the following steps:\n\nMake the data set wider and calculate change as \\(post-pre\\)\nGroup the data set by sets\nSummarize the data and create a figure with groups on the x-axis and change on the y-axis.\n\nExample code\n\n\n\n\n\n\n\n\n19.3.4 Model the change\nThe present data set is an example of a simple randomized trial. Participants have been randomized to either treatment before the trial and we are interested in the difference in treatment outcomes. There are several options for analyzing these data. A simple alternative would be to analyze the difference in post-training values with a t-test. This is not very efficient in terms of statistical power, i.e. we would need a lot of participants to show a difference if it existed due to differences between participants. In the case of the present data set, we have also collected more data than just follow-up scores. The baseline scores can be used to calculate a change-score which in turn can be used to compare treatments. This is a more direct comparison related to our actual question. We want to know what treatment is more effective in improving muscle mass.\nWrite code to test the if there is a difference between the groups in post values and change scores. Use the lm function with sets as the grouping variable.\nExample code\n\n\n\n\n\n\n19.3.5 Accounting for the baseline – Regression to the mean and extending the linear model\nAbove we created linear models from where you got exactly the same results as from the t-test. The models differed in that one model only estimated the difference in poast-scores whereas the other model assessed the differences between volume conditions in change scores. In this section we will see that the linear model is easily extended to create more robust statistics of outcome values.\nWhen we have calculated a change score we are taking the baseline values into account. However, this score might be related to the baseline due to a phenomena known as regression towards the mean. When we do repeated testing on the same participants, test scores that were close to the upper or lower limit of a participant potential scores will be replaced with a score closer to the mean. This in turn will create a negative association between initial values and change. We could account for this by adding the baseline values as a covariate.\nBy adding the baseline values as a covariate we adjust the estimate of the difference between treatments to a difference between groups if they had the same baseline values.\nSimilarly, we can add a baseline score to the model where we only assessed baseline scores. We are now assessing the difference between post-treatment values accounting for the baseline values. This model contains the same information as the adjusted change-score model.\nThe two extended models discussed above may be called ANCOVA models (ANalysis of CO-VAriance). The ordinary linear models we used above can be written e.g. as:\n\\[Change = \\beta_0 + \\beta_1 \\times Group\\]\nThe extended, ANCOVA model can be written as\n\\[Change =  \\beta_0 + \\beta_1 \\times Baseline + \\beta_2 \\times Group\\]\nUsing the DXA data dxadata_reduced, create models with the baseline values as a covariate, use the change score in one model and the post-treatment scores in the other model. Assess the difference between groups, how do you interpret it?\nCan you also explain the difference between models in the relationship between the dependent variable and the baseline covariate?\nExample code\n\n\ndata &lt;- dxadata_reduced %&gt;% \n        pivot_wider(names_from = time, \n                    values_from = mass) %&gt;%\n        mutate(change = post - pre) %&gt;%\n  print()\n  \n## The linear model of post scores adjusted for baseline values\nsummary(lm(post ~ pre + sets, data = data))\n\n## The linear model of change scores adjsuted for baseline values\nsummary(lm(change ~ pre + sets, data = data))\n\n\n\n\n\n19.3.6 Training volume and strength - Extending the model further\nIn the following example we will use the isokinetic (60 degrees per second) data from the same study isok.60 to infer on the effect of single- vs. multiple-set training. We will use the pre- to post training scores for change score calculations. In the data set we have men and women, to get an unbiased association to the baseline, we will mean center the pre-values per sex.\nWe need to prepare the data set, we will use the same “randomization” as above, accomplished by joining the legs data set containing the “randomization” to the strengthvolume (from the exscipackage). We will have to take the maximum isometric force from each time-point as multiple tests were made. We will accomplish this by grouping and summarizing the data set. Use the code below to prepare the data before statistical modeling.\n\ndata(\"strengthvolume\")\n\nisom_data &lt;- strengthvolume %&gt;% \n  \n  filter(time %in% c(\"pre\", \"post\"), \n         exercise == \"isok.60\") %&gt;%\n  inner_join(legs) %&gt;%\n  filter(leg == selected.leg) %&gt;%\n  group_by(participant, sex, time, sets) %&gt;%\n  summarise(load = max(load, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = time, \n                    values_from = load) %&gt;%\n          mutate(change = post - pre) %&gt;%\n  print()\n\n# A tibble: 34 × 6\n# Groups:   participant, sex [34]\n   participant sex    sets      post   pre change\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 FP11        male   multiple   230   214     16\n 2 FP12        female single     235   213     22\n 3 FP13        male   multiple   281   215     66\n 4 FP14        female multiple   180   151     29\n 5 FP15        male   single     258   256      2\n 6 FP16        female single     135   121     14\n 7 FP17        male   multiple   235   221     14\n 8 FP19        male   multiple   243   233     10\n 9 FP2         female multiple   144   132     12\n10 FP20        female multiple   153    99     54\n# ℹ 24 more rows\n\n\nSince the ANCOVA model is a multiple regression model we can add more variables that will help explain our results. In the randomization of our version of the study females and males were slightly unbalanced between volume groups. This is a factor that we might want to adjust for. We are interested in the differences between training conditions regardless of sex-differences.\nUsing the data set that we created above, create two models of post-treatment values, one without the added effect of sex and one with the sex added to the model to adjsut the scores.\nExample code\n\n\n# Without the effect of male/female\nm1 &lt;- lm(post ~ pre + sets, data = isom_data)\n\n# With the effect of male/female\nm2 &lt;- lm(post ~ sex + pre + sets, data = isom_data)\n\nsummary(m1)\nsummary(m2)\n\nThe un-adjusted model suggests that the difference between volume conditions are -12.1 but in the adjusted case, the difference is estimated to -9.8 units. Although both models suggest a small effects, failing to control for other factors could potentially bias your conclusions. This example shows how additional adjustment can be made to the very flexible regression model."
  },
  {
    "objectID": "22-analyzing-trials.html#references",
    "href": "22-analyzing-trials.html#references",
    "title": "19  Pre- to post-treatment analysis",
    "section": "19.4 References",
    "text": "19.4 References\n\n\n\n\nSenn, S. 2006. “Change from Baseline and Analysis of Covariance Revisited.” Journal Article. Stat Med 25 (24): 4334–44. https://doi.org/10.1002/sim.2682.\n\n\nVickers, Andrew J., and Douglas G. Altman. 2001. “Analysing Controlled Trials with Baseline and Follow up Measurements.” Journal Article. BMJ : British Medical Journal 323 (7321): 1123–24. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1121605/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1121605/pdf/1123.pdf."
  },
  {
    "objectID": "22-analyzing-trials.html#footnotes",
    "href": "22-analyzing-trials.html#footnotes",
    "title": "19  Pre- to post-treatment analysis",
    "section": "",
    "text": "This tests is made here for illustrating the point that it usually have a relationship with the change score (or follow-up score). This test should not be done to determine if the ANCOVA model should be used or not. Since adding the baseline covariate does not make things worse we should consider it regardless(Senn 2006; Vickers and Altman 2001)↩︎\nThis implies that a Welch t-test could improve the situation (var.equal = FALSE) and that heteroscedasticity in the linear model could be accounted for by variance components in a generalized least square model, gls from the nlme package)↩︎\nStronger evidence in the sense that we will generally have smaller p-values↩︎"
  },
  {
    "objectID": "23-visualizing-mixed-models.html#a-multiple-time-point-study",
    "href": "23-visualizing-mixed-models.html#a-multiple-time-point-study",
    "title": "20  Mixed models - Interpretation and visualization",
    "section": "20.1 A multiple time-point study",
    "text": "20.1 A multiple time-point study\nIn previous lessons where we used ANCOVA models to account for the baseline we were able to get away with a simple linear regression model. This because we modeled the change- or post-scores, we did not get any correlated errors (not multiple data points from each participant). The tenthirty data set contains three time-points per participant, it is thus not straight forward to put all this data in one model as the data are correlated. As we saw in a previous lesson, not accounting for the correlation in repeated measures studies could affect statistical power.\nIn this lesson we will model strength data with a linear mixed model (LMM). We will interpret the output and make plots of the model estimates.\n\n20.1.1 An hypothetical example\nA theoretical outcome of the study in this example could look something like in Figure 1 below.\n\n\n\n\n\n\n\n\n\nThe 30RM group starts off at approximately the same level as the 10RM group. After the first period (from Pre to Mid) the 10RM group potentially increases strength more than the 30RM group, the difference could be maintained to the Post time-point.\nInstead of modeling the effect of the baseline on the outcome we could control for it by adjusting the average levels at Mid and Post by removing the differences seen at baseline. This is the basic approach when analyzing repeated measures deigns with mixed models. Before we get to this step we will build the model from the bottom up.\nIn a first model we do not include any “fixed effects”. We only estimate the mean of the whole data set, the very basic model formulation could look some thing like:\n\\[y = \\beta_0\\] In R, we would write this model formula as\n\nstrength ~ 1\n\ngiven that the dependent variable (y) is a variable we call strength. The number 1 indicates that we only include an intercept in the model. Thus, the data are explained with an intercept only. In the figure below, this model is represented by the black cross.\nWe could add information about the time-points to the model, still leaving out information regarding grouping. This model formulation could look something like this:\n\\[y = \\beta_0 + \\beta_{Mid} + \\beta_{Post}\\] Where \\(\\beta_{Mid}\\) is a dummy variable assigned number 1 when time = “mid” otherwise 0, and \\(\\beta_{Post}\\) is a dummy variable assigned 1 when time = “post” otherwise 0. This model, given our hypothetical data in this example could look like the black line in the plot below.\nIn R, we would write this model formula as\n\nstrength ~ time\n\nR automatically converts the time variable to two dummy variables as described in the formula above.\n\n\n\n\n\nFigure 2. Hypothetical model of a study strength development in two groups. The black cross represents a model of the data only modelling the mean, the black line represents a model modelling the average change over time in the two groups. Hypothetical values per group and time are represented in colored lines\n\n\n\n\nTo account for the difference between groups at baseline we could add an estimate of the effect of baseline differences. In a simplified description this could look like:\n\\[y = \\beta_0 + \\beta_{Group} + \\beta_{Mid} + \\beta_{Post}\\] Where the additional \\(\\beta_{Group}\\) is a dummy variable assigning 1 to all observations in the 30RM group and 0 otherwise. In R we extend the model formulation simply by adding group:\nIn R, we would write this model formula as\n\nstrength ~ group + time \n\nEstimates from the new model will have one “line” per group representing the average difference between groups at baseline but not accounting for differences in changes over time per group. The two black dashed lines in Figure 3 represents this model.\n\n\n\n\n\nFigure 3. Hypothetical model of a study strength development in two groups. The black cross represents a model of the data only modelling the mean, the solid black line represents a model modelling the average change over time in the two groups. The dashed black lines represents the average change over time with baseline difference between groups at baseline accounted for. Hypothetical values per group and time are represented in colored lines\n\n\n\n\nNotice that the addition of the group term creates two parallel lines representing each group. To fully capture the average changes per group we need to let the model account for different changes per time in each group. We do this by adding an “interaction term”. The interaction term allows for each group to have different values at “Mid” and “Post”.\nA simplified model formulation could look like this:\n\\[y = \\beta_0 + \\beta_{Group} + \\beta_{Mid} + \\beta_{Post} + \\beta_{Mid:Group} + \\beta_{Post:Group}\\]\nIn R, we would write this model formula as\n\nstrength ~ group + time + group:time\n\nWe may also simplify the above formula like this:\n\nstrength ~ group * time \n\nThe output from this model given our hypothetical data could look like in table 1.\n\n\n\nTable 1. Regression coefficients from an hypothetical data set.\n\n\nCoefficient\nEstimate\n\n\n\n\nIntercept\n0.45\n\n\nTimeMid\n0.15\n\n\nTimePost\n0.20\n\n\nGroup10RM\n0.05\n\n\nTimeMid:Group10RM\n0.15\n\n\nTimePost:Group10RM\n0.20\n\n\n\n\n\nThe 30RM group is the reference group so the intercept estimate is the mean in this group at the Pre time point. The 30RM group increases by 0.15 units from Pre to Mid and 0.20 units from Pre to Post. At Pre, the 10RM group is 0.05 units above the 30RM group. Compared to the change seen in the 30RM group, the 10RM group increases an additional 0.15 points Pre to Mid and an additional 0.20 units from Pre to Post.\nTo calculate the average in the 30RM group at Mid we can combine estimates from the regression table (above) according to our model formula. Keeping only the terms needed\n\\[y = \\beta_0 + \\beta_{Mid} \\] \\[y = 0.45 + 0.15\\]\n\\[y = 0.6\\] The same principles apply if we want to know the estimated mean in the 10RM group at Post, keeping the relevant terms:\n\\[y = \\beta_0 + \\beta_{Group}  + \\beta_{Post} + \\beta_{Post:Group}\\] \\[y=0.45 + 0.05 + 0.2 + 0.2\\] \\[y=0.9\\] By adding up the terms in a linear fashion (hence linear regression) we can calculate the estimated averages from both groups.\nThe final model allows for different changes over time per group. And different regression estimates will reflect these patterns (Figure 4).\n\n\n\n\n\nFigure 4. Hypothetical model of a study on strength development in two groups. The dashed colored lines represents the average change over time per group with baseline difference between groups at baseline accounted for. Regression terms are printed to represent a full interaction model describing the data.\n\n\n\n\n\n\n20.1.2 Inference from a repeated measures study\nIn the scenario described above the most meaningful comparison between groups are the differences in change over time between groups. This is because our design lets us do this comparison. A comparison that is much more uncertain in a study like this is the within-group changes over time. This is because we do not have a control condition for comparison. Imagine that in our example above, the two groups are both training, comparing the two groups means that we are comparing two different training protocols. We might be tempted to say that the average increase in strength is due to the training, however, strictly speaking this problematic as we do not have a group of randomly assigned individuals who have performed all tests but not performed the training. The effect we see on strength on average could be an effect of learning the strength test. We can not isolate the training effect from other effects when we do not have a control group!\nIn comparing the two training protocols we do have a control as we are comparing the two groups to each other.\n\n20.1.2.1 Evaluating regression coefficients\nIn the hypothetical example above the two coefficients that are of interest are the two interaction estimates. Let’s evaluate their confidence intervals. Table 2 gives hypothetical 95% confidence intervals on the interaction terms.\n\n\n\nTable 2. Interaction coefficients from an hypothetical data set.\n\n\nCoefficient\nEstimate\n95% CI\n\n\n\n\nTimeMid:Group10RM\n0.15\n-0.05, 0.35\n\n\nTimePost:Group10RM\n0.20\n0.05, 0.35\n\n\n\n\n\nThe interpretation could be that the 10RM group increases strength more than the 30RM group, the 95% CI at time-point Mid indicate that no difference between groups are within the probable values (95% CI) indicating true effect might here might be no difference. At time-point post, our 95% CI indicates that we do not have the zore-effect among our probable values."
  },
  {
    "objectID": "23-visualizing-mixed-models.html#using-real-data",
    "href": "23-visualizing-mixed-models.html#using-real-data",
    "title": "20  Mixed models - Interpretation and visualization",
    "section": "20.2 Using real data",
    "text": "20.2 Using real data\nWe start by loading the data and packages. Notice that we are loading the lme4 package that we will use to fit mixed models. We also load the emmeans package that we will use to get estimates from each model. For this example we will work with leg-press data.\n\nlibrary(tidyverse); library(exscidata); library(lme4); library(emmeans); library(pbkrtest)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\ndata(\"tenthirty\")\n\ndat &lt;- tenthirty %&gt;%\n  filter(exercise == \"legpress\") %&gt;%\n    # Fix the time factor in the right order for plotting \n  # and fix the grouping so that the 30RM group comes first (comparing to 30RM)\n  mutate(time = factor(time, levels = c(\"pre\", \"mid\", \"post\")), \n         group = factor(group,  levels = c(\"RM30\", \"RM10\")))\n\n\n20.2.1 Data exploration\nIt is always a good idea to plot the data to get a sense of what you will find in any subsequent models. Let’s build a plot that shows changes in strength for each participant in each group.\n\ndat %&gt;%\n  # build the plot, each participant (subject) will get a line, group = subject\n  # will take care of this. Each group will get it's own color, color = group\n  # will take care of that.\n  ggplot(aes(time, load, group = participant, color = group)) + geom_line() +\n  theme_minimal()\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWe can see that most participants increase their strength, it is not clear if one group increases more than the other. We can add box-plots on top to get an idea.\n\ndat %&gt;%\n  # build the plot, each participant (subject) will get a line, group = subject\n  # will take care of this. Each group will get it's own color, color = group\n  # will take care of that.\n  ggplot(aes(time, load, group = participant, color = group)) + \n  geom_line() +\n  # remove grouping from boxplots to plot group averages\n  geom_boxplot(aes(group = NULL), width = 0.5) +\n  theme_minimal()\n\nWarning: Removed 3 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThe graphics suggests a small effect in favor of 10RM at post. Let’s fit the model.\n\n\n20.2.2 Fit a mixed model\nThe model will be fairly simple. We want to compare the two groups an model any differences in change over time. This means that the model must contain the time and group variable as well as their interaction giving each group a chance to “vary on their own”. The beauty of the mixed model is that we can account for repeated data just by using the subject indicator a random efect. The data in this example allows for the inclusion of a random intercept, meaning that each particiapnt will be compared from their own strating point (intercept).\n\n# The lmer function from the lme4 package.\nm1 &lt;- lmer(load ~ time * group + (1|participant), data = dat)\n\nBefore doing anything with this model we will have to check if our assumptions are met. Let’s do a residuals plot.\n\nplot(m1)\n\n\n\n\nThe residual plot indicates that there are no obvious patterns in the data, the residual variation is similar over the range of fitted values. The model is OK.\nWe can now check if the model behaves similar to what we expect when looking at the raw data. We will use the emmeans package to get model estimates, estimated averages at each time-point within each group. The data may be plotted. emmeans performs the calculations based on the fixed effects estimates as we did by hand above.\n\n# Using the model (m1) we can specify the emmeans function to get model estimates\n\n# The specs argument specifies what means to calculate from the model.\n# The code below takes care of the full model (including interactions).\nest &lt;- emmeans(m1, specs = ~ time|group)\n\nTo use an object created with emmeans in plotting we need to convert it to a data frame. It is then straight forward to plot.\n\nest %&gt;%\n  data.frame() %&gt;%\n  ggplot(aes(time, emmean, group = group, color = group)) + geom_line() +\n  theme_minimal()\n\n\n\n\nIn the code creating the plot above I specified the group and color arguments in ggplot to equal group in our data set. This means that lines will be drawn with different colors connecting data from each group.\nWe can see in the figure that the 10RM group on average seems to increase more from pre to post than the 30RM group.\nWe could add error-bars to this plot that represents the 95% confidence interval of each estimate. I will tweak the plot further to avoid over-plotting and add points to indicate where estimates have been made.\n\nest %&gt;%\n  data.frame() %&gt;%\n\n  ggplot(aes(time, emmean, group = group, color = group)) + \n  \n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), \n                position = position_dodge(width = 0.2), \n                width = 0.1) +\n  geom_line(position = position_dodge(width = 0.2)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  \n  theme_minimal()\n\n\n\n\nThe position = position_dodge(width = 0.2) argument inside geom_errorbar(), geom_point() and geom_line() moves the points, lines and error-bars away from each other based on the grouping (group = group in aes()). The width argument in geom_errobar sets the width of the error-bars. There is nothing magical with these numbers, I just think it looks nice.\nThe confidence bounds around each measurement does not really answer our question of differences between groups in change over time. These are just the 95% CI around the estimated means for each group at each time-point. Some would say that they are so non-informative that they should not be presented. I think it is good practice to plot what the model estimates, this gives you an opportunity to assess if the model actually captures the data! We get standard errors and 95% confidence intervals from model estimates and the 95% CI are preferred for plotting.\n\n\n20.2.3 Do the model capture the data?\nLets combine raw data and model estimates in the same plot! We can combine several data sources in ggplots like described in the code below. We will simply extend the last figure and include raw data. By including a separate data argument in a geom_ we can overlay these data on the same plot.\n\nest %&gt;%\n  data.frame() %&gt;%\n\n  ggplot(aes(time, emmean, group = group, color = group) ) + \n  \n  # Adds raw data\n  geom_line(data = dat, aes(time, load, group = participant, color = group), \n            # Add transparency to individual lines\n            alpha = 0.4) +\n  \n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), \n                position = position_dodge(width = 0.2), \n                width = 0.1) +\n  geom_line(position = position_dodge(width = 0.2)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  theme_minimal()\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\nUsing alpha = 0.4 we make the lines representing individual data a bit more transparent. Nice! We now have a plot showing the raw data and the model estimates. They seem to capture the data pretty well (we have also seen the residual plot indicating this).\n\n\n20.2.4 Inference about the effect of interest\nWe have still not answered our question regarding the effect of training with different intensity. To do this we will need to get the estimates of the interaction coefficients from the model. To keep up with our very visual way of looking at the data we could plot the confidence intervals from these effects. Using the confint() function we will get the 95% CI from each model coefficient.\nWe need to build a data frame out of this information and the coefficients from the summary() output.\n# Save the confidence intervals\nconf_intervals &lt;- confint(m1)\nComputing profile confidence intervals ...\n# Save the regression coefficients\ncoefs &lt;- summary(m1)$coef\n\n# Using cbind (column bind) to combine the two data frames\ncoef_summary &lt;- cbind(coefs, data.frame(conf_intervals)[3:8, ]) \n\n# Print the table in a pretty format\n\ncoef_summary %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nX2.5..\nX97.5..\n\n\n\n\n(Intercept)\n281.53846\n20.120706\n13.9924743\n242.37410\n320.70282\n\n\ntimemid\n35.00000\n8.968506\n3.9025454\n17.80241\n52.19759\n\n\ntimepost\n38.86796\n9.550460\n4.0697470\n20.53927\n57.16481\n\n\ngroupRM10\n-7.60989\n27.942231\n-0.2723437\n-61.99862\n46.77884\n\n\ntimemid:groupRM10\n11.07143\n12.454834\n0.8889262\n-12.81138\n34.95423\n\n\ntimepost:groupRM10\n38.57427\n13.043542\n2.9573464\n13.57176\n63.59255\n\n\n\n\nX2.5.. and X97.5.. represents the lower and upper confidence interval for each estimate. The estimate that we are interested in are timemid:groupRM10and timepost:groupRM10. We can now reduce the data frame and plot it to show estimated differences between groups at time-points “mid” and “post”.\nWe will make a plot mirroring the plot showing raw values and model estimates. We need all time-points to be represented but only “mid” and “post” to contain data.\n\ncoef_summary %&gt;%\n  mutate(coef = rownames(.)) %&gt;%\n  # Filter only the interaction variables\n  filter(coef %in% c(\"groupRM10\", \"timemid:groupRM10\", \"timepost:groupRM10\")) %&gt;%\n  # Make a \"timepoint\" variable to represent the \"dat\" data set.\n  mutate(time = gsub(\"time\", \"\", coef), \n         time = gsub(\":groupRM10\", \"\", time), \n         time = if_else(time == \"groupRM10\", \"pre\", time)) %&gt;%\n  # Fix order of the time variable\n  mutate(time = factor(time, levels = c(\"pre\", \"mid\", \"post\"))) %&gt;%\n  \n  # Create the plot\n  ggplot(aes(time, Estimate)) + \n  \n  \n  # Add a line indicating zero geom_hline (horizontal line)\n  geom_hline(yintercept = 0, lty = 2) +\n  \n  geom_errorbar(aes(ymin = X2.5.., ymax = X97.5..), width = 0.1) +\n  geom_point(shape = 24, size = 3, fill = \"white\") +\n  theme_minimal()\n\n\n\n\nThe plot shows the differences between groups at time-points “mid” and “post” after accounting for baseline differences (“pre”). This is the answer to our question regarding the differences between training intensity, 10RM seems to be superior!\nWe can combine the two plots. The package cowplot has functions to plot several ggplots into one single plot.\nFirst we save each plot and the combine them using plot_grid. I will do some modifications to make the plot a bit nicer.\n\n## Figure 1: Estimated means and raw data\n\nfigA &lt;- est %&gt;%\n  data.frame() %&gt;%\n  \n  ggplot(aes(time, emmean, group = group, color = group) ) + \n  \n  # Adds raw data\n  geom_line(data = dat, aes(time, load, group = participant, color = group), \n            # Add transparency to individual lines\n            alpha = 0.4) +\n  \n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), \n                position = position_dodge(width = 0.2), \n                width = 0.1) +\n  geom_line(position = position_dodge(width = 0.2)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  theme_minimal() +\n  \n  # Changing axis titles and title in the legend\n  labs(y = \"Legpress 1RM load (kg)\", \n       color = \"Intensity\") + \n  \n  # Removing text and indicators on the x-axis as these can be represented in the \n  # other plot\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank())\n  \n# Figure B: Differences between groups (interaction terms with 95% CI)\n\nfigB &lt;- coef_summary %&gt;%\n  mutate(coef = rownames(.)) %&gt;%\n  # Filter only the interaction variables\n  filter(coef %in% c(\"groupRM10\", \"timemid:groupRM10\", \"timepost:groupRM10\")) %&gt;%\n  # Make a \"timepoint\" variable to represent the \"dat\" data set.\n  mutate(time = gsub(\"time\", \"\", coef), \n         time = gsub(\":groupRM10\", \"\", time), \n         time = if_else(time == \"groupRM10\", \"pre\", time)) %&gt;%\n  # Fix order of the time variable\n  mutate(time = factor(time, levels = c(\"pre\", \"mid\", \"post\"))) %&gt;%\n  \n  # Create the plot\n  ggplot(aes(time, Estimate)) + \n  \n  \n  # Add a line indicating zero geom_hline (horizontal line)\n  geom_hline(yintercept = 0, lty = 2) +\n  \n  geom_errorbar(aes(ymin = X2.5.., ymax = X97.5..), width = 0.1) +\n  geom_point(shape = 24, size = 3, fill = \"white\") +\n  theme_minimal() + \n  \n    # Changing axis titles and title in the legend\n  labs(y = \"Average group differences\\n(kg, 95% CI)\", \n       color = \"Intensity\") \n  \n# Using cowplot to plot both figures in 1\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\n# Plot grid using figA and figB\nplot_grid(figA, figB, ncol = 1, \n          # Align vertically so that the x axis matches\n          align = \"v\", \n          # Set alignment so that axis align both on the left (l) and right (r)\n          # axis\n          axis = \"lr\")\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n# Learn more about aligning:\n# https://wilkelab.org/cowplot/articles/aligning_plots.html\n\nSuccess! We no have a two part figure showing model estimates and estimated mean differences between training conditions.\n\n\n20.2.5 More about the model summary\nUsing summary(m1) we can get more information about the model we have fitted.\n\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: load ~ time * group + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 752.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.77740 -0.40616 -0.04183  0.32323  2.07515 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 4740.1   68.85   \n Residual                 522.8   22.87   \nNumber of obs: 78, groups:  participant, 27\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         281.538     20.121  13.992\ntimemid              35.000      8.969   3.903\ntimepost             38.868      9.550   4.070\ngroupRM10            -7.610     27.942  -0.272\ntimemid:groupRM10    11.071     12.455   0.889\ntimepost:groupRM10   38.574     13.044   2.957\n\nCorrelation of Fixed Effects:\n            (Intr) timemd timpst grRM10 tmm:RM10\ntimemid     -0.223                              \ntimepost    -0.209  0.470                       \ngroupRM10   -0.720  0.160  0.151                \ntmmd:grRM10  0.160 -0.720 -0.338 -0.223         \ntmpst:gRM10  0.153 -0.344 -0.732 -0.213  0.477  \n\n\nLet’s first examine the random effects. In this model they are represented by a random intercept meaning that each participant has their own intercept in the model. The Std.Dev. (or standard deviation) in the random effects table indicates the mean variation between participants intercept. They vary at average 68.85 kg from the mean of each group.\nThe Residual indicates the variation that is not accounted for in the model by either random nor fixed effects.\nThe fixed effects are the model estimates of group and time effects in the model. This what we use to draw conclusions about their effects. Notice that we do not get any p-values. These have been left out of the lmer function.\nNext table contains “approximate correlation of the estimator of the fixed effects”1 (??). They are subject to more advanced discussions.\nWe have already seen that confint function can be used to get confidence intervals.\n\n20.2.5.1 I neeeeed p-values\nThere are several options (see ?lme4::pvalues). A nice intuitive option is the removal of the term you are interested in and model comparison.\nLet’s say that we are interested in the interaction between group and time. If this interaction is important, adding this information to the model would increase it’s ability to explain the data. We can actually test if this is true and in process get a p-value. Technically we will do a likelihood-ratio test. This tests compares two models and determines which one fits the data best.\n\n# Fit a model not containing the interaction\nm0 &lt;- lmer(load ~ time + group + (1|participant), data = dat)\n\n\n# Fit a model containing the interaction term\nm1 &lt;- lmer(load ~ time * group + (1|participant), data = dat)\n\n# Compare the models using the anova function\n\nanova(m0, m1)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: dat\nModels:\nm0: load ~ time + group + (1 | participant)\nm1: load ~ time * group + (1 | participant)\n   npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)  \nm0    6 813.37 827.51 -400.68   801.37                      \nm1    8 808.33 827.18 -396.16   792.33 9.039  2    0.01089 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output gives you the model formulation of each model being compared. The AIC is a measure of goodness of fit a lower value is better. The logLik is the value tested in a Chi2 test and the p-value indicates if the models differ more than would be expected due to sampling error. Above, the comparison of models indicate that it is quite unlikely to see this difference if the models explained the data similarly. Thus, adding the interaction increases model m1s ability to explain the data, there is evidence for an interaction effect in the data."
  },
  {
    "objectID": "23-visualizing-mixed-models.html#summary",
    "href": "23-visualizing-mixed-models.html#summary",
    "title": "20  Mixed models - Interpretation and visualization",
    "section": "20.3 Summary",
    "text": "20.3 Summary\nMixed models are very useful for fitting data containing correlated data. This e.g. is data where multiple data points are gathered from the same individuals such as in repeated measures designs. Mixed models can be extended in many ways to capture even more complicated data sets. Plotting the output of models are very useful to get a feeling about the model performace and what they actually tell you. Confidence intervals can be used for inference. If p-values are needed it is suggested that you use liklihood ratio tests to compare model."
  },
  {
    "objectID": "23-visualizing-mixed-models.html#footnotes",
    "href": "23-visualizing-mixed-models.html#footnotes",
    "title": "20  Mixed models - Interpretation and visualization",
    "section": "",
    "text": "https://stat.ethz.ch/pipermail/r-sig-mixed-models/2009q1/001941.html↩︎"
  },
  {
    "objectID": "24-mixed-models.html#accounting-for-the-baseline-differences-between-group-and-mixed-models",
    "href": "24-mixed-models.html#accounting-for-the-baseline-differences-between-group-and-mixed-models",
    "title": "21  Mixed models - Analyzing repeated measures designs",
    "section": "21.1 Accounting for the baseline – Differences between group and mixed models",
    "text": "21.1 Accounting for the baseline – Differences between group and mixed models\nIn a randomized controlled trial, there might be difference between groups at baseline just by chance. These differences can be controlled for using the ANCOVA model, as described in the previous chapter. Another way to account for any baseline differences is to analyze raw scores (not change-scores) and account for potential baseline differences in the model. This can be done using another “extension” of the regression model, the mixed effects model."
  },
  {
    "objectID": "24-mixed-models.html#statistical-name-dropping",
    "href": "24-mixed-models.html#statistical-name-dropping",
    "title": "21  Mixed models - Analyzing repeated measures designs",
    "section": "21.2 Statistical name dropping",
    "text": "21.2 Statistical name dropping\nHere we will briefly continue to talk about mixed effects models (or linear mixed models, or hierarchical models) which are models for continuous outcomes with normally distributed errors. These models can account for non-independence between data-points, meaning that we can fit a model using correlated data. This is advantageous when we want to analyze the same participants in a time-course manner (repeated measures design). Traditionally in exercise science, this has been done using repeated measures analysis of variance (repeated measures ANOVA). One might say that this is an outdated technique as the modern mixed effects model is more flexible and robust as it allows for e.g. unbalanced data (e.g. different number of participants in each group), missing data and more complex model formulations.\nThe mixed effects model can be extended to other problems under the framework of generalized linear models that brings further flexibility as data from different distributions can be modeled."
  },
  {
    "objectID": "24-mixed-models.html#the-model",
    "href": "24-mixed-models.html#the-model",
    "title": "21  Mixed models - Analyzing repeated measures designs",
    "section": "21.3 The model",
    "text": "21.3 The model\nA mixed model contains two kinds of effects. In our previous models (made with lm()), we have dealt with “fixed effects”, these are the population-level effects that we try to estimate. This can be the difference between groups, or the slope in a model where we try to explain VO2max with height. In the mixed model, we also include “random effects”. In the simple case we can think of these as a separate starting point in the model for each participant. This simple case is called a model with random intercepts. Why random? This is because we can think of these effects as sampled from a population of possible effects. A fixed effect on the other hand have fixed (population) values. In the model we will create first, we will estimate population averages over time-points and training conditions. These estimates are fixed by design in the study, but participants has been sampled at random.\nWe will use the function lmer() from the package lme4 to fit mixed effects models.\nHold up! Why use this new stuff, can we not just use the lm function?\nLet’s try. Before we do, we should agree that when fitting correlated data (data from the same participants sampled multiple times therefore creating data points “related” to each other) we violate an assumption of the ordinary linear model, the assumption of independence.\nIn this example we will use the tenthirty data set. Let’s start by fitting a model where we try to estimate the difference between groups over time using lm().\n\nlibrary(tidyverse); library(exscidata)\n\ndata(\"tenthirty\")\n\n\ntenthirty_reduced &lt;- tenthirty %&gt;% \n  filter(exercise == \"legpress\", \n         time %in% c(\"pre\", \"post\")) %&gt;%\n  # fix the order of the time factor\n  mutate(time = factor(time, levels = c(\"pre\", \"post\"))) %&gt;%\n  print()\n\n\n# Fit the model\n\nlm1 &lt;- lm(load ~ time * group, data = tenthirty_reduced)\n\nsummary(lm1)\n\nThe model formulation estimates four coefficients. The intercept is the mean in the 10RM group at baseline (pre). timepost is the mean in the 10RM group at time-point post. groupRM30 is the difference at baseline between groups. The model formulation contains an interaction, meaning that the two groups are allowed to change differently between time-points and timepost:group30RM is the difference between groups at time-point post when taking the difference at baseline into account. This is the coefficient of interest in this study. We want to know if the difference from pre- to post-training differs between groups, we can assess this by testing if the difference is smaller or greater than zero.\nWe can see that we estimate the difference in the interaction term (timepost:group30RM) to about -40 units. However, this difference is associated with a lot of uncertainty due to the large standard error. The resulting t-value is small, indicating that this result would be commonly found under the null-hypothesis of no differences between groups.\nNow let’s try to fit a mixed effects model.\n\n# Load package lme4\nlibrary(lme4)\n\n# Fit the model\nlmer1 &lt;- lmer(load ~ time * group + (1|participant), data = tenthirty_reduced)\n\nsummary(lmer1)\n\nFrom the fixed effects part of the summary, we can read about the same coefficients as were estimated in the above model. However, the estimates differs from the ordinary linear model especially when it comes to the standard errors t-values. The smaller standard errors is a result of telling the model that the same individuals are measured at both time-points. A lot of the unexplained variation in the ordinary linear model is explained by the difference between participants at baseline. We have added this information to the model through its random effects part. In the lmer function specifying a random intercept per participant is done through the addition of (1|participant). We can see from the model summary (under Random effects) that the estimated standard deviation of participants intercepts at baseline is about 68 units. The random effect has a mean of zero and some estimated standard deviation, in this case the average deviation from zero is 68.\nNotice also that lme4 does not produce p-values1. Instead of p-values we can use confidence intervals to evaluate our fixed effects. These are accessed with the confint function in both lm and lmer.\n\n# Confidence intervals from lm\nconfint(lm1)\n\n# Confidence intervals from lmer\nconfint(lmer1)\n\nWhat is your interpretation of the results from these two models?\nIn this comparison we have seen that the mixed effects model is more powerful by accounting for correlated data.\n\n21.3.1 Exercise: Effect of training volume on isokinetic strength\nThe complete strengthvolume data set contains data from a within-participant trial where participants performed two training protocols during the study. Each leg was randomly allocated to either low- or moderate-volume training (see (Hammarström et al. 2020)). The trial can be analyzed as a cross-over trial using a mixed-model. To account for the fact that each participant has performed both training protocols we will add a random intercept per participant.\nFit a mixed effects model to isokinetic strength (isok.60) data from pre and post training (strengthvolume data set in exscidata).\nAfter fitting the model, use plot(model) to assess the residual plot. This plot should not contain any clear patterns.\nIf the residual plot is considered ok, interpret the model parameters and estimates.\nExample code\n\n\n# Load packages\nlibrary(lme4); library(tidyverse); library(exscidata)\n\nLoading required package: Matrix\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the data\ndata(\"strengthvolume\")\n\n# Filter and store a data set\ndat &lt;- strengthvolume %&gt;% \n  # Filter data set only to include...\n  filter(exercise == \"isok.60\",  # Isokinetic data\n         time %in% c(\"pre\", \"post\")) %&gt;% # time points pre and post\n  # fix the order of time factor\n  # fix the order of the volume condidition factor\n  mutate(time = factor(time, levels = c(\"pre\", \"post\")), \n         sets = factor(sets, levels = c(\"single\", \"multiple\")))\n\n\n\n# Fit the model\nm2 &lt;- lmer(load ~ time * sets + (1|participant), data = dat)\n\n\n# Check the model assumption of homoscedasticity (equally distributed residuals)\nplot(m2)\n\n\n\n\nResiduals vs. fitted values\n\n\n\n# Check the estimates from the model\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: load ~ time * sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 1411.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.72030 -0.52152 -0.07838  0.52534  2.29654 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 2507.3   50.07   \n Residual                 352.1   18.76   \nNumber of obs: 150, groups:  participant, 39\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           182.4615     8.5625  21.309\ntimepost               14.1562     4.3770   3.234\nsetsmultiple            0.4359     4.2490   0.103\ntimepost:setsmultiple  13.7585     6.1330   2.243\n\nCorrelation of Fixed Effects:\n            (Intr) timpst stsmlt\ntimepost    -0.241              \nsetsmultipl -0.248  0.485       \ntmpst:stsml  0.172 -0.701 -0.693\n\nconfint(m2)\n\nComputing profile confidence intervals ...\n\n\n                           2.5 %     97.5 %\n.sig01                 39.795034  63.270318\n.sigma                 16.316258  21.238845\n(Intercept)           165.548599 199.374478\ntimepost                5.611650  22.684934\nsetsmultiple           -7.850492   8.722287\ntimepost:setsmultiple   1.798174  25.718920\n\n\n\n21.3.1.1 Interpreting the model\nThe intercept is the mean in the reference group (single-set) at time pre. Since we changed the levels of both the time and sets factors we determined the reference levels for both factors. The second coefficient, timepost tells us the average change from preto post in the reference sets condition. setsmultiple is the difference at baseline between conditions and timepost:setsmultiple is the difference between single-set and multiple-set at time-point post, this is the coefficient of interest in this model (and study). We want to know if the change from pre to post differs between groups, we can assess this by testing if the difference is smaller or greater than zero. This is done with the difference at baseline in mind (we control for the baseline).\nUsing the confint function we can see that the interaction term (timepost:setsmultiple) does not contain zero. How do you interpret the results?\n\n\n\n\n\n21.3.2 Multiple time-point – Mixed models\nThe mixed model can be extended using multiple time-points. When treating time-points as a factor variable we will estimate differences between volume condition at each time-point after taking the baseline into account. We do not anticipate any differences between time-points pre and session1 nor between volume-conditions at these times, lets see if we are correct. Fit a model containing data from isok.60 with time-points pre, session1 and post, and both volume conditions. Keep the interaction in the model with the formula: load ~ time * sets (1|participant). Intepret the results and calculate the average of each time and sets combination.\nAn interpretation\n\n\ndat &lt;- strengthvolume %&gt;% \n  filter(exercise == \"isok.60\") %&gt;%\n  # fix the order of time-point and volume factors\n  mutate(time = factor(time, levels = c(\"pre\", \"session1\", \"post\")), \n         sets = factor(sets, levels = c(\"single\", \"multiple\"))) \n\n\n\n# The model\nm &lt;- lmer(load ~ time * sets + (1|participant), data = dat)\n\n# Inspect the model residuals\nplot(m) # Tendency of larger variation at high fitted values, but ok for now.\n\n\n\n# See the summary of the model\nsummary(m)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: load ~ time * sets + (1 | participant)\n   Data: dat\n\nREML criterion at convergence: 2079.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1657 -0.4663 -0.0762  0.5425  3.0151 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n participant (Intercept) 2434.0   49.34   \n Residual                 323.2   17.98   \nNumber of obs: 228, groups:  participant, 39\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)               182.4615     8.4082  21.700\ntimesession1               -2.8205     4.0715  -0.693\ntimepost                   14.1831     4.1755   3.397\nsetsmultiple                0.4359     4.0715   0.107\ntimesession1:setsmultiple   1.4359     5.7579   0.249\ntimepost:setsmultiple      13.7585     5.8767   2.341\n\nCorrelation of Fixed Effects:\n            (Intr) tmsss1 timpst stsmlt tmss1:\ntimesessin1 -0.242                            \ntimepost    -0.236  0.488                     \nsetsmultipl -0.242  0.500  0.488              \ntmsssn1:sts  0.171 -0.707 -0.345 -0.707       \ntmpst:stsml  0.168 -0.346 -0.704 -0.693  0.490\n\n\nThe mean in the single-set condition at pre is 182.46 Nm. From pre to session1 the single-set condition increases by -2.82 Nm, from pre to post the single-set condition increases by 14.18 Nm. At pre there is a 0.44 unit difference between conditions Given this difference, at time-point session1 the multiple-set condition is 1.44 units lower than the single-sets condition, and at time-point post the multiple-set condition is 13.76 Nm lower than the single-set condition given a baseline difference and an increase in the single-set condition.\nBy adding these terms we can get the estimated mean for each group at post:\nsingle-set at post = (Intercept) + timetpost\nmultiple-set at post = (Intercept) + timepost + groupmultiple + timepost:groupmultiple\n\n\nSince we have a data set with strength assessments performed at multiple time-points it could be valuable to model the strength over time instead at specific time-points. We will use the isok.60 data again but reduce the data set to only containing data from participants wit a complete set of measurements. A number of participants did not complete all strength assessments (at weeks 2, 5, and 9) and are therefore filtered out from this analysis. We will filter out participants that do not have a full set of strength measurements (12, six assessments per leg). First we will model the time as a factor variable, resulting in a model with many coefficients. Run the code and inspect the resulting data frame to check if it is what you expect.\n\ndat &lt;- strengthvolume %&gt;% \n  # Filter to include isokinetic tests of included participants without missing values\n  filter(exercise == \"isok.60\",\n         !is.na(load)) %&gt;%\n  # Group the data frame by participants\n    group_by(participant) %&gt;%\n  # Filter to only keep participants with 12 measurements\n    filter(n() == 12) %&gt;%\n  # Ungroup the data frame to avoid problems\n  ungroup() %&gt;%\n  # fix the order of time-point and training volume factors\n  mutate(time = factor(time, levels = c(\"pre\", \"session1\", \"week2\", \"week5\", \"week9\", \"post\")),\n         sets = factor(sets, levels = c(\"single\", \"multiple\"))) %&gt;%\n  print()\n\nNext we will fit the model using the time factor, plot the residuals and inspect the results.\n\nm3 &lt;- lmer(load ~ time * sets + (1|participant), data = dat)\n\n\nplot(m3) # This might indicate a larger variation with increased values of predicted values\n\nsummary(m3)\n\nWhen inspecting the results we can see that there is a negligible difference between the baseline (pre) and session1 as indicated in the coefficient timesession1. Similarly, we did not expect any differences between volume conditions at this tests. The coefficient timesession1:setsmultiple shows us the estimated difference between volume conditions at session1. At later time-points we can see that the model suggests differences between volume conditions considerably larger than the estimated standard error, indicating a significant effect.\nWe will now convert the factor time into a continuous variable that represents the number of weeks of training timec (c for continuous). Strength tests at preand session1 were both performed before the training had started, so there is no way any differences would have been a result of the training intervention. If converting this to a continuous scale, both preand session1 would be zero weeks of training. The post time point is collected after 12 weeks of training and weeks 2, 5 and 9 are self explanatory. We will convert the time factor to a continuous scale to model the increase in strength over time.\nBy replacing the factor variable with a continuous variable we will reduce the number of estimated coefficients. A factor variable adds dummy variables to the regression model but the continuous variable can be interpreted as a single coefficient.\n\ndat &lt;- strengthvolume %&gt;% \n  # Filter to include isokinetic tests of included participants without missing values\n  filter(exercise == \"isok.60\",\n         !is.na(load)) %&gt;%\n  # Group the data frame by participants\n    group_by(participant) %&gt;%\n  # Filter to only keep participants with 12 measurements\n    filter(n() == 12) %&gt;%\n  # Ungroup the data frame to avoid problems\n  ungroup() %&gt;%\n  # fix the order of time-point and training volume factors\n  mutate(time = factor(time, levels = c(\"pre\", \"session1\", \"week2\", \"week5\", \"week9\", \"post\")),\n         sets = factor(sets, levels = c(\"single\", \"multiple\")),\n         # Convert to continuous time (number of weeks of training)\n         # pre and session1 are zero weeks\n         timec = if_else(time %in% c(\"pre\", \"session1\"), 0, \n                # post is 12 weeks\n                         if_else(time == \"post\", 12, \n              # From remaining time indicators, Remove the \"week\" and treat as numeric \n                                 as.numeric(gsub(\"week\", \"\", time))))) %&gt;%\n  print()\n\n\n# Fit the model\nm4 &lt;- lmer(load ~ timec * sets + (1|participant), data = dat)\n\n\n\n# Plot the residuals, still an issue of potential heteroscedasticity\nplot(m4)\n\nsummary(m4)\n\nInstead of reading the time factors as differences between a reference level (e.g. pre) and a specific level of interest we are now interpreting time as a difference between any to weeks. Technically, the slope for the coefficient timec can be read as for every week increase in timec, the strength increases about 1.63 Nm. There is a slight difference in baseline between volume conditions (4.91 Nm). The interaction coefficient tells us that in addition to the increase per week of 1.63 Nm in the single-set condition, the multiple-set condition increases an additional 1.16 Nm per week.\nWe can translate this to a comparison over the full study. The single-set condition increases its strength 19.5 Nm (\\(1.63 \\times 12~weeks\\)). The single set condition adds another 13.9 Nm over twelve weeks (\\(1.16 \\times 12~weeks\\)) resulting in a total increase of 33.5 Nm.\nTreating time as a continuous variable in a simple model as the one we have used comes with the assumption that strength increases linearly over time. This can be assessed visually by plotting data from each participant over time. We can add a straight line per participant to the plot indicating the per participant model relationship between strength and time.\nThere is some noise in the measurement but a straight line captures the overall pattern quite well. We do not need to make other assumptions regarding the relationship between time and strength other than a linear one (run the code to see the plot).\n\ndat %&gt;%\n  group_by(participant, sets, timec) %&gt;%\n  summarise(load = mean(load)) %&gt;%\n  ggplot(aes(timec, load, group = participant, color = participant)) + \n  geom_line() + \n  facet_wrap(~ sets) + geom_smooth(method = \"lm\", se = FALSE) + \n  theme(legend.position = \"none\")\n\n\n\n21.3.3 More on diagnostics\nThe same rules apply for the mixed effects model as for the ordinary linear model (except for the independence assumption). As already noted above, using simple commands we can first check the residual plot:\nplot(m4)\nThe residuals should show no pattern.\nWe can also make a qqplot of the residuals to check for normality:\nqqnorm(resid(m4)); qqline(resid(m4))\nThis is really two commands separated with a ; . The first plots the data, the second adds the line.\nThe interpretations of the above is that the residual plot might indicate a tendency of larger variation at higher number of the fitted data, but this deviation is not that big. The second plot assesses normality of the residuals, they are plotted against a theoretical distribution (the line) and deviations from the line indicates problems with the model fit. However, m4 from above looks good.\n\n\n21.3.4 More about mixed effects models\nWe have only scratched the surface. The models can be extended far beyond what this course covers. If you do an experiment in your master thesis, I’m pretty sure you will analyze it with a mixed model! It might be a good idea to read further.\nThis is a nice paper (the first part is a general introduction):\nHarrison, X. A., et al. (2018). “A brief introduction to mixed effects modelling and multi-model inference in ecology.” PeerJ 6: e4794-e4794."
  },
  {
    "objectID": "24-mixed-models.html#references",
    "href": "24-mixed-models.html#references",
    "title": "21  Mixed models - Analyzing repeated measures designs",
    "section": "21.4 References",
    "text": "21.4 References\n\n\n\n\nHammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. “Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis.” Journal Article. The Journal of Physiology 598 (3): 543–65. https://doi.org/10.1113/JP278455."
  },
  {
    "objectID": "24-mixed-models.html#footnotes",
    "href": "24-mixed-models.html#footnotes",
    "title": "21  Mixed models - Analyzing repeated measures designs",
    "section": "",
    "text": "If you write ?pvalues in your console you will get some alternatives on how to get p-values. We will focus on the confidence intervals obtained with confint for inference.↩︎"
  }
]
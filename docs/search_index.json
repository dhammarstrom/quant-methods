[["index.html", "Quantitative methods and statistics (In Sport and Exercise Science) Chapter 1 Introduction 1.1 Practical information 1.2 Assignments and Portfolio exam 1.3 Other information", " Quantitative methods and statistics (In Sport and Exercise Science) Daniel Hammarström Updated: 2021-08-25 Chapter 1 Introduction Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. These course notes covers almost the entire course through the combination of video lectures, tutorials and references to the course literature and external resources. 1.1 Practical information 1.1.1 Learning objectives Learning objectives can be read in Norwegian here. 1.1.2 Learning strategies The course will include lectures, laboratory exercises, computer exercises, seminars and student presentations. Lectures will be held on-line (zoom), as pre-recorded in this book and in-person on campus. Due to the current pandemic, you are required to do laboratory exercises in your cohort. Computer exercises require that you have special computer software installed on your computer. The software is free (see specific chapters in these course notes). Assignments will be presented in this text with information on how to hand them in. The whole course is evaluated based on a portfolio (see below). 1.1.3 Course evaluation As a student you can contribute to the quality of the course by engaging in course evaluation throughout the course. You will be asked to answer a pre-course questionnaire about your expectations and a post-course questionnaire about your experiences. You are also welcomed to take part in systematic discussions during the course about the quality of teaching and course material. With these notes I want to underline the importance of student participation in the continuous development of the course (and program) teaching/learning qualities. 1.1.4 Lecturers and course administration In order of appearance Daniel Hammarström (daniel.hammarstrom@inn.no), is responsible for course administration and will be teaching statistics and molecular methods. Kristian Lian, Ingvill Odden and Lars Nymoen will act as teacher assistants in organizing methods in the physiology lab. Stein Olaf Olsen will act as a teacher assistant in the molecular lab. Prof. Carten Lundby will cover aspects CO2 re-breathing techniques (physiology). Prof. Finnur Dellsén will cover philosophy of science. Prof. Stian Ellefsen will teach molecular methods. 1.1.5 Updates, notifications and general communication These course notes will be updated during the course. General information and last minute changes will be posted on Canvas, make sure to check it as part of your daily study routine. 1.1.6 Literature A full list of recommended literature can be found here. Literature will be referenced in specific sections in these course notes. 1.1.7 Grades The course is graded pass/fail. 1.1.8 Language My (Daniel) first language is Swedish, Im sure most of you will understand what Im talking about. However, due to the fact that we accept international students to the program, most written communication and some lectures will be in English. You are not expected to write in English, it is however possible! 1.2 Assignments and Portfolio exam The course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based. Assignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio. The table below shows all assignments that are part of the course. Some are not to be included in the portfolio and some assignments are group assignments (see Table). Assignment Due date Included in portfolio Group assignment Descriptive statistics, reliability and validity, tools for reproducible data science (See Chapter 10) 2021-09-10 Yes Yes Study designs 2021-10-01 Yes No Extraction and analysis of DNA 2021-10-15 Optionala Yes Extraction of RNA and analysis of qPCR experiments 2021-10- Optionala Yes Extraction and analysis of Protein 2021-10- Optionala Yes Regression models and prediction from data 2021-10- No Yes Drawing inference from statistical models 2021-10- No Yes Statistical power and sample size calculations 2021-11- Yes Yes Analyzing repeated measures experiments 2021-11- Yes No Philosophy of scienceb 2021-11- Yes No a Select one laboratory assignments for your portfolio exam. b This assignment is presented in connection with lectures. In addition to arbeidskrav/assignments, you are required to contribute to the course wiki. The wiki page is hosted at github.com/dhammarstrom/IDR4000-2021/. In order to contribute you need to set up your own github account. The language of the wiki should be Norwegian. Smaller assignments and quizzes are presented in this book, but you are not required to do them to pass the course. 1.3 Other information "],["introduction-to-data-science.html", "Chapter 2 Introduction to data science 2.1 About data in the world of sport and exercise 2.2 Replication and Reproducibility 2.3 Tools in data science", " Chapter 2 Introduction to data science 2.1 About data in the world of sport and exercise Data is everywhere. Most of us walk around with a data collection device in our pockets all the time. This device (your mobile phone), records and store data about you all throughout the day. Such data are the basis of the quantified self movement1 that have grown in popularity as capabilities to record data from daily life has become better. People interested in quantifying their personal life does so for different reasons, but often with the intent to improve their health2. Much of these kind of data are readily available to us due to the fact we are protected by data privacy policies and regarded as personal data3. With some effort you yourself can get your data out of your iphone to explore, for example, your daily step count. I discovered that my phone(s) has been collecting data for me since 2016 and I tend to walk less steps on Sundays compared to Saturdays (see Figure 2.1). Figure 2.1: Step count data from my iPhone displayed as all avalable data points (A, after data cleaning) and average step per weekday, per year and season (B). Data are also collected and stored in publicly available databases. Such databases are created for the purpose to store specific types of data, such as soccer4 or biathlon results5, or biological information such as gene sequences6. Even data from scientific studies are now days often publicly available7 meaning that we can perform scientific studies on unique data sets without collecting the data ourselves. The above examples shows that there are abundance of data around and available to us. The problem is that it is hard understand all this data. This is where data science and data literacy comes in. In the world of sport and exercise, regardless if you are interested in doing scientific investigations, coach a soccer-team or individual athletes or help patients recover from surgery using exercise therapy, you are faced with the problem of handling and make sense of data. Some of the key skills and deeper understanding about data science are very much transferable between such areas of practice. Think about the literature! Spiegelhalter (The Art of Statistics, in the introduction chapter) talks about how statistics has evolved towards the broader field of data science. In data science, statistical theory and methods are just parts of the problem solving cycle. Try to think about how you would use the PPDAC cycle as a exercise coach and a scientist. What are the similarities and differences? One broader aim of this course is for you to develop skills to better understand data. 2.2 Replication and Reproducibility In scientific research, replication is a way to confirm scientific claims. When a result can be confirmed by an independent group of researchers, the claim is likely more true. Many results will however never be possible to replicate due to the size of trials, costs and urgency of the research question. A recent example could perhaps be the many vaccine trials performed to develop a vaccines against COVID-198. Other examples concern studies with unique study populations, such as large scale epidemiological studies (Peng, Dominici, and Zeger 2006), but the same could be said to be true for unique investigations in sport and exercise science. When studies are not likely to be replicated, reproducibility of the analyses and results has been suggested to be a minimum standard for scientific studies. Reproducibility means that given the same datas, similar results or conclusions can be drawn by independent researchers (Peng, Dominici, and Zeger 2006). Peng et al. (Peng, Dominici, and Zeger 2006) suggests that a fully reproducible study has Available data. Computer code (software) that produces the results of the study. Documentation that describes the software and data used in the study, and ways to share the data and code. The above principally relates to the trust we can place in scientific results. However, the minimum standard of reproducibility has advantages also for the individual researcher (or master student)! When working with reproducible methods we will develop ways of documenting and automating our analyses. This will make it easier to collaborate with others. And, as it turns out, your most frequent collaborator is you, in the future! A reproducible data analysis means that you will make it explicit and transparent. In a traditional data analysis, most activities are in the black box. In order to avoid bias (Ioannidis 2005), the black box needs to be opened and you need to actively make transparent decisions all along the analytic pipeline (Leek and Peng 2015). This pipeline preferably involves the whole problem solving cycle described by Spiegelhalter (Spiegelhalter 2019). However the tools that we will learn about in this course focuses primarily on the steps from the experimental design to presentation of statistical results (Leek and Peng 2015). These steps includes data collection (and storage), data cleaning, exploratory data analysis, statistical modelling and statistical inference (and communication) (Leek and Peng 2015). 2.3 Tools in data science Ways to interpret and make sense of data involves different methods. These methods are now days often implemented in computer software. This means that when you as a practitioner (scientist, coach, analyst ) want to understand data, you have to master some kind of computer software. The most common software used to understand data is probably Microsofts Excel. You can do amazing stuff with Excel! In the world of sport and exercise Excel has been used in such diverse activities such as scientific investigations, planning and recording training for Olympic medalists9 and scheduling appointments. For scientific research, most people use additional software to do statistical analyses. If you have spent time in higher education you have probably heard about SPSS, Stata or Jamovi. These are all specialized software used for statistical analyses. The above mentioned tools can all be used as part of a fully reproducible workflow. However, there are software solutions that actually suits this requirement better than others. Going back to the description of reproducible science as made by Peng et al. (Peng, Dominici, and Zeger 2006), we want software where analyses can be Human- and computer-readable, meaning that we want to be able to write scripts, or computer programs that execute the analyses. Documented, meaning that along the code we want to be able to describe what the code does. Available and able to share with other, meaning that we analyses can be run on open and free software to maximize ability to share them. This means that the software that we would prefer should be run using scripts (as opposed to point and click) and be free of charge (and open source, as opposed to expensive and proprietary). These criteria can be fulfilled when we use software that is written around the R language (although alternatives exists10). R is a computer language that is especially well suited for reproducible data analysis. As users are able to contribute software extensions, also called packages, many specialized software implementation exists for different tasks, such as creating figures or analyses of specific data. Around R, people have been developing auxiliary software to enable reproducible data analysis. The negative part of all these opportunities is that using R requires some effort. The learning curve is steep! Even though you might not use R ever again after this course, making and effort trying to learn it will let you know something about programming, capabilities of modern data science, statistical analysis and software/computers in general. These areas are all part of our modern society and are very much transferrable regardless of what computer language we are talking about. In a following chapter of these course notes we will go through installing and starting up R. Cheat sheets are available in R Studio: Help &gt; Cheatsheets Make sure to look through the installation instructions to get pdf options working See e.g. Apples Privacy Policy. understat.com stores match specific data from major leagues. Data are available through software packages such as worldfootballR biathlonresults.com/ hosts results from the international biathlon federation. An example of analyzed data can be seen here. Ensembl and the National center for biotechnology information are commonly used databases in the biomedical sciences. We published our raw data together with a recent paper (Mølmen et al 2021 doi: 10.1186/s12967-021-02969-1.) together with code to analyze it in a public repository. https://www.evaluate.com/vantage/articles/news/snippets/its-official-covid-19-vaccine-trials-rank-among-largest The amount of time used by different coaches to create their own specific coaching software really makes many of them amateur software engineers. See for example this training journal from swedish orienteering. In addition to R, Python offers a free open source environment for reproducible analyses. The choice between the two are matter of taste. "],["storing-data-in-spreadsheets.html", "Chapter 3 Storing data in spreadsheets 3.1 Cells and simple functions 3.2 Tidy data and data storage 3.3 Recording data 3.4 Saving data", " Chapter 3 Storing data in spreadsheets I previously mentioned spreadsheets like those created in Excel. These are indeed great, but not great for reproducible science or data analysis. This is because they are not easily documented and scripted. The data is actually part of the analysis. Another danger with spreadsheets (like MS Excel) is that it re-formats your data. This is such a big problem for scientists that we have apparently started renaming genes. Errors are frequent in spreadsheets, not only because renaming (Ziemann, Eren, and El-Osta 2016), but also because of bad formatting of formulas (Stephen, Kenneth, and Barry 2009). These are both reasons for using spreadsheets only for what they do best: data input and data storage. Think about the literature Broman and Woo (Broman and Woo 2018) gives several pointers on how to use spreadsheets for data input and storage. Think about your experince with Excel, what is the most common mistake you made when handling data in spreadsheets? Although data storage and data input are great ways to use spreadsheets, its good to know a little about the capabilities of your spreadsheet software. 3.1 Cells and simple functions A spreadsheet consists of cells, these can contain values, such as text, numbers, formulas and functions. Cells may also be formatted with attributes such as color or text styles. Below is an example of some data entered in a spreadsheet. Figure 3.1: Example entries from an Excel spreadsheet Cell B6 contains a simple formula: = C6 + D6. This formula adds cells C6 and D6 resulting in the sum, 8. In formulas, mathematical operators can be used (\\(+, -, \\times , \\div\\) ). Formulas can be also extended with inbuilt function such as showed in 3.1. Table 3.1: Often used functions in excel. Function English Norwegian Sum SUM() SUMMER() Average AVERAGE() GJENNOMSNITT() Standard deviation STDEV.S() STDEV.S() Count COUNT() ANTALL() Intercept INTERCEPT() SKJÆRINGSPUNKT() Slope SLOPE() STIGNINGSTALL() If IF() HVIS() The sum, average and standard deviation and count are simple functions for summarizing data. Intercept and slope are both examples of functions used to get simple associations from to sets of numbers (based on a regression model). The if function is an example of a function that can be used to conditionally enter data in a cell. For example, IF cell A1 contains a certain number, then cell B1 should display another a specified text. When looking for tips and tricks online, you may come across functions for excel in other languages than what is installed on your computer. To translate functions, and for a full overview of functions included in Microsoft Excel, see this website en.excel-translator.de/. 3.2 Tidy data and data storage Hadley Wickham uses a quote from Tolstoy when describing the principle of tidy data (Wickham 2014). This quote is so famous that it has given name to a principle. The principle in turn comes in many variants but basically states that when something goes wrong, it can be wrong in multiple ways. But when it is right/correct/works/succeeds, it does so in only one way11. This principle can be applied to data sets. There are so many ways that formatting of data sets can be problematic, but a limited sets of principles makes it good. Figure 3.2: Leo Tolstoy at the time when he was (possibly) authoring Anna Karenina. (Source: https://en.wikipedia.org/wiki/Leo_Tolstoy) A tidy data set consists of values originating from observations and belonging to variables. A variable is a definition of the values based on attributes. An observation may consist of several variables (Wickham 2014). A tidy data set typically has got one observation per row and one variable per column. Lets say that we want to collect data from a strength test. A participant (participant is a variable) in our study conducts tests before and after the intervention (time is a variable) in two exercises (exercise is a variable) and we record the maximal strength in kg (load is a variable). The data set will look something like in the table below (3.2). Table 3.2: Example of tidy data. Participant Time Exercise Load Bruce Wayne pre Bench press 95 Bruce Wayne post Bench press 128 Bruce Wayne pre Leg press 180 Bruce Wayne post Leg press 280 Another example contains variables that actually carries two pieces of information in one variable. We again did a strength test, this time as maximal isometric contractions and in each test consisted of two attempts. We record this in two different variables, attempt 1 and 2. The resulting data set could look something like in Table 3.3. Table 3.3: Another example of tidy data. Participant Time Exercise Attempt1 Attempt2 Selina Kyle pre Isometric 81.3 92.5 Selina Kyle post Isometric 97.1 114.1 To make this data set tidy we need to extract the attempt information and record it in another variable as seen in Table 3.4. Table 3.4: A third example of tidy data. Participant Time Exercise Attempt load Selina Kyle pre Isometric 1 81.3 Selina Kyle pre Isometric 2 92.5 Selina Kyle post Isometric 1 97.1 Selina Kyle post Isometric 2 114.1 This naturally gives additional rows to the data set. This is sometimes referred to as long format data as opposed to the structure where each attempt is given separate variables, something that is called wide format. You will notice during the course that for most purposes, the long format is most convenient. This is true when we create graphs and do statistical modelling. But sometimes a variable needs to be structured in a wide format to allow for certain operations. If we follow what is recommended by Broman and Woo (Broman and Woo 2018), it is clear that each cell in a spreadsheet should only contain one value. If we for example decide to format a cell to a certain color, we add data to that cell on top of the actual data. You might add color to a cell in order to remember to add or change data. However, when you use the data set in other software, this information is lost. You should instead add another variable to allow for such data to be properly recorded. Using a variable called comments you can add text thta actually describes some information about that particular observation, information that is not lost when you use the data set in another software. 3.3 Recording data A trade secret12 from people who work all day with data and programming is that they are lazy. Lazy in the sense that you do not want to type too much, and absolutely not use the computer mouse when it can be avoided. When recording data we can try to be lazy to. We can do this by shortening variable names and not e.g. using CAPITAL letters when entering text in data storage. After a hard day at the keyboard, you will be happy to write strtest instead of Strength Test. The extra effort of using two capital letters might be the thing to tip you over the edge13. However, we should not be too lazy either, variable names and values should short, but meaningful (Broman and Woo 2018). Figure 3.3: D-FENS Foster gets pushed over the edge (Source: https://en.wikipedia.org/wiki/Falling_Down) Data and variables should also be consistent. Do not mix data type, use a consistent way of entering e.g. dates and time, do not uses spaces or special characters. To enforce this you might want start your data collection with writing up a data dictionary that describes all variables you are collection. The dictionary can set the rules for your variables. This dictionary can also guide your data validation. In Excel, data validation can be used to set rules for data entry. For example, if you have a numeric variable, you can set Excel only to accept numbers in specified set of cells. This makes it harder to enter erroneous data. 3.4 Saving data Data from spreadsheets can be saved as special spreadsheet files, such as .xlsx. This format allows for functions, multiple spreadsheets in the same file (tabs) and cell formatting. If you follow the tips described above and in (Broman and Woo 2018) you do not need this fancy format. Instead you can store your data as a .csv file. This format may be read and edited with Excel (or another spreadsheet software) but also in a plain text editor. Data entered in this format (comma-separated values; csv) can look like this in a text editor: Participant;Time;Exercise;Attempt;load Selina Kyle;pre;Isometric;1;81.3 Selina Kyle;pre;Isometric;2;92.5 Selina Kyle;post;Isometric;1;97.1 Selina Kyle;post;Isometric;2;114.1 This is actually quite nice. The data takes little space, the simple format requires that data is well documented using e.g. a data dictionary and it is available for many other softwares as the format is simple. The data can be documented using a README file that could describe the purpose and methods of data collection, how the data is structured and what kind of data the variables contains. A simple README file can be written in a text editor such as Notepad and saved as a .txt file. Later in this course we will introduce a markup language often used to create README files containing a syntax that formats the text to a more pleasant style when converted to other formats. See https://en.wikipedia.org/wiki/Anna_Karenina_principle A trade secret as in not generally known to the public. See en.wikipedia.org/wiki/Trade_secret. In the movie Falling Down, Michael Douglas plays a unemployed engineer who gets push over edge, would it have been enough with a few to many capital letters? "],["installing-and-starting-up-r-and-r-studio.html", "Chapter 4 Installing and starting up R (and R Studio) 4.1 Installing R 4.2 Installing R Studio 4.3 Getting to know R and RStudio 4.4 Reproducible computing 4.5 Packages 4.6 Installing and using swirl 4.7 R scripts 4.8 R markdown files", " Chapter 4 Installing and starting up R (and R Studio) This chapter contains step-by-step instructions for installing and running R and RStudio. It will also introduce you to some concepts when talking to R. By the end of this chapter you will be able to answer these questions: What is R and RStudio? How can I interact with R? What are the components of RStudio How do I maintain a reproducible work-flow in R and RStudio? What is a R-script What is a R-markdown file 4.1 Installing R R is a free, open-source software designed for statistical computing. We will use R as a part of an environment (using R Studio, introduced below). To download and install R: Go to https://cran.uib.no/, select your operating system (Download R for Windows, MacOS or Linux). If you have Windows, choose base, click on Download R () for windows, save and run the file. The installation process should be self explanatory. If you have MacOS, download and install the latest release. 4.2 Installing R Studio RStudio is a software designed to make it easier to use R. It is free to download and use. It is designed as an integrated development environment that lets you organize your work together with R and other tools. Install it by going to https://www.rstudio.com/. Select Products and RStudio Go to desktop and select DOWNLOAD RSTUDIO DESKTOP Select the free open source license and download the file made for your operating system (use the installers). 4.3 Getting to know R and RStudio R is a software used for scientific/statistical computing. If R is the engine, RStudio is the rest of the car. What does this mean? When doing operations in R, you are actually interacting with R through RStudio. RStudio have some important components to help you interact with R. 4.3.1 The source The source is where you keep your code. When writing your code in a text-file, you can call it a script, this is essentially a computer program where you tell R what to do. It is executed from top to bottom. You can send one line of code, multiple lines or whole sections into R. In the image below, the source window is in the top left corner. 4.3.2 Environment The environment is where all your objects are located. Objects can be variables or data sets that you are working with. In RStudio the environment is listed under the environment tab (bottom left in the image). Copy and run the code below. a &lt;- c(1, 2, 4) What happened in your environment? 4.3.3 The console Here you can directly interact with R. This is also where output from R is printed. In the image below, the console is in the top right corner. 4.3.4 Files, plots, packages and help files In RStudio files are accessible from the Files tab. The files tab shows the files in you root folder. The root folder is where R will search for files if you till it to. We will talk more about the root folder later in connection with projects. Plots are displayed in the Plot tab. Packages are listed in the packages tab. If you access the help files, these will be displayed in the help tab. In the image below all these tabs are in the bottom right corner. Figure 4.1: RStudio when first opened up. 4.3.5 Customizing the apperance of RStudio To access options for RStudio, go to Tools -&gt; Global options Figure 4.2: Accessing options for your RStudio IDE Under appearance you can customize the theme of RStudio, select something that is easy on the eye! Figure 4.3: Accessing options for your RStudio IDE and selection a theme Under pane layout, you can set where you want your tabs, I like to have the source on the left, above the environment. This way you can have the source window at full vertical size and still look at plots and the console to the right. Figure 4.4: Accessing options for your RStudio IDE and set the panes 4.4 Reproducible computing Computations are reproducible when you can show how they were performed. This is achieved by creating programs from where your analyses are done. In R, these programs are lines or R code stored in a text-file, either .R-files or .Rmd-files. .R-files are scripts only containing code and comments. A .Rmd-file is a special script combining text and computer code, when the Rmd-file is executed, it creates a report and outputs the results from the code. This means that to work in a reproducible way, you need to script all your operations. Figure 4.5: Reproducible vs. non-reproducible workflow Importantly, in RStudio you can shut down storing temporary objects in a environment that is relaunched on start up. What is the consequence of having such a situation? To disable this option, set save works pace to NEVER! Figure 4.6: Set the workspace option to never save. 4.5 Packages The R ecosystem consists of packages. These are functions organized in a systematic manner. Functions are created to perform a specialized task. And packages often have many function used to do e.g. analyses of a specific kind of data, or more general task such as making figures or handle data. In this course we will use many different packages, for example dplyr, tidyr and ggplot2. dplyr and tidyr are packages used to transform and clean data. ggplot2 is used for making figures. To install a package, you use the install.packages() function. You only need to do this once on your computer (unless you re-install R). You can write the following code in your console to install dplyr. install.packages(&quot;dplyr&quot;) Alternatively, click Packages and Install and search for the package you want to install. To use a package, you have to load it into your environment. Use the library() function to load a package. library(&quot;dplyr&quot;) 4.6 Installing and using swirl Swirl is a great way to get to know how to talk with R. Swirl consists of lessons created for different topics. Install swirl by typing the following into your console: install.packages(&quot;swirl&quot;) When swirlis installed you will need to load the package This means that all functions that are included in package becomes available to you in your R session. To load the package you use the library function. library(&quot;swirl&quot;) When you run the above command in your console you will get a message saying to call swirl() when you are ready to learn. I would like you to run the course R Programming: The basics of programming in R. Swirl will ask if you want to install it. After installation, just follow the instructions in the console. To get out of swirl, just press ESC. 4.7 R scripts As pointed out elsewhere (Wickham and Grolemund 2017; Peng, Dominici, and Zeger 2006), programming is an important part of a reproducible data analysis. This lets you build your analysis, go back a change components of it an re-run it with any number of changes. In this process you will probably learn more about your data. Putting all these steps in a program lets you save the whole analytic process. In R you may start by working with R scripts. These are basically text files that are written with a special syntax that can be interpreted by your computer. Additionally you have the possibility to add comments that makes the code more readable. R code is generally easy to read. But you will likely need additional comments to make it easier to show what you intend to do. A R script can be thought of as a computer program that when executed from top to bottom perform as series of steps in the order that they appear in the file. A feature of a well working program is that it is self-contained, i.e. it contains all parts needed to run. If you need to load a package or data, make sure that these steps are in the beginning of the script. A nice feature of combining code and comments can be that you first write a plan in plain language and then add computer code to perform the steps that you want to do. Below is a simple example. Comments start with a #, this is interpreted by R as non-code line and will be ignored. The R code is structured in series, the first steps are needed to perform sequential steps. As mentioned, the work-flow of creating the example below would be to first make a plan by writing the comments and the adding the code. # Create a data set in a data.frame df &lt;- data.frame(x = rnorm(100, 100, 10), y = runif(100, min = 10, max = 25)) # Add column x and y together in a new variable called z df$z &lt;- df$x + df$y # Make a figure of the resulting data frame by plotting x against z with(df, plot(x, z)) When working in RStudio, you can run a bit of the code by selecting it and pressing CTRL + ENTER (CMD + ENTER on Mac). You can also execute a line simply by having your cursor on a specific line and press CTRL + ENTER. Code execution means that the specific part of the script (line or section) is copied to the console and activated. R Scripts can also be sourced. This means that the whole script will be executed from to to bottom when you tell R to do so. Lets say that you have a script that creates a figure and saves it, called figure1.R. By using the source() function you can tell R to execute the script (source(\"figure1.R\")). As you can see in this example the filename extension .R tells you that a file can be intepreted as a R script. 4.8 R markdown files R markdown files are more advanced computer programs as they in a structured way combines plain text and code to create an output file such as a .html, .pdf or .doc document. The text parts are written using a special syntax, markdown. The point of markdown is that you will use the same syntax that is later possible to convert to multiple formats. The syntax lets you do all formatting explicitly, for example instead of getting your mouse to superscript some text you can add syntax a^2^ to achieve a2. A full guide to RMarkdown can be found on the official R markdown web pages. I suggest you take the time to get an overview of this langiage as it will make more fluent in the tools that enables reproducible computing. When writing R markdown, it is handy to have a cheat sheet close by when writing, here is an example14. If you do not want to write text in a simple text editor, RStudio has its own visual markdown editor. This editor contains similar functions by press of buttons as in for example word. R Markdown files has the file name extension .Rmd. 4.8.1 Starting up your first R markdown file A R markdown report is basically a text document containing plain text and code. When you compile your report, the code will be evaluated and figures, calculations and so on will be performed per your specifications. The resulting file will be an html, docx or pdf file. You can choose if you would like to display your code or not but your code is always available in your source document. R Markdown is very versatile, you can make word documents, blog posts, websites and pdf documents15. When in R Studio, you can start a new document using File &gt; New File &gt; R Markdown. This will launch a file in your script window looking something like this: --- title: &quot;Untitled&quot; author: &quot;Daniel Hammarström&quot; date: &quot;2020 05 09&quot; output: html_document --- ## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;. This is not an empty document and you have to remove the pre-written instructions. These instructions are quite handy though. Basically, in code chunks you write R code, this code will be evaluated and the output will be displayed in the file you create. Between code chunks you can write markdown text. This will be displayed as ordinary text in your created document. The plain text sections can also contain code. A code chunk is created using ```{r, eval=TRUE} 1 + 1 ``` This code chunk calculates 1+1, when you compile the document, the result of this calculation will be shown below the code chunk. The same computation can be made inline. An inline code chunk is created using `r 1+1`, here only the result of this computation will be shown in your text. When you compile the doucument it is called knitting, R uses a package called knitr made to compile R Markdown files. In the upper part of the source window, there is a button kalled Knit. When you press it, RStudio will aske you to save the Rmd file and an output file will be created. 4.8.2 Microsoft Word intergration Sometimes it is usefull to knit to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown can be used as a source for word documents (.docx). To create a word document from your Rmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this: --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document --- The output: word_document tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use thiss as reference. --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document: reference_docx: reference.docx --- Edit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the Title style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it. When you knit the document again, your updated styles will be used your word document. Here you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here. 4.8.3 Adding references References/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are avaliable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib. The bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information: --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document: reference_docx: reference.docx bibliography: bibliography.bib --- Add citations to the file in bibtex-format. Here is an example: @Article{refID1, Author=&quot;Ellefsen, S. and Hammarstrom, D. and Strand, T. A. and Zacharoff, E. and Whist, J. E. and Rauk, I. and Nygaard, H. and Vegge, G. and Hanestadhaugen, M. and Wernbom, M. and Cumming, K. T. and Rønning, R. and Raastad, T. and Rønnestad, B. R. &quot;, Title=&quot;{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}&quot;, Journal=&quot;Am. J. Physiol. Regul. Integr. Comp. Physiol.&quot;, Year=&quot;2015&quot;, Volume=&quot;309&quot;, Number=&quot;7&quot;, Pages=&quot;R767--779&quot;, Month=&quot;Oct&quot;} The part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form Blood flow-restricted training leads to similar adaptations as traditional training [@refID1]. This will appear in text as: Blood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015). The reference will end up in the end of the document (as on this webpage). You can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with scandinavian letters (å æ ä ø ö). Recently RStudio added support for adding citations inside the visual markdown editor. Cheat sheets are available in R Studio: Help &gt; Cheatsheets Make sure to look through the installation instructions to get pdf options working "],["creating-your-first-graph.html", "Chapter 5 Creating your first graph 5.1 Resources 5.2 Learning objectives 5.3 Prerequisites 5.4 The ggplot2 system 5.5 Different geoms using real data 5.6 Themes 5.7 Test your understandning", " Chapter 5 Creating your first graph Data visualization is an efficient way of understanding data. By using graphs we can communicate characteristics of a data set that would have been impossible with a limited number of summary statistics (central tendencies, spread etc.). In Chapter 2 of his book (Spiegelhalter 2019), Spiegelhalter touches upon this fact when he describes different types of graphs and their use to understand different data sets. A great argument for the use of data visualization is the need to understand what factors might explain variation in a given data set (Spiegelhalter 2019). In this sense, data visualization can be thought of as an initial step in understanding data, data visualization as an exploratory tool. RStudio is a powerful environment for data visualization. Together with R (that is excellent for creating graphs), you can create and preview figures that represents your data in RStudio. R has got several systems for creating figures, plots, graphs. In this course, we will use ggplot2. Another system for plotting comes with the base installation of R. This is sometimes referred to as base R (see this tutorial, or this. Another well described and used system is lattice. We choose ggplot2 because it works well with the tidyverse, and it is well described. 5.1 Resources There are several good resources aimed at ggplot2: Chapter 4 in R for data science The ggplot2 book The ggplot2 cheatsheet 5.2 Learning objectives After this session, you should be able to answer: What are geoms? What is mapping data to aesthetics? What are theme components? You should also be able to create your first graph. 5.3 Prerequisites To follow the exercises below you will need to some data. For the purpose of this course, I have created a package that contains the data sets we need. In this session we will work with the cyclingstudy data set. To install the package (exscidata) you will need another package called remotes. The code below first checks if the package remotes is installed, or more specifically, if \"remotes\" cannot be found in the list of installed packages. Using the if function makes install.packages(remotes) conditional. If we do not find \"remotes\" among installed packages, then install remotes. The next line of code does the same with the exscidata package. However, since the package is not on CRAN but hosted on github we will need to use remotes to install it. The part of the second line of code that says remotes::install_github(\"dhammarstrom/exscidata\") uses the function install_github without loading the remotes package. The last line of the code below loads the package exscidata using the library function. # Check if remotes is not installed, if TRUE, install remotes if (!&quot;remotes&quot; %in% installed.packages()) install.packages(remotes) # Check if exscidata is not installed, if TRUE, install exscidata from github if (!&quot;exscidata&quot; %in% installed.packages()) remotes::install_github(&quot;dhammarstrom/exscidata&quot;) # Load exscidata library(exscidata) Next we need to load the tidyverse package. This package in turn loads several packages that we will use when transforming data and making our figures. I will include the line of code that checks if the package is installed, if not R will download and install it. We subsequently load the package using library. # Check if tidyverse is not installed, if TRUE, install remotes if (!&quot;tidyverse&quot; %in% installed.packages()) install.packages(tidyverse) library(tidyverse) We are now ready to explore the data set. But first we should talk about the main components of the ggplot2 system. 5.4 The ggplot2 system When using the ggplot2 system we can think of the resulting graph as containing data that has been mapped to different coordinates, colors, shapes, sizes and other attributes that determines what is being visualized. We are using different geometric representations of the data in the visualization. When we map data in ggplot we use a specific function, aes() (short for aesthetic). We will use this inside the main engine, ggplot(). For this first simple example we will create a data set. When you simulate data in R you can tell R what should be the starting point in the random number generator. Using set.seed(100) we can recreate the same numbers from what ever number generator we later use. In the example below, we use rnorm() to simulate numbers from a normal distribution. The settings n = 10, mean = 0 and sd = 1 we will simulate randomly picking 10 numbers from a distribution that has a mean of 0 and a standard deviation of 1. These numbers are stored in a data frame that as assigned to an object that we have named dat. set.seed(99) dat &lt;- data.frame(x = rnorm(10, mean = 0, sd = 1), y = rnorm(10, mean = 10, sd = 2)) The dataset consist of two variables. We will start by creating the canvas, this basically sets the border of the figure we want to create. The ggplot() function takes the dataset as its first argument, followed by the aes() function that is used to map data to coordinates and other attributes. ggplot(dat, aes(x = x, y = y)) Figure 5.1: An empty ggplot canvas. As you can see in Figure 5.1 the code above creates an empty canvas that has enough room to visualize our data. The x- and y-axes are adjusted to give room for graphical representations of the data. Next we need to add geometric shapes. These are functions that we add to the plot using the + sign. These functions all start with geom_ and has and ending that describes the geoms, like point, line, etc. We will add geom_point() to our empty canvas as plotted in Figure 5.1. The geom_point function inherits the mapping from from ggplot(). This means that we do not need to specify anything in geom_point at this stage. ggplot(dat, aes(x = x, y = y)) + geom_point() Figure 5.2: A ggplot canvas with points added. In Figure 5.2 we have added black points to each x- and y-coordinate representing x and y from our data set. To extend the example we will add data to our dataset. In the code below, we create a new variable in the dataset using $ effectively giving us a new column in the data. We use rep(\"A\", 5) to replicate the letter A five times and the same for B. The c() function combines the two in a single vector. We can use head(dat) to see what we accomplished with these operations. The head() function prints the first six rows from the dataset. dat$z &lt;- c(rep(&quot;A&quot;, 5), rep(&quot;B&quot;, 5)) head(dat) ## x y z ## 1 0.2139625 8.508462 A ## 2 0.4796581 11.843101 A ## 3 0.0878287 11.500109 A ## 4 0.4438585 4.982892 A ## 5 -0.3628379 3.918132 A ## 6 0.1226740 10.000532 B We can see that we have an additional variable z that contains \"A\" and \"B\". This new variable can be used to add more information to the plot. Lets say that we want to map the z variable to different colors. We do this by adding color = z to aes. This means that we want the z variable to determine colors. ggplot(dat, aes(x = x, y = y, color = z)) + geom_point() Figure 5.3: A ggplot canvas with colored points added. In Figure 5.3 we can see that different colors are used for the two letters \"A\" and \"B\". Other attributes can also be specified like shape, fill or size. The shape specifies the appearance of the points. When we use use data to map to shapes, ggplot2 will start from the standard shape. Figure 5.4: Shapes in R Possible shapes in the standard framework in R are shown in Figure 5.4. We may use this information to either fix the shape of the points. Lets say that instead of colored points we want filled points. We would change the color = z argument to fill = z instead and select a point shape that can be filled (shapes 21-25, see Figure 5.4. Notice in the code below that shape = 21 has been added to geom_point(). We have specified how points should be displayed. ggplot(dat, aes(x = x, y = y, fill = z)) + geom_point(shape = 21) Figure 5.5: A ggplot canvas with filled points added. Since shape is an attribute it to can be mapped by data. If we want data to determine both shape and fill we could add this information in the aes() function by setting both shape = z and fill = z. We now have to specify what shapes ggplot should use in order to be sure we can combine both shapes and fill. We will use scale_fill_manual and scale_shape_manual to do this. These functions takes lets you specify different values for aesthetics. Notice that we removed shape = 21 from the geom_point() function, but we added size to increase the size of the points. ggplot(dat, aes(x = x, y = y, fill = z, shape = z)) + geom_point(size = 3) + scale_fill_manual(values = c(&quot;red&quot;, &quot;green&quot;)) + scale_shape_manual(values = c(21, 23)) Figure 5.6: Data mapped to fill and shape, and size specified manually to override the default. 5.5 Different geoms using real data We have now seen that the basic ggplot2 figure maps underlying data to coordinates and geometric representations, such as points. We will go further by using some real data. We will be using the cyclingstudy dataset from the exscidata-package. We will start by loading the data and select a few columns that we are interested in. By using data(\"cyclingstudy\") we will load the data set that is part of the exscidata-package to our environment. By looking at the environment tab you can see that this operation adds a data set to the environment. It has 80 observations and 101 variables. Using the glimpse() function from dplyr (which is loaded by loading tidyverse) we will get an overview of all variables in the dataset. I have omitted the output from the code below # Load the data and have a first look data(&quot;cyclingstudy&quot;) glimpse(cyclingstudy) We will store a selected set of variables in a new object for ease of use. We will call this object cycdat. We select variables using the function with the very suitable name select where the first argument specifies the dataset, following arguments specifies what variables we want. Lets say that we are interested in squat jump height. The exscidata package comes with descriptions of the datasets. By writing ?cyclingstudy in your console you will see the description of the data in your help tab. Squat jump is recorded as sj.max, we select this variable together with subject, group and timepoint to create a smaller data set. # Assign a selected set of variables to a smaller data set cycdat &lt;- select(cyclingstudy, subject, group, timepoint, sj.max) # Printing the data set cycdat ## # A tibble: 80 x 4 ## subject group timepoint sj.max ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 INCR pre 31.0 ## 2 2 DECR pre 31.6 ## 3 3 INCR pre 26.8 ## 4 4 DECR pre 29.2 ## 5 5 DECR pre 31.2 ## 6 6 INCR pre 34.2 ## 7 7 MIX pre 30.1 ## 8 8 MIX pre 32.8 ## 9 9 MIX pre 22.7 ## 10 10 INCR pre 29.7 ## # ... with 70 more rows By printing the object we can see that we have a tibble of 80 rows and 4 columns. A tibble can to a large extent be regarded as a data frame, and we will use these words interchangeably. Tibbles are new in the sense that they are developed as part of the tidyverse (Wickham and Grolemund 2017).16 Printing a tibble will display the first 10 rows as we can see from the resulting output. 5.5.1 A plot of values per group Lets say that we want to see how the values differs between groups. Boxplots are a good way to start as they will bring a standardized way of summarizing data. Boxplots can be plotted using the geom_boxplot function. Notice below that we put group on the x-axis (the first argument in the aes function) and sj.max on the y-axis. By doing so ggplot will make the x-axis discrete and the y-axis continuous. # Creating a boxplot of all values per group ggplot(cycdat, aes(group, sj.max)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.7: Boxplot of all data per group from the cycling dataset. We can layers of more geoms to the same plot. We might want to add individual data points also. geom_jitter might be a good place to start. This geom is good as it can be plotted over a group variable and points gets jittered or spread so we avoid overlap. # Creating a boxplot of all values per group ggplot(cycdat, aes(group, sj.max)) + geom_boxplot() + geom_jitter() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). Figure 5.8: Boxplot and jittered points of all data per group from the cycling dataset. Notice that we get warnings saying that there are some data missing, these are removed from the calculation of summary statistics in the boxplots and omitted from plotting the values as points. 5.5.2 Data over time per group and individual In the data set we have a time variable consisting of the labels pre, meso1, meso2 and meso3. When we load the data into R we do so without providing information about the order of these labels. R will put them in alphabetical order when order is required (as in a figure). If we want to plot these data in the right order we have to tell R that these data should have an order. We will convert the timepoint variable to a factor. Factors are variables that can contain more information than what is contained in each cell. Using the factor function we will set the order of the timepoint variable. We assign this transformation of the variable to its original place in the data frame. cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;)) We are now ready to plot data over time, where the time variable is correctly ordered. Lets use the boxplot again to plot all values over time. # Creating a boxplot of all values per time point ggplot(cycdat, aes(timepoint, sj.max)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.9: Boxplot of all data per time-point from the cycling dataset. We do not see any great tendencies in the whole data set. To further explore the data we might want to have different boxes per group per time. We can accomplish this by adding fill = group to our aes function. # Creating a boxplot of all values per group over time ggplot(cycdat, aes(timepoint, sj.max, fill = group)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.10: Boxplot of all data per time-point and group from the cycling dataset. This is possible because geom_boxplots can be filled. The same separation of groups would have been accomplished using color = group, however, then the boxes would get different border colors instead. You might have noticed that the boxplots do not contain all the data, a few data points are outside 1.5 IQR (interquartile range). This, by standard definitions, defines the data point as an outlier. As mentioned above, boxplots does some summarizing and not all data is shown. To explore further we might want to track every participant. To do this we have to tell ggplot on what factor to group the data. In aes() the group argument lets you connect lines based on some grouping variable, in our case it will be subject. We will use a line to connect each participants score over time. Using color = group will additionally give every line a different color depending on which group it belongs to. # Creating a line plot of all values per participant over time, color per group ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.11: Boxplot of all data per time-point and group from the cycling dataset. In Figure 5.11 each line represents a participant, different colors represents different groups. 5.5.3 Titles and labels Often we need to add information to the plot to better communicate its message. Such information could be appropriate titles on axes and legends and extra text needed to explain aspects of the plot. Using the labs() function we can add information that will replace variable names that are being used for all variables that have been mapped in the figure. In the figure below we will start by adding better axis titles. This information goes into x and y in labs() which simply changes the titles of the x- and y-axis. # Creating a line plot of all values per participant over time, color per group ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.12: Boxplot of all data per time-point and group from the cycling dataset. The resulting Figure 5.12 now have better titles for each axis. Notice in the code above that titles needs to be specified with quotation marks. This is a tricky aspect of R, if we wold have omitted the quotation marks we would have told R to look for objects by the name of e.g. Time-point, and this would actually mean that we tryed to subtract time from point since - is interpreted as a minus sign. We might want to add information to the legend also. Since we specified color = group in the aes() function, the same can be manipulated in labs. Lets just add a capital G. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.13: Boxplot of all data per time-point and group from the cycling dataset. We still have the original labels for the tim variable. Remember that we used the factor function above to set the order of the labels. Actually we specified the levels of the factor. We can use the same function to add better labels. In the code below, I will first change the variable in the dataset and then use the exact same code for the plot. cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;), labels = c(&quot;Pre-training&quot;, &quot;Meso-cycle 1&quot;, &quot;Meso-cycle 2&quot;, &quot;Meso-cycle 3&quot;)) ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.14: Boxplot of all data per time-point and group from the cycling dataset. The same goes for the group variable. You can try to change the levels and labels of the grouping variable to make it more descriptive. You can type ?cyclingstudy in your console to read about the group variable and then use this infomation to write better labels using the factor function. In the factor function, the first argument is the variable you want to use as basis of your new factor, the second argument you need to specify is levels which sets the order and lastly you will need to set the labels for each level using labels =. If you write ?factor in your console you will get the help pages for the factor function. Click here to display a possible solution # Change the grouping variable cycdat$group &lt;- factor(cycdat$group, levels = c(&quot;DECR&quot;, &quot;INCR&quot;, &quot;MIX&quot;), labels = c(&quot;Decreased\\nintensity&quot;, &quot;Increased\\nintensity&quot;, &quot;Mixed\\nintensity&quot;)) # Plotting the data as before with the new information added ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) Note: Adding \\n in the the text string breaks the line to get two rows. 5.5.4 Annotations Annotation may become handy when you want to add elements to the graph that is not in the data set. Using ggplot2 annotations are added using the annotate() function. This function first needs to be specified with a geom, these are commonly text or lines or segments. In the code chunk below are several examples of annotations. Firts I save the plot as an object called myplot and then add different annotations to it. myplot &lt;- ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) # A text annotation myplot + annotate(&quot;text&quot;, x = 1, y = 37, label = &quot;This is an annotation&quot;) # A line/segment myplot + annotate(&quot;segment&quot;, x = 1, xend = 3, y = 25, yend = 35, colour = &quot;red&quot;, size = 4) You can copy the code and run it yourself to see the results. annotate is documented here but documentation can also be accessed by typing ?annotate in your console. Try to read the documentation and add a transparent rectangle to a previous plot. Click here for a solution # Change the grouping variable cycdat$group &lt;- factor(cycdat$group, levels = c(&quot;DECR&quot;, &quot;INCR&quot;, &quot;MIX&quot;), labels = c(&quot;Decreased\\nintensity&quot;, &quot;Increased\\nintensity&quot;, &quot;Mixed\\nintensity&quot;)) # Plotting the data as before with the new information added ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) + # A rectangular annotation (alpha = 0.4 makes the rectangle transparent) annotate(&quot;rect&quot;, xmin = 1, xmax = 2, ymin = 30, ymax = 35, alpha = 0.4) Note: Adding \\n in the the text string breaks the line to get two rows. 5.6 Themes Themes in ggplot2 can be used to change everything else about the plot concerning text, colors etc. ggplot2 has some built in themes that are easily activated by adding them to the plot. For example the theme_bw() function will change the theme to a black and white one as in the figure below. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) + theme_bw() ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.15: A figure using the black and white theme from theme_bw. A collection of built in themes are documented here. Individual components of the theme can also be changed using the theme() function. There is a long list of theme components that can be changed using this function. The list can be found here. If we put the theme function last in the ggplot call we will modify one part of the existing theme. Lets say that we want to change the color of the text on the x axis. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) + theme_bw() + theme(axis.text.x = element_text(color = &quot;black&quot;, size = 12, face = &quot;bold&quot;)) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.16: A figure using the black and white theme from theme_bw. The component axis.text.x can be modified using a function that changes appearance of text components, namely element_text. Similarly, other components are changed with specific functions for lines and rectangular shapes (see the help pages for theme. 5.7 Test your understandning In this section you can try to implement what we have discussed above An example solution exists below each figure by press of button. In Figure 5.17, I have used the VO2max data from the cyclingstudy dataset. I have made changes to the time variable (timepoint) to make the labels better. I have added a title to the figure and changed the appearance of the text. I will use an extra package called (ggtext)[https://wilkelab.org/ggtext/index.html] to make it possible to use markdown syntaxt in axis labels. In order to use ggtext you have to install it from CRAN. Figure 5.17: Example figure 1 Click for a solution # Load the package ggtext to make markdown avalable in axis labels. library(ggtext) # For ease of use I save a smaller dataset in a new object cycdat &lt;- select(cyclingstudy, subject, timepoint, VO2.max) # Change the labels of the time variable cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;), labels = c(&quot;Pre-training&quot;, &quot;Meso-cycle 1&quot;, &quot;Meso-cycle 2&quot;, &quot;Meso-cycle 3&quot;)) # create the basic plot ggplot(data = cycdat, aes(timepoint, VO2.max, group = subject)) + # Add lines to connect dots. Putting the lines first and plotting points on top geom_line() + # Add points foe each participant/time geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + # Adding correct axis titles and a figure title labs(x = &quot;Time-point&quot;, y = &quot;VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt; -1&lt;/sup&gt;)&quot;, title = &quot;Maximal aerobic power in response to systematic training in trained cyclists&quot;) + # Changing the text rendering using element_markdown from the ggtext package. theme(axis.title.y = element_markdown(size = 12)) Note: Adding \\n in the the text string breaks the line to get two rows. See Chapter 10 in R for data science "],["wrangling-data-to-create-your-first-table.html", "Chapter 6 Wrangling data to create your first table 6.1 Making Table 1 6.2 Summary 6.3 Flextable 6.4 An exercise, reproduce Table 1 from (Haun et al. 2019)", " Chapter 6 Wrangling data to create your first table Tables can be created as part of a R markdown document. This is because most table generators actually converts your code into an output format based on the rules of that particular format. The most common format to work with when you are building your analysis is html. Tables are special since their creation depend more heavily on the output format compared to other elements of R markdown documents. For example, when using the kable function from the knitr package you have to specify if you want html or latex output. Word output is not possible using the knitr package, but other table generators in rmarkdown (such as flextable) can produce word output, meaning you can put tables in word documents from your R markdown file. There are several alternatives for generating tables17. We will start by exploring the built-in function in the knitr package. The basic work-flow of creating a table in R markdown is to first transform the data into a nice format and then get this underlying data into the table generator. The table generator is written in a code chunk and upon rendering of the R markdown file, the table generator will create, for example, html output. In this chapter we will also introduce some data wrangling tools since the table we will produce consists of summarized data. The functions we will introduce are found in the pakages dplyr and tidyr. These packages are loaded as part of the tidyverse package. 6.0.1 Resources All tidyversepackages are well documented and generally well represented in help forums. Google is your friend when looking for help. The kable function is described in a newly developed book, available online called the R Markdown Cookbook. The package, kableExtra comes with excellent vignettes for both html and pdf outputs. kableExtra provides extra functions to customize your basic knitr table. 6.1 Making Table 1 The first table in many report in sport and exercise studies is the Participant characteristics table. This first table summarizes background information on the participants. We will try to create this table based on data from (Hammarström et al. 2020). These data can be found in the exscidata package. To load the data and other required packages run the following code. library(tidyverse) # for data wrangling library(knitr) # for table creation library(kableExtra) # for extra styling of the table library(exscidata) # the dxadata The end result of this exercise can be found below in Table @(tab:table1-example). This summary table contains the average and standard deviation per group for the variables age, body mass and stature (height) and body fat as a percentage of the body mass. This table is a reproduction of the first part of Table 1 from (Hammarström et al. 2020). Female Male Included Excluded Included Excluded N 18 4 16 3 Age (years) 22 (1.3) 22.9 (1.6) 23.6 (4.1) 24.3 (1.5) Mass (kg) 64.4 (10) 64.6 (9.7) 75.8 (11) 88.2 (22) Stature (cm) 168 (6.9) 166 (7.6) 183 (5.9) 189 (4.6) Body fat (%) 34.1 (5.6) 28.8 (8.7) 20.4 (6) 24.3 (15) We have to make several operations to re-create this table. First we can select the columns we want to work with further from the data set that also contains a lot of other variables. Let us start by looking at the full data set. Below we use the function glmipse from the dplyrtibblepackage (which is also loaded withtidyverse`). data(&quot;dxadata&quot;) glimpse(dxadata) ## Rows: 80 ## Columns: 59 ## $ participant &lt;chr&gt; &quot;FP28&quot;, &quot;FP40&quot;, &quot;FP21&quot;, &quot;FP34&quot;, &quot;FP23&quot;, &quot;FP26&quot;, &quot;FP36&quot;, &quot;FP38&quot;, &quot;FP25&quot;, ... ## $ time &lt;chr&gt; &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;p... ## $ multiple &lt;chr&gt; &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R... ## $ single &lt;chr&gt; &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L... ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;femal... ## $ include &lt;chr&gt; &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;excl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, ... ## $ age &lt;dbl&gt; 24.5, 22.1, 26.8, 23.1, 24.8, 24.2, 20.5, 20.6, 37.4, 22.3, 22.7, 21.3, ... ## $ height &lt;dbl&gt; 170.0, 175.0, 184.0, 164.0, 176.5, 163.0, 158.0, 181.0, 183.0, 178.5, 18... ## $ weight &lt;dbl&gt; 66.5, 64.0, 85.0, 53.0, 68.5, 56.0, 60.5, 83.5, 65.0, 73.5, 69.0, 67.5, ... ## $ BMD.head &lt;dbl&gt; 2.477, 1.916, 2.306, 2.163, 2.108, 2.866, 1.849, 2.216, 1.987, 2.038, 1.... ## $ BMD.arms &lt;dbl&gt; 0.9520, 0.8150, 0.9800, 0.8760, 0.9170, 0.9730, 0.8710, 0.9120, 0.9270, ... ## $ BMD.legs &lt;dbl&gt; 1.430, 1.218, 1.598, 1.256, 1.402, 1.488, 1.372, 1.428, 1.460, 1.489, 1.... ## $ BMD.body &lt;dbl&gt; 1.044, 0.860, 1.060, 0.842, 0.925, 0.984, 0.923, 1.015, 0.949, 1.059, 0.... ## $ BMD.ribs &lt;dbl&gt; 0.770, 0.630, 0.765, 0.636, 0.721, 0.737, 0.648, 0.707, 0.695, 0.789, 0.... ## $ BMD.pelvis &lt;dbl&gt; 1.252, 1.078, 1.314, 1.044, 1.154, 1.221, 1.194, 1.328, 1.204, 1.291, 1.... ## $ BMD.spine &lt;dbl&gt; 1.316, 0.979, 1.293, 0.899, 1.047, 1.089, 1.006, 1.147, 1.013, 1.216, 1.... ## $ BMD.whole &lt;dbl&gt; 1.268, 1.082, 1.325, 1.119, 1.181, 1.350, 1.166, 1.242, 1.210, 1.265, 1.... ## $ fat.left_arm &lt;dbl&gt; 1168, 715, 871, 610, 788, 372, 932, 1312, 388, 668, 510, 947, 371, 1335,... ## $ fat.left_leg &lt;dbl&gt; 4469, 4696, 3467, 3023, 3088, 2100, 4674, 5435, 1873, 2305, 2484, 3971, ... ## $ fat.left_body &lt;dbl&gt; 6280, 4061, 7740, 3638, 6018, 2328, 4896, 9352, 2921, 5251, 2881, 4315, ... ## $ fat.left_whole &lt;dbl&gt; 12365, 9846, 12518, 7565, 10259, 5048, 10736, 16499, 5392, 8525, 6160, 9... ## $ fat.right_arm &lt;dbl&gt; 1205, 769, 871, 610, 741, 374, 940, 1292, 413, 716, 500, 916, 371, 1364,... ## $ fat.right_leg &lt;dbl&gt; 4497, 4900, 3444, 3017, 3254, 2082, 4756, 5455, 1782, 2330, 2456, 3918, ... ## $ fat.right_body &lt;dbl&gt; 6082, 3923, 8172, 3602, 5699, 2144, 4705, 8674, 2640, 5234, 2714, 4072, ... ## $ fat.right_whole &lt;dbl&gt; 12102, 9862, 12856, 7479, 10020, 4821, 10806, 15876, 5048, 8527, 5894, 9... ## $ fat.arms &lt;dbl&gt; 2373, 1484, 1742, 1220, 1529, 747, 1872, 2604, 802, 1384, 1010, 1863, 74... ## $ fat.legs &lt;dbl&gt; 8965, 9596, 6911, 6040, 6342, 4182, 9430, 10890, 3655, 4635, 4940, 7889,... ## $ fat.body &lt;dbl&gt; 12362, 7984, 15912, 7239, 11717, 4472, 9601, 18026, 5561, 10485, 5594, 8... ## $ fat.android &lt;dbl&gt; 1880, 963, 2460, 1203, 1933, 527, 1663, 3183, 1240, 1728, 835, 1025, 111... ## $ fat.gynoid &lt;dbl&gt; 5064, 5032, 4779, 3739, 4087, 2740, 5217, 6278, 2309, 3532, 2668, 4090, ... ## $ fat.whole &lt;dbl&gt; 24467, 19708, 25374, 15044, 20278, 9869, 21542, 32375, 10440, 17051, 120... ## $ lean.left_arm &lt;dbl&gt; 1987, 1931, 2884, 1753, 2652, 2425, 1913, 2266, 3066, 3760, 2991, 2529, ... ## $ lean.left_leg &lt;dbl&gt; 7059, 7190, 10281, 6014, 8242, 7903, 6829, 8889, 9664, 9704, 10086, 8570... ## $ lean.left_body &lt;dbl&gt; 9516, 10693, 13847, 9736, 11387, 10573, 8954, 11482, 13580, 13831, 12876... ## $ lean.left_whole &lt;dbl&gt; 20305, 21778, 29332, 19143, 24185, 22946, 18809, 24318, 28124, 29381, 28... ## $ lean.right_arm &lt;dbl&gt; 2049, 2081, 2888, 1754, 2487, 2439, 1930, 2236, 3253, 4051, 2930, 2447, ... ## $ lean.right_leg &lt;dbl&gt; 7104, 7506, 10200, 6009, 8685, 7841, 6950, 8923, 9198, 9806, 9971, 8459,... ## $ lean.right_body &lt;dbl&gt; 9199, 10304, 14593, 9636, 10779, 9733, 8602, 10672, 12288, 13789, 12125,... ## $ lean.right_whole &lt;dbl&gt; 19605, 21310, 29643, 18792, 23653, 21837, 19407, 23727, 26586, 29359, 26... ## $ lean.arms &lt;dbl&gt; 4036, 4012, 5773, 3508, 5139, 4864, 3843, 4501, 6319, 7811, 5921, 4975, ... ## $ lean.legs &lt;dbl&gt; 14163, 14696, 20482, 12023, 16928, 15744, 13779, 17812, 18862, 19509, 20... ## $ lean.body &lt;dbl&gt; 18715, 20998, 28440, 19372, 22166, 20306, 17556, 22155, 25868, 27621, 25... ## $ lean.android &lt;dbl&gt; 2669, 2782, 3810, 2455, 2904, 2656, 2297, 3094, 3344, 3644, 3144, 3073, ... ## $ lean.gynoid &lt;dbl&gt; 6219, 7209, 10233, 5866, 7525, 5970, 5825, 8175, 7760, 9646, 8776, 6966,... ## $ lean.whole &lt;dbl&gt; 39910, 43088, 58976, 37934, 47837, 44783, 38216, 48045, 54710, 58740, 55... ## $ BMC.left_arm &lt;dbl&gt; 181, 138, 204, 144, 180, 173, 140, 173, 220, 226, 225, 179, 287, 167, 25... ## $ BMC.left_leg &lt;dbl&gt; 567, 508, 728, 441, 562, 574, 482, 631, 633, 630, 672, 631, 880, 482, 70... ## $ BMC.left_body &lt;dbl&gt; 622, 414, 696, 367, 526, 465, 370, 629, 473, 629, 509, 513, 677, 397, 75... ## $ BMC.left_whole &lt;dbl&gt; 1680, 1321, 1945, 1201, 1527, 1580, 1131, 1688, 1544, 1750, 1671, 1581, ... ## $ BMC.right_arm &lt;dbl&gt; 198, 150, 210, 142, 176, 183, 140, 176, 224, 251, 226, 167, 295, 166, 26... ## $ BMC.right_leg &lt;dbl&gt; 574, 514, 739, 431, 552, 565, 491, 641, 622, 636, 690, 652, 874, 478, 71... ## $ BMC.right_body &lt;dbl&gt; 592, 428, 730, 351, 502, 409, 358, 582, 420, 616, 483, 450, 688, 379, 67... ## $ BMC.right_whole &lt;dbl&gt; 1582, 1288, 1958, 1130, 1451, 1466, 1229, 1668, 1478, 1720, 1600, 1507, ... ## $ BMC.arms &lt;dbl&gt; 379, 288, 414, 285, 356, 357, 280, 348, 444, 478, 451, 346, 582, 333, 51... ## $ BMC.legs &lt;dbl&gt; 1142, 1022, 1467, 872, 1115, 1139, 974, 1272, 1255, 1266, 1362, 1283, 17... ## $ BMC.body &lt;dbl&gt; 1214, 842, 1426, 718, 1028, 874, 728, 1211, 893, 1245, 992, 964, 1365, 7... ## $ BMC.android &lt;dbl&gt; 80, 57, 90, 44, 56, 54, 43, 77, 52, 72, 59, 60, 65, 52, 87, 44, 43, 41, ... ## $ BMC.gynoid &lt;dbl&gt; 314, 285, 427, 245, 299, 262, 241, 379, 335, 378, 332, 307, 441, 247, 43... ## $ BMC.whole &lt;dbl&gt; 3261, 2609, 3903, 2331, 2978, 3046, 2360, 3356, 3022, 3469, 3271, 3088, ... We can see that we got 80 rows and 59 columns in the dataset. The columns of interest to us are: participant time sex include age height weight fat.whole For a full description of the data set, you can type ?dxadata in your console. The participant columns is good to have to keep track of the dataset in the beginning. You might want to have this information to perform sanity checks on some results. time is needed to remove some observation that are not needed. This pre-training table only sums up information from before the intervention starts. sex is a grouping variable together with include, Table 1 in (Hammarström et al. 2020) uses Sex and Inclusion in data analysis as grouping for descriptive data. The other variables are used to describe the data sample. 6.1.1 The pipe operator and select As mentioned above, we will start by selecting the variables we want to work further with. Using the select function from dplyr we can select columns that we need. In the code below I will use select as part of a pipe. Think of the pipe as doing operations in sequel. Each time you use the pipe operator (%&gt;%) you say then do. The code below translates to: Take the dataset dxadata, then do select the following variables, then do print print, is a function that outputs the results of the operations. In each new function of a pipe, the data that we take with us from the above line ends up as the first argument. A representation of this behavior can be expressed as: DATA %&gt;% FUNCTION(DATA, ARGUMENTS) %&gt;% FUNCTION(DATA, ARGUMENTS) %&gt;% FUNCTION(DATA) We do not need to type the data part, instead the pipe operator (%&gt;%) gathers the data from each step and puts it in the subsequent function. Copy the code below to your own script or Rmarkdown document and run it. If you use a Rmarkdown document (.Rmd), you might want to set Chunk output in console in the settings menu. In my experience, this makes developing code a bit faster. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole print() # print the output Notice that I have added short comments after each line to make it clear what I want to accomplish. We will build further on the above code, and this is a common work-flow. Using pipes, it is easy to extend the code by adding lines doing certain operations, one at the time. Notice also that the select function uses a list of variable names with include:weight being short for \"take all variables from include to weight. 6.1.2 Filter observations The next step will be to filter observations. We need to remove the observations that comes from the post-intervention tests. The time variable contains to values pre and post to remove post-values we need to tell R to remove these all observations (rows) containing post. We will use the filter function from dplyr. This will be our first experience with logical operators. Lets try out to alternatives, copy the code to your console to see the results. ## Alternative 1: ## dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter away all observation with &quot;post&quot; filter(time != &quot;post&quot;) %&gt;% print() # print the output ## Alternative 2: ## dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% print() # print the output The above code should give the same output. The operator != says not equal to, the operator == says equal to. Notice that R uses two equal signs to say equal to. A single equal sign is used as an assignment operator in R. 6.1.3 Create or change variables The next problem for us is that we need to manipulate or combine information from two variables in order to calculate body fat percentage. The formula that we will use is simply expressing body fat as a percentage of the body weight. \\[Body~fat~(\\%) = \\frac{Body~fat~(g)/1000}{Body~weight~(kg)} \\times 100\\] By using the mutate function we can add or manipulate existing variables in a pipe. Mutate takes as arguments a list of new variables: dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% print() # print the output In the code above, we replace the variable fat.whole with the re-calculated variable. 6.1.4 Grouped operations and summary statistics In a pipe, we can group the data set giving us opportunities to calculate summary statistics over one or several grouping variables. In Table 1 in (Hammarström et al. 2020), include and sex are the two grouping variables. Using the group_by() function from dplyr sets a grouping of the data frame. If we use functions that summarizes data, such summarizes will be per group. In Table 1 in (Hammarström et al. 2020) the number of participants in each group are specified. We can use the function n() to calculate the number of observations per group in a mutate call. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% print() # print the output The new variable n now contains the number of observations in each group. For now we can regard this as a new variable. Each participant belongs to a specified group, and this specific group has n number of members. We can now go further and use the summarise function. Instead of adding variables to the existing data set, summarize reduces the data set using some summarizing function, such as mean() or sd(). These estimates are what we are looking for in our data set. Example of other summarizing functions for descriptive data are min() and max() for the minimum and maximum. We can use the summarise() function to calculate the mean and standard deviation for the weight variable. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Summarise weight summarise(weight.m = mean(weight), weight.s = sd(weight)) %&gt;% print() # print the output Try out the code in your own script. The above example gives us what we want, however, it means that we need to type a lot. Instead of needing to make a summary for each variable, we can combine the variables in a long format. To get to the long format we will use the pivot_longer() function. This function gathers several variables into two columns, one with the variables names as values and a second column with each value from the original variables. In our case we want to gather the variables age, height, weight, fat.whole and n. I will call the new variables that we create variable and value. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% print() The cols = age:n part of pivot_longer specifies what columns to gather. The dataset is still grouped by include and sex. We may now proceed by summarizing over these groups, however, we need to add another group to specify that we want differnt numbers per variable. To do this we re-specify the grouping. After this we add the summarize function. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% print() If you run the above code you will notice that the the standard deviation of each variable is larger than zero except for n which has no variability. This is because we created it per group and simply calculated it as the sum of observations. Take a look at Table 1 in (Hammarström et al. 2020). The format of the descriptive statistics are mean (SD), this is a preferred way of reporting these estimates. In order to achieve this we need to manually convert the numbers. In the example below, I will start by making a new variable by simply pasting the numbers together. I will also add the brackets. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = paste0(m, &quot; (&quot;, s, &quot;)&quot;)) print() In mutate(ms = paste0(m, \" (\", s, \")\")), the paste0 function simply paste components together to form a string of text. First, the vector of means are being used, then we add a parenthesis, followed by the SD and finally a parenthesis. If you run the above code you will notice that you end up with numbers looking like this: 167.666666666667 (6.86851298231541) This is neither good or good looking. We have to take care of the decimal places. There are a number of ways to do this but in this case the function signif seems to make the situation better. signif rounds to significant digits. This means that we will get different rounding dependning on the size of the value. I find signif(m, 3) to be agood starting point. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)) %&gt;% print() Things are starting to look good. Run the code, what do you think. A problem with the above is that we do not want any dispersion/variability in the n variable. So if the variable is nwe do not want that kind of formatting. It is time to add a conditional statment. In dplyr there are easy-to-use if/else functions. The function if_else sets a condition, if this is met then we can decide what to do, and likewise decide what to do if it is not met. This looks something like this inside a dplyr pipe: ... %&gt;% if_else(IF_THIS_IS_TRUE, THE_DO_THIS, OTHERWISE_DO_THIS) %&gt;% print() If variable is n, then we only want to display m otherwise we want the full code as described above: paste0(signif(m, 3), \" (\", signif(s, 3), \")\"). We add this to the code: dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;))) %&gt;% print() The as.character part is needed because the output of if_else must be the same regardless of what the outcome of the test is. We are getting close to something! The next step is to remove variables that we do not longer need. The select function will help us with that. we can remove mand s by select(-m, -s), the minus sign tells are to remove them from the list of variables in the dataset. We can then combine the grouping variables into a include_sex variable. Similarly to what we did above, we can simply paste them together. Now we will use the paste (function instead of paste0). In paste we specify a separator, maybe _ is a nice alternative. Selecting away the individual variables from the new combined one leaves us with this code and data set. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) print() If ungroup is not used, we cannot select away variables since they are used to group the dataset. We will now perform the last operations befor we can make it a table. To make it formatted as in Table 1 in (Hammarström et al. 2020), we can make the present dataset wider. Each group has its own variable in addition to the variable name column. We will use the opposite function to pivoy_longer, namely pivot_wider18. pivot_wider takes a variable or key column and a values column and divide the values based on the key. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% print() A final step is to format the variable variable(!). The easiest is to make it a factor variable with specified levels and names. After we have added this information, we can use arrange to sort the dataset. Using select will help you sort the columns to match wath we want. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% arrange(variable) %&gt;% print() 6.1.5 Starting the table generator - The kable function The next step is to pipe everything into the table generator. kable from the knitr package creates the framework for the table we are going to create. Next we will use functions from the kableExtra package to customize the table. Use the help pages for kable to see what can be specified in the function. We will make use of only a few arguments. kable can be part of a pipe, the same as the following kableExtra functions. The kablefunction is well described in the Rmarkdown cookbook. For functions belonging to the kableExtra package, see the vignettes. We will add the kable function to the end of the pipe we have built so far. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;) An important aspect is that we need to work in a RMarkdown document (.Rmd). This allows for conversion of html code generated by kable to the output file, if the output file format is html. In kable, as you can see in the example above, we specify this with the argument format =. Possible common formats are html and latex. \\(\\LaTeX\\) is a more advanced syntax for document preparation, the output is often a pdf file. In this session we are working with html, so we will keep format = \"html\". When knitting your .Rmd-document, the code chunk where you keep your table code needs to have a setting specifying results = \"asis\". The top of the code chunk should look like this: ```{r my_table, results=&quot;asis&quot;} dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% ... ``` When you run the above code, you may get output in your console. This output will look something like this: &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align:left;&quot;&gt; variable &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; excl_female &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; excl_male &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; incl_female &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; incl_male &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; N &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 4 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 3 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 18 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 16 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Age (years) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 22.9 (1.57) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 24.3 (1.46) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 22 (1.25) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 23.6 (4.11) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Mass (kg) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 64.6 (9.71) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 88.2 (22.4) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 64.4 (10.4) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 75.8 (10.7) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Stature (cm) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 166 (7.59) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 189 (4.58) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 168 (6.87) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 183 (5.88) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Body fat (%) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 28.8 (8.69) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 24.3 (15.3) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 34.1 (5.64) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 20.4 (5.99) &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; This is html code. For some reason, RStudio will not interpret it as html-code and preview it in the Viewer pane, yet. We will now add some information to our table. First we want to change the headings. In kable we do this by setting the col.names argument. We do not need to repeat ourselves adding Female and Male to columns 2 and 3, and 4 and 5, respectively. Instead we specify the Include/Exclude grouping. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;)) Notice that I added a final select function to arrange the columns of the table. The col.names= argeumnet is specified in the same order. But how to separate male from females. This is where kableExtra comes in. This package provides functions for customizing the simple kable table. Using the function add_header_above we can specify in a named character vector what headers we want over a certain column span. We do not want any information in the first column, but we want Female to span two columns followed by Male spanning two columns. The resulting code will look like this: add_header_above(c(\" \" = 1, \"Female\" = 2, \"Male\", = 2)). Adding this code to our pipe (for some reason at least in my RStudio) puts the table output in the viewer pane! dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;)) %&gt;% add_header_above(c(&quot; &quot;= 1, &quot;Female&quot; = 2, &quot;Male&quot; = 2)) The table is almost done! We need to add some last information. Maybe a caption is in place and a footnote describing what the data represents. The caption is added in the kable function using caption = \"Table 1. Participant characteristics\". The footnote is added using a function from kableExtra, `add dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;), caption = &quot;Table 1. Participant characteristics.&quot;) %&gt;% add_header_above(c(&quot; &quot;= 1, &quot;Female&quot; = 2, &quot;Male&quot; = 2)) %&gt;% add_footnote(label = &quot;Data are means and standard deviations (SD).&quot;, notation = &quot;none&quot;) Success! The table is complete. As with alot of things in R, there are at least five ways of getting to this end result, and there still might be some things to tweak to make it look even more like Table 1 in (Hammarström et al. 2020). 6.2 Summary In the example above we have introduced functions from dplyr, from the dplyr documentation: mutate() adds new variables that are functions of existing variables select() picks variables based on their names. filter() picks cases based on their values. summarise() reduces multiple values down to a single summary. arrange() changes the ordering of the rows. We have also explored pivot_wider and pivot_longer from the tidyr package. We used the above functions to prepare a table. The table was then made pretty by using kable from the knitr package. Functions from kableExtra gave us ways of making the table more suited to what we wanted to show. The vignette describining the kableExtra package is highly recommended! If you want to extend to pdf-tables (which are really nice!). Use the \\(\\LaTeX\\) vignette found here. When creating pdf/\\(\\LaTeX\\) tables/reports some basic understandning of \\(\\LaTeX\\) is good. 6.3 Flextable Flextable comes with the option of specifying tables directly in word outputs. This does not need any further formatting compared to HTML formatting. Lets use the the same pipe as used above to specify the flextable. Flextable also works as part of a pipe. The basic function is flextable. library(flextable) dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() We can now add changes to the underlying table and add new header labels to produce a table similar to the one above. library(flextable) dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() %&gt;% set_header_labels(variable = &quot;&quot;, incl_female = &quot;Include&quot;, excl_female = &quot;Exclude&quot;, incl_male = &quot;Include&quot;, excl_male = &quot;Exclude&quot;) Note that flextable does not need results = \"asis\" to work in rmarkdown files. Similar to kable, flextable can give a preview by just running the code, meaning you do not have to knit the whole document to see what changes you have done. Lets finnish up by adding a header and a footnote. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() %&gt;% set_header_labels(variable = &quot;&quot;, incl_female = &quot;Include&quot;, excl_female = &quot;Exclude&quot;, incl_male = &quot;Include&quot;, excl_male = &quot;Exclude&quot;) %&gt;% # Adds a header specified for all columns of the table add_header_row(values = c(&quot;&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;)) %&gt;% # Merge the new cells with the same data # part = &quot;header&quot; meands that we want to add a row in the &quot;header&quot; part of the table. # i = 1 means row 1 # j = 2:3 means column 2 to 3 merge_at(part = &quot;header&quot;, i = 1, j = 2:3) %&gt;% merge_at(part = &quot;header&quot;, i = 1, j = 4:5) %&gt;% # Add footnote add_footer_row(values = &quot;Values are mean and (SD)&quot;, colwidths = 5) %&gt;% # Make the columns widths match the content autofit() Nice! This table should also print if the code chunk is included in a R Markdown file and the output is choosen to be Knit to word. See flextable documentation for more customizations. 6.4 An exercise, reproduce Table 1 from (Haun et al. 2019) In total 30 college students performed a heavy resistance training protocol where training volume was constantly increased over six weeks. In (Haun et al. 2018), a part of the study, focusing on supplementation was reported. In (Haun et al. 2019), participants were devided into two clusters based on training responses and the authors aimed to answer the question what separates high- from low-responders to resistance training. In this exercise we want to reproduce a big part of Table 1 in (Haun et al. 2019). The Table as re-produced here can be seen below. See the orginal article for explenation of clusters. To select variables, see the data description in the exscidata package, the dataset is called hypertrophy. Table 6.1: Table 1. Baseline characteristics at PRE and back squat training volume between clusters Variable LOW (n = 10) HIGH (n = 10) Age (years) 20.9 (1.9) 21.5 (1) Training age (years) 5.5 (2.3) 5.5 (2) Body mass (kg) 78.8 (8) 83.1 (12.8) DXA LBM (kg) 62.2 (5.9) 65.1 (9.7) DXA FM (kg) 13.5 (4.9) 14.5 (4.9) Type II fiber (%) 59.4 (16.9) 50.2 (13.6) 3RM back squat (kg) 127 (23.3) 135.4 (14.1) Total back squat training volume (kg) 106610.2 (18679.4) 111820.8 (12962.5) Click for a possible solution # load the data data(hypertrophy) hypertrophy %&gt;% # Select the variables needed to reproduce the table dplyr::select(PARTICIPANT, GROUP, CLUSTER, AGE, BODYMASS_T1, TRAINING_AGE, PERCENT_TYPE_II_T1, SQUAT_3RM, DXA_LBM_T1, DXA_FM_T1, SQUAT_VOLUME) %&gt;% # Pivot longer to gather all variables pivot_longer(cols = AGE:SQUAT_VOLUME, names_to = &quot;variable&quot;, values_to = &quot;values&quot;) %&gt;% # Remove participants not belonging to a cluster filter(!is.na(CLUSTER)) %&gt;% # Create a grouping before summarizing group_by(CLUSTER, variable) %&gt;% summarise(m = mean(values), s = sd(values)) %&gt;% # For nice printing, paste mean and SD mutate(m.s = paste0(round(m,1), &quot; (&quot;, round(s,1), &quot;)&quot;)) %&gt;% # Select only variables needed for the table select(CLUSTER, variable, m.s) %&gt;% # Transform the data set to a wide format based on clusters pivot_wider(names_from = CLUSTER, values_from = m.s) %&gt;% # Re-arrange the &quot;variable&quot; variable, correct order with levels, and correct labels mutate(variable = factor(variable, levels = c(&quot;AGE&quot;, &quot;TRAINING_AGE&quot;, &quot;BODYMASS_T1&quot;, &quot;DXA_LBM_T1&quot;, &quot;DXA_FM_T1&quot;, &quot;PERCENT_TYPE_II_T1&quot;, &quot;SQUAT_3RM&quot;, &quot;SQUAT_VOLUME&quot;), labels = c(&quot;Age (years)&quot;, &quot;Training age (years)&quot;, &quot;Body mass (kg)&quot;, &quot;DXA LBM (kg)&quot;, &quot;DXA FM (kg)&quot;, &quot;Type II fiber (%)&quot;, &quot;3RM back squat (kg)&quot;, &quot;Total back squat training volume (kg)&quot;))) %&gt;% # Sort/order the dataset arrange(variable) %&gt;% # Use kable to output the table with appropriate caption and column names. kable(col.names = c(&quot;Variable&quot;, &quot;LOW (n = 10)&quot;, &quot;HIGH (n = 10)&quot;), caption = &quot;Table 1. Baseline characteristics at PRE and back squat training volume between clusters&quot;) See for example the description of the gt package that list a bunch of table generators. All this talk about pivot, take a break and watch this clip from the hit series Friends, its about pivot! "],["writing-your-first-reproducible-report.html", "Chapter 7 Writing your first reproducible report 7.1 RStudio projects and your reproducible report", " Chapter 7 Writing your first reproducible report As we have already discussed, the degree to which research is reproducible is to a degree determined by the availability of: The data Code to analyze the data To make these ingredients even more tasty, we might want to have them nicely stored together. A way of doing this using the tools we discuss in this course is to think of data analysis projects as self-contained projects with all necessary ingredients. In RStudio, projects can help you organize your data and code in one place. You can also link your project to an online repository for others to access. In this chapter we will discuss the reproducible report as belonging to such a project. The online repository/sharing part will be discussed in the next chapter. 7.1 RStudio projects and your reproducible report When you build an analysis in a R markdown file, R will use the folder that the file is in as the root directory. This directory (or folder) is the top directory in a file system. This means that R will look for data or other files used to generate the report in this folder structure. Think of this folder as ./ (confusing, I know! But bare with me!). Any sub-folders to the root directory can be called things like ./data/ (a folder where you keep data files), ./figures/ (a folder where you output figures from analyses). The R markdown file, being in the root directory will have the address ./my_rmarkdown_file.Rmd This has several advantages, as long as you stick to one rule: When doing an analysis, always use relative paths (addresses to files and folders). Never reference a folder or file by their absolute path. The absolute path for the file Im writing in now is C:/Users/706194/Dropbox/Undervisning%20och%20kurser%20HIL/IDR3002%20Course%20notes/IDR3002/markdown.Rmd. The relative path is ./markdown.Rmd. When working in a project you may move the folder containing your project to other locations, but relative paths will not break. If you want to share your analysis, all you need to do is share the folder with all content with your friend. If you use relative paths, everything will work on your friends computer. If you use absolute paths, nothing will work, unless your friends computer uses a very similar folder structure (highly unlikely). RStudio projects makes it easy to jump back and forth between projects. The project menu (top right corner in RStudio) contains all your recent projects. When starting a new project, R will create a .Rproj file that contains the settings for your project. If you start a project a click this file, a settings menu will appear where you can customize settings for your particular project. What does this have to do with my RMarkdown file? As mentioned above, the RMarkdown file is often written in a context where you have data and other files that help you create your desired output. By always working in a project makes it easy to keep every file in the right place. "],["version-control-and-collaborative-coding-introducing-git-and-github-com.html", "Chapter 8 Version control and collaborative coding  introducing git and github.com", " Chapter 8 Version control and collaborative coding  introducing git and github.com In the previous chapter we underlined the importance of the project as a way of keeping data, code (and text) in an organized manner. "],["reliability-in-the-physiology-lab.html", "Chapter 9 Reliability in the physiology lab", " Chapter 9 Reliability in the physiology lab "],["assignment1.html", "Chapter 10 Assignment 1: Reliability and tools for reproducible data science 10.1 Elements of the report 10.2 The format of the report 10.3 How to hand in the report.", " Chapter 10 Assignment 1: Reliability and tools for reproducible data science The purpose of this assignment is to present estimates of reliability of measures collected in the physiology lab. A second purpose is to use tools for reproducible data science. The report that you are expected to hand in therefore has some strict requirements in its format (see below). The assignment is a group assignment and at least three students are expected to contribute to each report. 10.1 Elements of the report The report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refferes to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner, Sport, and Gore 2012) The next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature. Finally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you. 10.2 The format of the report The report should be uploaded to github with both a raw .Rmd file and a .html file as output. The github folder should also contain the datset being used in the calculations. Contributions from members of the group can be made directly to the github directory. Follow this checklist on how to get started: (A nice introduction to github with R can be found here) Create a github account. All members of the group should have their own account. Once signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account. Be sure to download and install git on your computer. Start up RStudio, start a new project, select the Version Control option and copy the address to the repository. Add files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal: git add -A git commit -m &quot;A short message to describe your changes&quot; git push When you want to download the latest version of your project, type in the terminal: git pull You may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved Have patience! 10.3 How to hand in the report. Copy the link to your github folder into canvas. "],["introduction-to-the-molecular-exercise-physiology-lab.html", "Chapter 11 Introduction to the molecular exercise physiology lab", " Chapter 11 Introduction to the molecular exercise physiology lab "],["references.html", "Chapter 12 References", " Chapter 12 References Broman, Karl W., and Kara H. Woo. 2018. Data Organization in Spreadsheets. Journal Article. The American Statistician 72 (1): 210. https://doi.org/10.1080/00031305.2017.1375989. Ellefsen, S., D. Hammarstrom, T. A. Strand, E. Zacharoff, J. E. Whist, I. Rauk, H. Nygaard, et al. 2015. Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training. Am. J. Physiol. Regul. Integr. Comp. Physiol. 309 (7): R767779. Halperin, I., D. B. Pyne, and D. T. Martin. 2015. Threats to Internal Validity in Exercise Science: A Review of Overlooked Confounding Variables. Journal Article. Int J Sports Physiol Perform 10 (7): 82329. https://doi.org/10.1123/ijspp.2014-0566. Hammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis. Journal Article. The Journal of Physiology 598 (3): 54365. https://doi.org/10.1113/JP278455. Haun, C. T., C. G. Vann, C. B. Mobley, P. A. Roberson, S. C. Osburn, H. M. Holmes, P. M. Mumford, et al. 2018. Effects of Graded Whey Supplementation During Extreme-Volume Resistance Training. Journal Article. Front Nutr 5: 84. https://doi.org/10.3389/fnut.2018.00084. Haun, C. T., C G. Vann, C. Brooks Mobley, Shelby C. Osburn, Petey W. Mumford, Paul A. Roberson, Matthew A. Romero, et al. 2019. Pre-Training Skeletal Muscle Fiber Size and Predominant Fiber Type Best Predict Hypertrophic Responses to 6 Weeks of Resistance Training in Previously Trained Young Men. Journal Article. Frontiers in Physiology 10 (297). https://doi.org/10.3389/fphys.2019.00297. Hopkins, W. G. 2000. Measures of Reliability in Sports Medicine and Science. Journal Article. Sports Med 30 (1): 115. http://www.ncbi.nlm.nih.gov/pubmed/10907753. Ioannidis, John P. A. 2005. Why Most Published Research Findings Are False. Journal Article. PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Leek, J. T., and R. D. Peng. 2015. Statistics: P Values Are Just the Tip of the Iceberg. Journal Article. Nature 520 (7549): 612. https://doi.org/10.1038/520612a. Peng, R. D., F. Dominici, and S. L. Zeger. 2006. Reproducible Epidemiologic Research. Journal Article. Am J Epidemiol 163 (9): 78389. https://doi.org/10.1093/aje/kwj093. Spiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books. Stephen, G. Powell, R. Baker Kenneth, and Lawson Barry. 2009. Errors in Operational Spreadsheets. Journal Article. Journal of Organizational and End User Computing (JOEUC) 21 (3): 2436. https://doi.org/10.4018/joeuc.2009070102. Tanner, R. K., Australian Institute of Sport, and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC. Wickham, Hadley. 2014. Tidy Data. Journal Article. Journal of Statistical Software; Vol 1, Issue 10 (2014). https://www.jstatsoft.org/v059/i10. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; OReilly Media. http://r4ds.had.co.nz/. Ziemann, Mark, Yotam Eren, and Assam El-Osta. 2016. Gene Name Errors Are Widespread in the Scientific Literature. Journal Article. Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7. "]]

[["index.html", "Quantitative methods and statistics (In Sport and Exercise Science) Chapter 1 Introduction 1.1 Quick links: 1.2 Practical information 1.3 Assignments and Portfolio exam 1.4 Other information", " Quantitative methods and statistics (In Sport and Exercise Science) Daniel Hammarström Updated: 2021-09-17 Chapter 1 Introduction Welcome to the course Quantitative methods and Statistics (IDR4000). The course aims to give students an overview of methodological aspects within the field of sport and exercise-physiology. Specifically, planning, conducting and analyzing research projects with human participants will be covered. These course notes covers almost the entire course through the combination of video lectures, tutorials and references to the course literature and external resources. 1.1 Quick links: Study questions 1.2 Practical information 1.2.1 Learning objectives Learning objectives can be read in Norwegian here. 1.2.2 Learning strategies The course will include lectures, laboratory exercises, computer exercises, seminars and student presentations. Lectures will be held on-line (zoom), as pre-recorded in this book and in-person on campus. Due to the current pandemic, you are required to do laboratory exercises in your cohort. Computer exercises require that you have special computer software installed on your computer. The software is free (see specific chapters in these course notes). Assignments will be presented in this text with information on how to hand them in. The whole course is evaluated based on a portfolio (see below). 1.2.3 Course evaluation As a student you can contribute to the quality of the course by engaging in course evaluation throughout the course. You will be asked to answer a pre-course questionnaire about your expectations and a post-course questionnaire about your experiences. You are also welcomed to take part in systematic discussions during the course about the quality of teaching and course material. With these notes I want to underline the importance of student participation in the continuous development of the course (and program) teaching/learning qualities. 1.2.4 Lecturers and course administration In order of appearance Daniel Hammarström (daniel.hammarstrom@inn.no), is responsible for course administration and will be teaching statistics and molecular methods. Kristian Lian, Ingvill Odden and Lars Nymoen will act as teacher assistants in organizing methods in the physiology lab. Stein Olaf Olsen will act as a teacher assistant in the molecular lab. Prof. Carten Lundby will cover aspects CO2 re-breathing techniques (physiology). Prof. Finnur Dellsén will cover philosophy of science. Prof. Stian Ellefsen will teach molecular methods. 1.2.5 Updates, notifications and general communication These course notes will be updated during the course. General information and last minute changes will be posted on Canvas, make sure to check it as part of your daily study routine. 1.2.6 Literature A full list of recommended literature can be found here. Literature will be referenced in specific sections in these course notes. 1.2.7 Grades The course is graded pass/fail. 1.2.8 Language My (Daniel) first language is Swedish, Im sure most of you will understand what Im talking about. However, due to the fact that we accept international students to the program, most written communication and some lectures will be in English. You are not expected to write in English, it is however possible! 1.3 Assignments and Portfolio exam The course is based on several assignments. Some of these assignments are to be handed in as part of a portfolio exam upon which your grade is based. Assignments that are due during the course (arbeidskrav) are expected to be further improved after feedback from fellow students and teachers before inclusion in your portfolio. The table below shows all assignments that are part of the course. Some are not to be included in the portfolio and some assignments are group assignments (see Table). Assignment Due date Included in portfolio Group assignment Descriptive statistics, reliability and validity, tools for reproducible data science (See Chapter 10) 2021-09-10 Yes Yes Regression models and prediction from data (See Chapter 16) 2021-10-01 No Yes Extraction and analysis of DNA 2021-10-15 Optionala Yes Extraction of RNA and analysis of qPCR experiments 2021-10- Optionala Yes Extraction and analysis of Protein 2021-10- Optionala Yes Study designs 2021-10-01 Yes No |Drawing inference from statistical models|2021-10- |No|Yes| |Statistical power and sample size calculations|2021-11-|Yes|Yes| |Analyzing repeated measures experiments|2021-11-|Yes|No| |Philosophy of scienceb|2021-11- |Yes|No| a Select one laboratory assignments for your portfolio exam. b This assignment is presented in connection with lectures. In addition to arbeidskrav/assignments, you are required to contribute to the course wiki. The wiki page is hosted at github.com/dhammarstrom/IDR4000-2021/. In order to contribute you need to set up your own github account. The language of the wiki should be Norwegian. Smaller assignments and quizzes are presented in this book, but you are not required to do them to pass the course. 1.4 Other information "],["introduction-to-data-science.html", "Chapter 2 Introduction to data science 2.1 About data in the world of sport and exercise 2.2 Replication and Reproducibility 2.3 Tools in data science", " Chapter 2 Introduction to data science 2.1 About data in the world of sport and exercise Data is everywhere. Most of us walk around with a data collection device in our pockets all the time. This device (your mobile phone), records and store data about you all throughout the day. Such data are the basis of the quantified self movement1 that have grown in popularity as capabilities to record data from daily life has become better. People interested in quantifying their personal life does so for different reasons, but often with the intent to improve their health2. Much of these kind of data are readily available to us due to the fact we are protected by data privacy policies and regarded as personal data3. With some effort you yourself can get your data out of your iphone to explore, for example, your daily step count. I discovered that my phone(s) has been collecting data for me since 2016 and I tend to walk less steps on Sundays compared to Saturdays (see Figure 2.1). Figure 2.1: Step count data from my iPhone displayed as all avalable data points (A, after data cleaning) and average step per weekday, per year and season (B). Data are also collected and stored in publicly available databases. Such databases are created for the purpose to store specific types of data, such as soccer4 or biathlon results5, or biological information such as gene sequences6. Even data from scientific studies are now days often publicly available7 meaning that we can perform scientific studies on unique data sets without collecting the data ourselves. The above examples shows that there are abundance of data around and available to us. The problem is that it is hard understand all this data. This is where data science and data literacy comes in. In the world of sport and exercise, regardless if you are interested in doing scientific investigations, coach a soccer-team or individual athletes or help patients recover from surgery using exercise therapy, you are faced with the problem of handling and make sense of data. Some of the key skills and deeper understanding about data science are very much transferable between such areas of practice. Think about the literature! Spiegelhalter (The Art of Statistics, in the introduction chapter) talks about how statistics has evolved towards the broader field of data science. In data science, statistical theory and methods are just parts of the problem solving cycle. Try to think about how you would use the PPDAC cycle as a exercise coach and a scientist. What are the similarities and differences? One broader aim of this course is for you to develop skills to better understand data. 2.2 Replication and Reproducibility In scientific research, replication is a way to confirm scientific claims. When a result can be confirmed by an independent group of researchers, the claim is likely more true. Many results will however never be possible to replicate due to the size of trials, costs and urgency of the research question. A recent example could perhaps be the many vaccine trials performed to develop a vaccines against COVID-198. Other examples concern studies with unique study populations, such as large scale epidemiological studies (Peng, Dominici, and Zeger 2006), but the same could be said to be true for unique investigations in sport and exercise science. When studies are not likely to be replicated, reproducibility of the analyses and results has been suggested to be a minimum standard for scientific studies. Reproducibility means that given the same datas, similar results or conclusions can be drawn by independent researchers (Peng, Dominici, and Zeger 2006). Peng et al. (Peng, Dominici, and Zeger 2006) suggests that a fully reproducible study has Available data. Computer code (software) that produces the results of the study. Documentation that describes the software and data used in the study, and ways to share the data and code. The above principally relates to the trust we can place in scientific results. However, the minimum standard of reproducibility has advantages also for the individual researcher (or master student)! When working with reproducible methods we will develop ways of documenting and automating our analyses. This will make it easier to collaborate with others. And, as it turns out, your most frequent collaborator is you, in the future! A reproducible data analysis means that you will make it explicit and transparent. In a traditional data analysis, most activities are in the black box. In order to avoid bias (Ioannidis 2005), the black box needs to be opened and you need to actively make transparent decisions all along the analytic pipeline (Leek and Peng 2015). This pipeline preferably involves the whole problem solving cycle described by Spiegelhalter (Spiegelhalter 2019). However the tools that we will learn about in this course focuses primarily on the steps from the experimental design to presentation of statistical results (Leek and Peng 2015). These steps includes data collection (and storage), data cleaning, exploratory data analysis, statistical modelling and statistical inference (and communication) (Leek and Peng 2015). 2.3 Tools in data science Ways to interpret and make sense of data involves different methods. These methods are now days often implemented in computer software. This means that when you as a practitioner (scientist, coach, analyst ) want to understand data, you have to master some kind of computer software. The most common software used to understand data is probably Microsofts Excel. You can do amazing stuff with Excel! In the world of sport and exercise Excel has been used in such diverse activities such as scientific investigations, planning and recording training for Olympic medalists9 and scheduling appointments. For scientific research, most people use additional software to do statistical analyses. If you have spent time in higher education you have probably heard about SPSS, Stata or Jamovi. These are all specialized software used for statistical analyses. The above mentioned tools can all be used as part of a fully reproducible workflow. However, there are software solutions that actually suits this requirement better than others. Going back to the description of reproducible science as made by Peng et al. (Peng, Dominici, and Zeger 2006), we want software where analyses can be Human- and computer-readable, meaning that we want to be able to write scripts, or computer programs that execute the analyses. Documented, meaning that along the code we want to be able to describe what the code does. Available and able to share with other, meaning that we analyses can be run on open and free software to maximize ability to share them. This means that the software that we would prefer should be run using scripts (as opposed to point and click) and be free of charge (and open source, as opposed to expensive and proprietary). These criteria can be fulfilled when we use software that is written around the R language (although alternatives exists10). R is a computer language that is especially well suited for reproducible data analysis. As users are able to contribute software extensions, also called packages, many specialized software implementation exists for different tasks, such as creating figures or analyses of specific data. Around R, people have been developing auxiliary software to enable reproducible data analysis. The negative part of all these opportunities is that using R requires some effort. The learning curve is steep! Even though you might not use R ever again after this course, making and effort trying to learn it will let you know something about programming, capabilities of modern data science, statistical analysis and software/computers in general. These areas are all part of our modern society and are very much transferrable regardless of what computer language we are talking about. In a following chapter of these course notes we will go through installing and starting up R. Cheat sheets are available in R Studio: Help &gt; Cheatsheets Make sure to look through the installation instructions to get pdf options working See e.g. Apples Privacy Policy. understat.com stores match specific data from major leagues. Data are available through software packages such as worldfootballR biathlonresults.com/ hosts results from the international biathlon federation. An example of analyzed data can be seen here. Ensembl and the National center for biotechnology information are commonly used databases in the biomedical sciences. We published our raw data together with a recent paper (Mølmen et al 2021 doi: 10.1186/s12967-021-02969-1.) together with code to analyze it in a public repository. https://www.evaluate.com/vantage/articles/news/snippets/its-official-covid-19-vaccine-trials-rank-among-largest The amount of time used by different coaches to create their own specific coaching software really makes many of them amateur software engineers. See for example this training journal from swedish orienteering. In addition to R, Python offers a free open source environment for reproducible analyses. The choice between the two are matter of taste. "],["storing-data-in-spreadsheets.html", "Chapter 3 Storing data in spreadsheets 3.1 Cells and simple functions 3.2 Tidy data and data storage 3.3 Recording data 3.4 Saving data", " Chapter 3 Storing data in spreadsheets I previously mentioned spreadsheets like those created in Excel. These are indeed great, but not great for reproducible science or data analysis. This is because they are not easily documented and scripted. The data is actually part of the analysis. Another danger with spreadsheets (like MS Excel) is that it re-formats your data. This is such a big problem for scientists that we have apparently started renaming genes. Errors are frequent in spreadsheets, not only because renaming (Ziemann, Eren, and El-Osta 2016), but also because of bad formatting of formulas (Stephen, Kenneth, and Barry 2009). These are both reasons for using spreadsheets only for what they do best: data input and data storage. Think about the literature Broman and Woo (Broman and Woo 2018) gives several pointers on how to use spreadsheets for data input and storage. Think about your experince with Excel, what is the most common mistake you made when handling data in spreadsheets? Although data storage and data input are great ways to use spreadsheets, its good to know a little about the capabilities of your spreadsheet software. 3.1 Cells and simple functions A spreadsheet consists of cells, these can contain values, such as text, numbers, formulas and functions. Cells may also be formatted with attributes such as color or text styles. Below is an example of some data entered in a spreadsheet. Figure 3.1: Example entries from an Excel spreadsheet Cell B6 contains a simple formula: = C6 + D6. This formula adds cells C6 and D6 resulting in the sum, 8. In formulas, mathematical operators can be used (\\(+, -, \\times , \\div\\) ). Formulas can be also extended with inbuilt function such as showed in 3.1. Table 3.1: Often used functions in excel. Function English Norwegian Sum SUM() SUMMER() Average AVERAGE() GJENNOMSNITT() Standard deviation STDEV.S() STDEV.S() Count COUNT() ANTALL() Intercept INTERCEPT() SKJÆRINGSPUNKT() Slope SLOPE() STIGNINGSTALL() If IF() HVIS() The sum, average and standard deviation and count are simple functions for summarizing data. Intercept and slope are both examples of functions used to get simple associations from to sets of numbers (based on a regression model). The if function is an example of a function that can be used to conditionally enter data in a cell. For example, IF cell A1 contains a certain number, then cell B1 should display another a specified text. When looking for tips and tricks online, you may come across functions for excel in other languages than what is installed on your computer. To translate functions, and for a full overview of functions included in Microsoft Excel, see this website en.excel-translator.de/. 3.2 Tidy data and data storage Hadley Wickham uses a quote from Tolstoy when describing the principle of tidy data (Wickham 2014). This quote is so famous that it has given name to a principle. The principle in turn comes in many variants but basically states that when something goes wrong, it can be wrong in multiple ways. But when it is right/correct/works/succeeds, it does so in only one way11. This principle can be applied to data sets. There are so many ways that formatting of data sets can be problematic, but a limited sets of principles makes it good. Figure 3.2: Leo Tolstoy at the time when he was (possibly) authoring Anna Karenina. (Source: https://en.wikipedia.org/wiki/Leo_Tolstoy) A tidy data set consists of values originating from observations and belonging to variables. A variable is a definition of the values based on attributes. An observation may consist of several variables (Wickham 2014). A tidy data set typically has got one observation per row and one variable per column. Lets say that we want to collect data from a strength test. A participant (participant is a variable) in our study conducts tests before and after the intervention (time is a variable) in two exercises (exercise is a variable) and we record the maximal strength in kg (load is a variable). The data set will look something like in the table below (3.2). Table 3.2: Example of tidy data. Participant Time Exercise Load Bruce Wayne pre Bench press 95 Bruce Wayne post Bench press 128 Bruce Wayne pre Leg press 180 Bruce Wayne post Leg press 280 Another example contains variables that actually carries two pieces of information in one variable. We again did a strength test, this time as maximal isometric contractions and in each test consisted of two attempts. We record this in two different variables, attempt 1 and 2. The resulting data set could look something like in Table 3.3. Table 3.3: Another example of tidy data. Participant Time Exercise Attempt1 Attempt2 Selina Kyle pre Isometric 81.3 92.5 Selina Kyle post Isometric 97.1 114.1 To make this data set tidy we need to extract the attempt information and record it in another variable as seen in Table 3.4. Table 3.4: A third example of tidy data. Participant Time Exercise Attempt load Selina Kyle pre Isometric 1 81.3 Selina Kyle pre Isometric 2 92.5 Selina Kyle post Isometric 1 97.1 Selina Kyle post Isometric 2 114.1 This naturally gives additional rows to the data set. This is sometimes referred to as long format data as opposed to the structure where each attempt is given separate variables, something that is called wide format. You will notice during the course that for most purposes, the long format is most convenient. This is true when we create graphs and do statistical modelling. But sometimes a variable needs to be structured in a wide format to allow for certain operations. If we follow what is recommended by Broman and Woo (Broman and Woo 2018), it is clear that each cell in a spreadsheet should only contain one value. If we for example decide to format a cell to a certain color, we add data to that cell on top of the actual data. You might add color to a cell in order to remember to add or change data. However, when you use the data set in other software, this information is lost. You should instead add another variable to allow for such data to be properly recorded. Using a variable called comments you can add text thta actually describes some information about that particular observation, information that is not lost when you use the data set in another software. 3.3 Recording data A trade secret12 from people who work all day with data and programming is that they are lazy. Lazy in the sense that you do not want to type too much, and absolutely not use the computer mouse when it can be avoided. When recording data we can try to be lazy to. We can do this by shortening variable names and not e.g. using CAPITAL letters when entering text in data storage. After a hard day at the keyboard, you will be happy to write strtest instead of Strength Test. The extra effort of using two capital letters might be the thing to tip you over the edge13. However, we should not be too lazy either, variable names and values should short, but meaningful (Broman and Woo 2018). Figure 3.3: D-FENS Foster gets pushed over the edge (Source: https://en.wikipedia.org/wiki/Falling_Down) Data and variables should also be consistent. Do not mix data type, use a consistent way of entering e.g. dates and time, do not uses spaces or special characters. To enforce this you might want start your data collection with writing up a data dictionary that describes all variables you are collection. The dictionary can set the rules for your variables. This dictionary can also guide your data validation. In Excel, data validation can be used to set rules for data entry. For example, if you have a numeric variable, you can set Excel only to accept numbers in specified set of cells. This makes it harder to enter erroneous data. 3.4 Saving data Data from spreadsheets can be saved as special spreadsheet files, such as .xlsx. This format allows for functions, multiple spreadsheets in the same file (tabs) and cell formatting. If you follow the tips described above and in (Broman and Woo 2018) you do not need this fancy format. Instead you can store your data as a .csv file. This format may be read and edited with Excel (or another spreadsheet software) but also in a plain text editor. Data entered in this format (comma-separated values; csv) can look like this in a text editor: Participant;Time;Exercise;Attempt;load Selina Kyle;pre;Isometric;1;81.3 Selina Kyle;pre;Isometric;2;92.5 Selina Kyle;post;Isometric;1;97.1 Selina Kyle;post;Isometric;2;114.1 This is actually quite nice. The data takes little space, the simple format requires that data is well documented using e.g. a data dictionary and it is available for many other softwares as the format is simple. The data can be documented using a README file that could describe the purpose and methods of data collection, how the data is structured and what kind of data the variables contains. A simple README file can be written in a text editor such as Notepad and saved as a .txt file. Later in this course we will introduce a markup language often used to create README files containing a syntax that formats the text to a more pleasant style when converted to other formats. See https://en.wikipedia.org/wiki/Anna_Karenina_principle A trade secret as in not generally known to the public. See en.wikipedia.org/wiki/Trade_secret. In the movie Falling Down, Michael Douglas plays a unemployed engineer who gets push over edge, would it have been enough with a few to many capital letters? "],["installing-and-starting-up-r-and-r-studio.html", "Chapter 4 Installing and starting up R (and R Studio) 4.1 Installing R 4.2 Installing R Studio 4.3 Getting to know R and RStudio 4.4 Reproducible computing 4.5 Packages 4.6 Installing and using swirl 4.7 R scripts 4.8 R markdown files", " Chapter 4 Installing and starting up R (and R Studio) This chapter contains step-by-step instructions for installing and running R and RStudio. It will also introduce you to some concepts when talking to R. By the end of this chapter you will be able to answer these questions: What is R and RStudio? How can I interact with R? What are the components of RStudio How do I maintain a reproducible work-flow in R and RStudio? What is a R-script What is a R-markdown file 4.1 Installing R R is a free, open-source software designed for statistical computing. We will use R as a part of an environment (using R Studio, introduced below). To download and install R: Go to https://cran.uib.no/, select your operating system (Download R for Windows, MacOS or Linux). If you have Windows, choose base, click on Download R () for windows, save and run the file. The installation process should be self explanatory. If you have MacOS, download and install the latest release. 4.2 Installing R Studio RStudio is a software designed to make it easier to use R. It is free to download and use. It is designed as an integrated development environment that lets you organize your work together with R and other tools. Install it by going to https://www.rstudio.com/. Select Products and RStudio Go to desktop and select DOWNLOAD RSTUDIO DESKTOP Select the free open source license and download the file made for your operating system (use the installers). 4.3 Getting to know R and RStudio R is a software used for scientific/statistical computing. If R is the engine, RStudio is the rest of the car. What does this mean? When doing operations in R, you are actually interacting with R through RStudio. RStudio have some important components to help you interact with R. 4.3.1 The source The source is where you keep your code. When writing your code in a text-file, you can call it a script, this is essentially a computer program where you tell R what to do. It is executed from top to bottom. You can send one line of code, multiple lines or whole sections into R. In the image below, the source window is in the top left corner. 4.3.2 Environment The environment is where all your objects are located. Objects can be variables or data sets that you are working with. In RStudio the environment is listed under the environment tab (bottom left in the image). Copy and run the code below. a &lt;- c(1, 2, 4) What happened in your environment? 4.3.3 The console Here you can directly interact with R. This is also where output from R is printed. In the image below, the console is in the top right corner. 4.3.4 Files, plots, packages and help files In RStudio files are accessible from the Files tab. The files tab shows the files in you root folder. The root folder is where R will search for files if you till it to. We will talk more about the root folder later in connection with projects. Plots are displayed in the Plot tab. Packages are listed in the packages tab. If you access the help files, these will be displayed in the help tab. In the image below all these tabs are in the bottom right corner. Figure 4.1: RStudio when first opened up. 4.3.5 Customizing the apperance of RStudio To access options for RStudio, go to Tools -&gt; Global options Figure 4.2: Accessing options for your RStudio IDE Under appearance you can customize the theme of RStudio, select something that is easy on the eye! Figure 4.3: Accessing options for your RStudio IDE and selection a theme Under pane layout, you can set where you want your tabs, I like to have the source on the left, above the environment. This way you can have the source window at full vertical size and still look at plots and the console to the right. Figure 4.4: Accessing options for your RStudio IDE and set the panes 4.4 Reproducible computing Computations are reproducible when you can show how they were performed. This is achieved by creating programs from where your analyses are done. In R, these programs are lines or R code stored in a text-file, either .R-files or .Rmd-files. .R-files are scripts only containing code and comments. A .Rmd-file is a special script combining text and computer code, when the Rmd-file is executed, it creates a report and outputs the results from the code. This means that to work in a reproducible way, you need to script all your operations. Figure 4.5: Reproducible vs. non-reproducible workflow Importantly, in RStudio you can shut down storing temporary objects in a environment that is relaunched on start up. What is the consequence of having such a situation? To disable this option, set save works pace to NEVER! Figure 4.6: Set the workspace option to never save. 4.5 Packages The R ecosystem consists of packages. These are functions organized in a systematic manner. Functions are created to perform a specialized task. And packages often have many function used to do e.g. analyses of a specific kind of data, or more general task such as making figures or handle data. In this course we will use many different packages, for example dplyr, tidyr and ggplot2. dplyr and tidyr are packages used to transform and clean data. ggplot2 is used for making figures. To install a package, you use the install.packages() function. You only need to do this once on your computer (unless you re-install R). You can write the following code in your console to install dplyr. install.packages(&quot;dplyr&quot;) Alternatively, click Packages and Install and search for the package you want to install. To use a package, you have to load it into your environment. Use the library() function to load a package. library(&quot;dplyr&quot;) 4.6 Installing and using swirl Swirl is a great way to get to know how to talk with R. Swirl consists of lessons created for different topics. Install swirl by typing the following into your console: install.packages(&quot;swirl&quot;) When swirlis installed you will need to load the package This means that all functions that are included in package becomes available to you in your R session. To load the package you use the library function. library(&quot;swirl&quot;) When you run the above command in your console you will get a message saying to call swirl() when you are ready to learn. I would like you to run the course R Programming: The basics of programming in R. Swirl will ask if you want to install it. After installation, just follow the instructions in the console. To get out of swirl, just press ESC. 4.7 R scripts As pointed out elsewhere (Wickham and Grolemund 2017; Peng, Dominici, and Zeger 2006), programming is an important part of a reproducible data analysis. This lets you build your analysis, go back a change components of it an re-run it with any number of changes. In this process you will probably learn more about your data. Putting all these steps in a program lets you save the whole analytic process. In R you may start by working with R scripts. These are basically text files that are written with a special syntax that can be interpreted by your computer. Additionally you have the possibility to add comments that makes the code more readable. R code is generally easy to read. But you will likely need additional comments to make it easier to show what you intend to do. A R script can be thought of as a computer program that when executed from top to bottom perform as series of steps in the order that they appear in the file. A feature of a well working program is that it is self-contained, i.e. it contains all parts needed to run. If you need to load a package or data, make sure that these steps are in the beginning of the script. A nice feature of combining code and comments can be that you first write a plan in plain language and then add computer code to perform the steps that you want to do. Below is a simple example. Comments start with a #, this is interpreted by R as non-code line and will be ignored. The R code is structured in series, the first steps are needed to perform sequential steps. As mentioned, the work-flow of creating the example below would be to first make a plan by writing the comments and the adding the code. # Create a data set in a data.frame df &lt;- data.frame(x = rnorm(100, 100, 10), y = runif(100, min = 10, max = 25)) # Add column x and y together in a new variable called z df$z &lt;- df$x + df$y # Make a figure of the resulting data frame by plotting x against z with(df, plot(x, z)) When working in RStudio, you can run a bit of the code by selecting it and pressing CTRL + ENTER (CMD + ENTER on Mac). You can also execute a line simply by having your cursor on a specific line and press CTRL + ENTER. Code execution means that the specific part of the script (line or section) is copied to the console and activated. R Scripts can also be sourced. This means that the whole script will be executed from to to bottom when you tell R to do so. Lets say that you have a script that creates a figure and saves it, called figure1.R. By using the source() function you can tell R to execute the script (source(\"figure1.R\")). As you can see in this example the filename extension .R tells you that a file can be intepreted as a R script. 4.8 R markdown files R markdown files are more advanced computer programs as they in a structured way combines plain text and code to create an output file such as a .html, .pdf or .doc document. The text parts are written using a special syntax, markdown. The point of markdown is that you will use the same syntax that is later possible to convert to multiple formats. The syntax lets you do all formatting explicitly, for example instead of getting your mouse to superscript some text you can add syntax a^2^ to achieve a2. A full guide to RMarkdown can be found on the official R markdown web pages. I suggest you take the time to get an overview of this langiage as it will make more fluent in the tools that enables reproducible computing. When writing R markdown, it is handy to have a cheat sheet close by when writing, here is an example14. If you do not want to write text in a simple text editor, RStudio has its own visual markdown editor. This editor contains similar functions by press of buttons as in for example word. R Markdown files has the file name extension .Rmd. 4.8.1 Starting up your first R markdown file A R markdown report is basically a text document containing plain text and code. When you compile your report, the code will be evaluated and figures, calculations and so on will be performed per your specifications. The resulting file will be an html, docx or pdf file. You can choose if you would like to display your code or not but your code is always available in your source document. R Markdown is very versatile, you can make word documents, blog posts, websites and pdf documents15. When in R Studio, you can start a new document using File &gt; New File &gt; R Markdown. This will launch a file in your script window looking something like this: --- title: &quot;Untitled&quot; author: &quot;Daniel Hammarström&quot; date: &quot;2020 05 09&quot; output: html_document --- ## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;. This is not an empty document and you have to remove the pre-written instructions. These instructions are quite handy though. Basically, in code chunks you write R code, this code will be evaluated and the output will be displayed in the file you create. Between code chunks you can write markdown text. This will be displayed as ordinary text in your created document. The plain text sections can also contain code. A code chunk is created using ```{r, eval=TRUE} 1 + 1 ``` This code chunk calculates 1+1, when you compile the document, the result of this calculation will be shown below the code chunk. The same computation can be made inline. An inline code chunk is created using `r 1+1`, here only the result of this computation will be shown in your text. When you compile the doucument it is called knitting, R uses a package called knitr made to compile R Markdown files. In the upper part of the source window, there is a button kalled Knit. When you press it, RStudio will aske you to save the Rmd file and an output file will be created. 4.8.2 Microsoft Word intergration Sometimes it is usefull to knit to a word file. For example when you want to share a report with fellow students who are not familiar with R. R Markdown can be used as a source for word documents (.docx). To create a word document from your Rmd-file you need a working installation of Microsoft Word. Settings for the output is specified in the YAML metadata field in the Rmd-file. This is the first section of a Rmd file, and when you want it to create a word file you specify it like this: --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document --- The output: word_document tells R to create a word file. If you are not happy with the style of the word document (e.g. size and font of text) you can tell R to use a template file. Save a word file that you have knitted as reference.docx and use specify in the YAML field that you will use thiss as reference. --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document: reference_docx: reference.docx --- Edit styles (Stiler in Norwegian) used in the reference file (right click on the style and edit). For example, editing the Title style (Tittel in Norwegian) will change the main titel of the document. After you have edited the document, save it. When you knit the document again, your updated styles will be used your word document. Here you can read more about using R Markdown together with word. If you do not have word installed, you can also use Open Office. Read more about it here. 4.8.3 Adding references References/citations can be added to the report using the bibliography option in the YAML field. Citations needs to be listed in a file, multiple formats are avaliable. A convenient format is bibtex. When using this format, create a text file with the ending .bib, for example, bibliography.bib. The bibliography.bib-file needs to be activated in the YAML-field. Do it by adding this information: --- title: &quot;A title&quot; author: Daniel Hammarström date: 2020-09-05 output: word_document: reference_docx: reference.docx bibliography: bibliography.bib --- Add citations to the file in bibtex-format. Here is an example: @Article{refID1, Author=&quot;Ellefsen, S. and Hammarstrom, D. and Strand, T. A. and Zacharoff, E. and Whist, J. E. and Rauk, I. and Nygaard, H. and Vegge, G. and Hanestadhaugen, M. and Wernbom, M. and Cumming, K. T. and Rønning, R. and Raastad, T. and Rønnestad, B. R. &quot;, Title=&quot;{Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training}&quot;, Journal=&quot;Am. J. Physiol. Regul. Integr. Comp. Physiol.&quot;, Year=&quot;2015&quot;, Volume=&quot;309&quot;, Number=&quot;7&quot;, Pages=&quot;R767--779&quot;, Month=&quot;Oct&quot;} The part that says refID1 can be edited to something appropriate. This is a reference identification, you use it to get the citation into the text. When citing you do it in the form Blood flow-restricted training leads to similar adaptations as traditional training [@refID1]. This will appear in text as: Blood flow-restricted training leads to similar adaptations as traditional training (Ellefsen et al. 2015). The reference will end up in the end of the document (as on this webpage). You can gather references in bibtex format from Oria (use the BIBTEX icon) and from PubMed using TeXMed. You can also export reference in bibtex format from citation software like Endnote or Zotero. Make sure you check all references when entering them, especially MedTex gives some problems with scandinavian letters (å æ ä ø ö). Recently RStudio added support for adding citations inside the visual markdown editor. Cheat sheets are available in R Studio: Help &gt; Cheatsheets Make sure to look through the installation instructions to get pdf options working "],["creating-your-first-graph.html", "Chapter 5 Creating your first graph 5.1 Resources 5.2 Learning objectives 5.3 Prerequisites 5.4 The ggplot2 system 5.5 Different geoms using real data 5.6 Themes 5.7 Test your understandning", " Chapter 5 Creating your first graph Data visualization is an efficient way of understanding data. By using graphs we can communicate characteristics of a data set that would have been impossible with a limited number of summary statistics (central tendencies, spread etc.). In Chapter 2 of his book (Spiegelhalter 2019), Spiegelhalter touches upon this fact when he describes different types of graphs and their use to understand different data sets. A great argument for the use of data visualization is the need to understand what factors might explain variation in a given data set (Spiegelhalter 2019). In this sense, data visualization can be thought of as an initial step in understanding data, data visualization as an exploratory tool. RStudio is a powerful environment for data visualization. Together with R (that is excellent for creating graphs), you can create and preview figures that represents your data in RStudio. R has got several systems for creating figures, plots, graphs. In this course, we will use ggplot2. Another system for plotting comes with the base installation of R. This is sometimes referred to as base R (see this tutorial, or this. Another well described and used system is lattice. We choose ggplot2 because it works well with the tidyverse, and it is well described. 5.1 Resources There are several good resources aimed at ggplot2: Chapter 4 in R for data science The ggplot2 book The ggplot2 cheatsheet 5.2 Learning objectives After this session, you should be able to answer: What are geoms? What is mapping data to aesthetics? What are theme components? You should also be able to create your first graph. 5.3 Prerequisites To follow the exercises below you will need to some data. For the purpose of this course, I have created a package that contains the data sets we need. In this session we will work with the cyclingstudy data set. To install the package (exscidata) you will need another package called remotes. The code below first checks if the package remotes is installed, or more specifically, if \"remotes\" cannot be found in the list of installed packages. Using the if function makes install.packages(remotes) conditional. If we do not find \"remotes\" among installed packages, then install remotes. The next line of code does the same with the exscidata package. However, since the package is not on CRAN but hosted on github we will need to use remotes to install it. The part of the second line of code that says remotes::install_github(\"dhammarstrom/exscidata\") uses the function install_github without loading the remotes package. The last line of the code below loads the package exscidata using the library function. # Check if remotes is not installed, if TRUE, install remotes if (!&quot;remotes&quot; %in% installed.packages()) install.packages(remotes) # Check if exscidata is not installed, if TRUE, install exscidata from github if (!&quot;exscidata&quot; %in% installed.packages()) remotes::install_github(&quot;dhammarstrom/exscidata&quot;) # Load exscidata library(exscidata) Next we need to load the tidyverse package. This package in turn loads several packages that we will use when transforming data and making our figures. I will include the line of code that checks if the package is installed, if not R will download and install it. We subsequently load the package using library. # Check if tidyverse is not installed, if TRUE, install remotes if (!&quot;tidyverse&quot; %in% installed.packages()) install.packages(tidyverse) library(tidyverse) We are now ready to explore the data set. But first we should talk about the main components of the ggplot2 system. 5.4 The ggplot2 system When using the ggplot2 system we can think of the resulting graph as containing data that has been mapped to different coordinates, colors, shapes, sizes and other attributes that determines what is being visualized. We are using different geometric representations of the data in the visualization. When we map data in ggplot we use a specific function, aes() (short for aesthetic). We will use this inside the main engine, ggplot(). For this first simple example we will create a data set. When you simulate data in R you can tell R what should be the starting point in the random number generator. Using set.seed(100) we can recreate the same numbers from what ever number generator we later use. In the example below, we use rnorm() to simulate numbers from a normal distribution. The settings n = 10, mean = 0 and sd = 1 we will simulate randomly picking 10 numbers from a distribution that has a mean of 0 and a standard deviation of 1. These numbers are stored in a data frame that as assigned to an object that we have named dat. set.seed(99) dat &lt;- data.frame(x = rnorm(10, mean = 0, sd = 1), y = rnorm(10, mean = 10, sd = 2)) The dataset consist of two variables. We will start by creating the canvas, this basically sets the border of the figure we want to create. The ggplot() function takes the dataset as its first argument, followed by the aes() function that is used to map data to coordinates and other attributes. ggplot(dat, aes(x = x, y = y)) Figure 5.1: An empty ggplot canvas. As you can see in Figure 5.1 the code above creates an empty canvas that has enough room to visualize our data. The x- and y-axes are adjusted to give room for graphical representations of the data. Next we need to add geometric shapes. These are functions that we add to the plot using the + sign. These functions all start with geom_ and has and ending that describes the geoms, like point, line, etc. We will add geom_point() to our empty canvas as plotted in Figure 5.1. The geom_point function inherits the mapping from from ggplot(). This means that we do not need to specify anything in geom_point at this stage. ggplot(dat, aes(x = x, y = y)) + geom_point() Figure 5.2: A ggplot canvas with points added. In Figure 5.2 we have added black points to each x- and y-coordinate representing x and y from our data set. To extend the example we will add data to our dataset. In the code below, we create a new variable in the dataset using $ effectively giving us a new column in the data. We use rep(\"A\", 5) to replicate the letter A five times and the same for B. The c() function combines the two in a single vector. We can use head(dat) to see what we accomplished with these operations. The head() function prints the first six rows from the dataset. dat$z &lt;- c(rep(&quot;A&quot;, 5), rep(&quot;B&quot;, 5)) head(dat) ## x y z ## 1 0.2139625 8.508462 A ## 2 0.4796581 11.843101 A ## 3 0.0878287 11.500109 A ## 4 0.4438585 4.982892 A ## 5 -0.3628379 3.918132 A ## 6 0.1226740 10.000532 B We can see that we have an additional variable z that contains \"A\" and \"B\". This new variable can be used to add more information to the plot. Lets say that we want to map the z variable to different colors. We do this by adding color = z to aes. This means that we want the z variable to determine colors. ggplot(dat, aes(x = x, y = y, color = z)) + geom_point() Figure 5.3: A ggplot canvas with colored points added. In Figure 5.3 we can see that different colors are used for the two letters \"A\" and \"B\". Other attributes can also be specified like shape, fill or size. The shape specifies the appearance of the points. When we use use data to map to shapes, ggplot2 will start from the standard shape. Figure 5.4: Shapes in R Possible shapes in the standard framework in R are shown in Figure 5.4. We may use this information to either fix the shape of the points. Lets say that instead of colored points we want filled points. We would change the color = z argument to fill = z instead and select a point shape that can be filled (shapes 21-25, see Figure 5.4. Notice in the code below that shape = 21 has been added to geom_point(). We have specified how points should be displayed. ggplot(dat, aes(x = x, y = y, fill = z)) + geom_point(shape = 21) Figure 5.5: A ggplot canvas with filled points added. Since shape is an attribute it to can be mapped by data. If we want data to determine both shape and fill we could add this information in the aes() function by setting both shape = z and fill = z. We now have to specify what shapes ggplot should use in order to be sure we can combine both shapes and fill. We will use scale_fill_manual and scale_shape_manual to do this. These functions takes lets you specify different values for aesthetics. Notice that we removed shape = 21 from the geom_point() function, but we added size to increase the size of the points. ggplot(dat, aes(x = x, y = y, fill = z, shape = z)) + geom_point(size = 3) + scale_fill_manual(values = c(&quot;red&quot;, &quot;green&quot;)) + scale_shape_manual(values = c(21, 23)) Figure 5.6: Data mapped to fill and shape, and size specified manually to override the default. 5.5 Different geoms using real data We have now seen that the basic ggplot2 figure maps underlying data to coordinates and geometric representations, such as points. We will go further by using some real data. We will be using the cyclingstudy dataset from the exscidata-package. We will start by loading the data and select a few columns that we are interested in. By using data(\"cyclingstudy\") we will load the data set that is part of the exscidata-package to our environment. By looking at the environment tab you can see that this operation adds a data set to the environment. It has 80 observations and 101 variables. Using the glimpse() function from dplyr (which is loaded by loading tidyverse) we will get an overview of all variables in the dataset. I have omitted the output from the code below # Load the data and have a first look data(&quot;cyclingstudy&quot;) glimpse(cyclingstudy) We will store a selected set of variables in a new object for ease of use. We will call this object cycdat. We select variables using the function with the very suitable name select where the first argument specifies the dataset, following arguments specifies what variables we want. Lets say that we are interested in squat jump height. The exscidata package comes with descriptions of the datasets. By writing ?cyclingstudy in your console you will see the description of the data in your help tab. Squat jump is recorded as sj.max, we select this variable together with subject, group and timepoint to create a smaller data set. # Assign a selected set of variables to a smaller data set cycdat &lt;- select(cyclingstudy, subject, group, timepoint, sj.max) # Printing the data set cycdat ## # A tibble: 80 x 4 ## subject group timepoint sj.max ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 INCR pre 31.0 ## 2 2 DECR pre 31.6 ## 3 3 INCR pre 26.8 ## 4 4 DECR pre 29.2 ## 5 5 DECR pre 31.2 ## 6 6 INCR pre 34.2 ## 7 7 MIX pre 30.1 ## 8 8 MIX pre 32.8 ## 9 9 MIX pre 22.7 ## 10 10 INCR pre 29.7 ## # ... with 70 more rows By printing the object we can see that we have a tibble of 80 rows and 4 columns. A tibble can to a large extent be regarded as a data frame, and we will use these words interchangeably. Tibbles are new in the sense that they are developed as part of the tidyverse (Wickham and Grolemund 2017).16 Printing a tibble will display the first 10 rows as we can see from the resulting output. 5.5.1 A plot of values per group Lets say that we want to see how the values differs between groups. Boxplots are a good way to start as they will bring a standardized way of summarizing data. Boxplots can be plotted using the geom_boxplot function. Notice below that we put group on the x-axis (the first argument in the aes function) and sj.max on the y-axis. By doing so ggplot will make the x-axis discrete and the y-axis continuous. # Creating a boxplot of all values per group ggplot(cycdat, aes(group, sj.max)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.7: Boxplot of all data per group from the cycling dataset. We can layers of more geoms to the same plot. We might want to add individual data points also. geom_jitter might be a good place to start. This geom is good as it can be plotted over a group variable and points gets jittered or spread so we avoid overlap. # Creating a boxplot of all values per group ggplot(cycdat, aes(group, sj.max)) + geom_boxplot() + geom_jitter() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). ## Warning: Removed 4 rows containing missing values (geom_point). Figure 5.8: Boxplot and jittered points of all data per group from the cycling dataset. Notice that we get warnings saying that there are some data missing, these are removed from the calculation of summary statistics in the boxplots and omitted from plotting the values as points. 5.5.2 Data over time per group and individual In the data set we have a time variable consisting of the labels pre, meso1, meso2 and meso3. When we load the data into R we do so without providing information about the order of these labels. R will put them in alphabetical order when order is required (as in a figure). If we want to plot these data in the right order we have to tell R that these data should have an order. We will convert the timepoint variable to a factor. Factors are variables that can contain more information than what is contained in each cell. Using the factor function we will set the order of the timepoint variable. We assign this transformation of the variable to its original place in the data frame. cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;)) We are now ready to plot data over time, where the time variable is correctly ordered. Lets use the boxplot again to plot all values over time. # Creating a boxplot of all values per time point ggplot(cycdat, aes(timepoint, sj.max)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.9: Boxplot of all data per time-point from the cycling dataset. We do not see any great tendencies in the whole data set. To further explore the data we might want to have different boxes per group per time. We can accomplish this by adding fill = group to our aes function. # Creating a boxplot of all values per group over time ggplot(cycdat, aes(timepoint, sj.max, fill = group)) + geom_boxplot() ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). Figure 5.10: Boxplot of all data per time-point and group from the cycling dataset. This is possible because geom_boxplots can be filled. The same separation of groups would have been accomplished using color = group, however, then the boxes would get different border colors instead. You might have noticed that the boxplots do not contain all the data, a few data points are outside 1.5 IQR (interquartile range). This, by standard definitions, defines the data point as an outlier. As mentioned above, boxplots does some summarizing and not all data is shown. To explore further we might want to track every participant. To do this we have to tell ggplot on what factor to group the data. In aes() the group argument lets you connect lines based on some grouping variable, in our case it will be subject. We will use a line to connect each participants score over time. Using color = group will additionally give every line a different color depending on which group it belongs to. # Creating a line plot of all values per participant over time, color per group ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.11: Boxplot of all data per time-point and group from the cycling dataset. In Figure 5.11 each line represents a participant, different colors represents different groups. 5.5.3 Titles and labels Often we need to add information to the plot to better communicate its message. Such information could be appropriate titles on axes and legends and extra text needed to explain aspects of the plot. Using the labs() function we can add information that will replace variable names that are being used for all variables that have been mapped in the figure. In the figure below we will start by adding better axis titles. This information goes into x and y in labs() which simply changes the titles of the x- and y-axis. # Creating a line plot of all values per participant over time, color per group ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.12: Boxplot of all data per time-point and group from the cycling dataset. The resulting Figure 5.12 now have better titles for each axis. Notice in the code above that titles needs to be specified with quotation marks. This is a tricky aspect of R, if we wold have omitted the quotation marks we would have told R to look for objects by the name of e.g. Time-point, and this would actually mean that we tryed to subtract time from point since - is interpreted as a minus sign. We might want to add information to the legend also. Since we specified color = group in the aes() function, the same can be manipulated in labs. Lets just add a capital G. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.13: Boxplot of all data per time-point and group from the cycling dataset. We still have the original labels for the tim variable. Remember that we used the factor function above to set the order of the labels. Actually we specified the levels of the factor. We can use the same function to add better labels. In the code below, I will first change the variable in the dataset and then use the exact same code for the plot. cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;), labels = c(&quot;Pre-training&quot;, &quot;Meso-cycle 1&quot;, &quot;Meso-cycle 2&quot;, &quot;Meso-cycle 3&quot;)) ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.14: Boxplot of all data per time-point and group from the cycling dataset. The same goes for the group variable. You can try to change the levels and labels of the grouping variable to make it more descriptive. You can type ?cyclingstudy in your console to read about the group variable and then use this infomation to write better labels using the factor function. In the factor function, the first argument is the variable you want to use as basis of your new factor, the second argument you need to specify is levels which sets the order and lastly you will need to set the labels for each level using labels =. If you write ?factor in your console you will get the help pages for the factor function. Click here to display a possible solution # Change the grouping variable cycdat$group &lt;- factor(cycdat$group, levels = c(&quot;DECR&quot;, &quot;INCR&quot;, &quot;MIX&quot;), labels = c(&quot;Decreased\\nintensity&quot;, &quot;Increased\\nintensity&quot;, &quot;Mixed\\nintensity&quot;)) # Plotting the data as before with the new information added ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) Note: Adding \\n in the the text string breaks the line to get two rows. 5.5.4 Annotations Annotation may become handy when you want to add elements to the graph that is not in the data set. Using ggplot2 annotations are added using the annotate() function. This function first needs to be specified with a geom, these are commonly text or lines or segments. In the code chunk below are several examples of annotations. Firts I save the plot as an object called myplot and then add different annotations to it. myplot &lt;- ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) # A text annotation myplot + annotate(&quot;text&quot;, x = 1, y = 37, label = &quot;This is an annotation&quot;) # A line/segment myplot + annotate(&quot;segment&quot;, x = 1, xend = 3, y = 25, yend = 35, colour = &quot;red&quot;, size = 4) You can copy the code and run it yourself to see the results. annotate is documented here but documentation can also be accessed by typing ?annotate in your console. Try to read the documentation and add a transparent rectangle to a previous plot. Click here for a solution # Change the grouping variable cycdat$group &lt;- factor(cycdat$group, levels = c(&quot;DECR&quot;, &quot;INCR&quot;, &quot;MIX&quot;), labels = c(&quot;Decreased\\nintensity&quot;, &quot;Increased\\nintensity&quot;, &quot;Mixed\\nintensity&quot;)) # Plotting the data as before with the new information added ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Periodization strategy&quot;) + # A rectangular annotation (alpha = 0.4 makes the rectangle transparent) annotate(&quot;rect&quot;, xmin = 1, xmax = 2, ymin = 30, ymax = 35, alpha = 0.4) Note: Adding \\n in the the text string breaks the line to get two rows. 5.6 Themes Themes in ggplot2 can be used to change everything else about the plot concerning text, colors etc. ggplot2 has some built in themes that are easily activated by adding them to the plot. For example the theme_bw() function will change the theme to a black and white one as in the figure below. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) + theme_bw() ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.15: A figure using the black and white theme from theme_bw. A collection of built in themes are documented here. Individual components of the theme can also be changed using the theme() function. There is a long list of theme components that can be changed using this function. The list can be found here. If we put the theme function last in the ggplot call we will modify one part of the existing theme. Lets say that we want to change the color of the text on the x axis. ggplot(cycdat, aes(timepoint, sj.max, color = group, group = subject)) + geom_line() + labs(x = &quot;Time-point&quot;, y = &quot;Squat jump height (cm)&quot;, color = &quot;Group&quot;) + theme_bw() + theme(axis.text.x = element_text(color = &quot;black&quot;, size = 12, face = &quot;bold&quot;)) ## Warning: Removed 2 row(s) containing missing values (geom_path). Figure 5.16: A figure using the black and white theme from theme_bw. The component axis.text.x can be modified using a function that changes appearance of text components, namely element_text. Similarly, other components are changed with specific functions for lines and rectangular shapes (see the help pages for theme. 5.7 Test your understandning In this section you can try to implement what we have discussed above An example solution exists below each figure by press of button. In Figure 5.17, I have used the VO2max data from the cyclingstudy dataset. I have made changes to the time variable (timepoint) to make the labels better. I have added a title to the figure and changed the appearance of the text. I will use an extra package called (ggtext)[https://wilkelab.org/ggtext/index.html] to make it possible to use markdown syntaxt in axis labels. In order to use ggtext you have to install it from CRAN. Figure 5.17: Example figure 1 Click for a solution # Load the package ggtext to make markdown avalable in axis labels. library(ggtext) # For ease of use I save a smaller dataset in a new object cycdat &lt;- select(cyclingstudy, subject, timepoint, VO2.max) # Change the labels of the time variable cycdat$timepoint &lt;- factor(cycdat$timepoint, levels = c(&quot;pre&quot;, &quot;meso1&quot;, &quot;meso2&quot;, &quot;meso3&quot;), labels = c(&quot;Pre-training&quot;, &quot;Meso-cycle 1&quot;, &quot;Meso-cycle 2&quot;, &quot;Meso-cycle 3&quot;)) # create the basic plot ggplot(data = cycdat, aes(timepoint, VO2.max, group = subject)) + # Add lines to connect dots. Putting the lines first and plotting points on top geom_line() + # Add points foe each participant/time geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + # Adding correct axis titles and a figure title labs(x = &quot;Time-point&quot;, y = &quot;VO&lt;sub&gt;2max&lt;/sub&gt; (ml min&lt;sup&gt; -1&lt;/sup&gt;)&quot;, title = &quot;Maximal aerobic power in response to systematic training in trained cyclists&quot;) + # Changing the text rendering using element_markdown from the ggtext package. theme(axis.title.y = element_markdown(size = 12)) Note: Adding \\n in the the text string breaks the line to get two rows. See Chapter 10 in R for data science "],["wrangling-data-to-create-your-first-table.html", "Chapter 6 Wrangling data to create your first table 6.1 Making Table 1 6.2 Summary 6.3 Flextable 6.4 An exercise, reproduce Table 1 from (Haun et al. 2019)", " Chapter 6 Wrangling data to create your first table Tables can be created as part of a R markdown document. This is because most table generators actually converts your code into an output format based on the rules of that particular format. The most common format to work with when you are building your analysis is html. Tables are special since their creation depend more heavily on the output format compared to other elements of R markdown documents. For example, when using the kable function from the knitr package you have to specify if you want html or latex output. Word output is not possible using the knitr package, but other table generators in rmarkdown (such as flextable) can produce word output, meaning you can put tables in word documents from your R markdown file. There are several alternatives for generating tables17. We will start by exploring the built-in function in the knitr package. The basic work-flow of creating a table in R markdown is to first transform the data into a nice format and then get this underlying data into the table generator. The table generator is written in a code chunk and upon rendering of the R markdown file, the table generator will create, for example, html output. In this chapter we will also introduce some data wrangling tools since the table we will produce consists of summarized data. The functions we will introduce are found in the pakages dplyr and tidyr. These packages are loaded as part of the tidyverse package. 6.0.1 Resources All tidyversepackages are well documented and generally well represented in help forums. Google is your friend when looking for help. The kable function is described in a newly developed book, available online called the R Markdown Cookbook. The package, kableExtra comes with excellent vignettes for both html and pdf outputs. kableExtra provides extra functions to customize your basic knitr table. 6.1 Making Table 1 The first table in many reports in sport and exercise studies is the Participant characteristics table. This first table summarizes background information on the participants. We will try to create this table based on data from (Hammarström et al. 2020). These data can be found in the exscidata package. To load the data and other required packages run the following code. library(tidyverse) # for data wrangling library(knitr) # for table creation library(kableExtra) # for extra styling of the table library(exscidata) # the dxadata The end result of this exercise can be found below in Table @ref(tab:table1_example). This summary table contains the average and standard deviation per group for the variables age, body mass and stature (height) and body fat as a percentage of the body mass. This table is a reproduction of the first part of Table 1 from (Hammarström et al. 2020). Female Male Included Excluded Included Excluded N 18 4 16 3 Age (years) 22 (1.3) 22.9 (1.6) 23.6 (4.1) 24.3 (1.5) Mass (kg) 64.4 (10) 64.6 (9.7) 75.8 (11) 88.2 (22) Stature (cm) 168 (6.9) 166 (7.6) 183 (5.9) 189 (4.6) Body fat (%) 34.1 (5.6) 28.8 (8.7) 20.4 (6) 24.3 (15) We have to make several operations to re-create this table. First we can select the columns we want to work with further from the data set that also contains a lot of other variables. Let us start by looking at the full data set. Below we use the function glmipse from the dplyrtibblepackage (which is also loaded withtidyverse`). data(&quot;dxadata&quot;) glimpse(dxadata) ## Rows: 80 ## Columns: 59 ## $ participant &lt;chr&gt; &quot;FP28&quot;, &quot;FP40&quot;, &quot;FP21&quot;, &quot;FP34&quot;, &quot;FP23&quot;, &quot;FP26&quot;, &quot;FP36... ## $ time &lt;chr&gt; &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre&quot;, &quot;pre... ## $ multiple &lt;chr&gt; &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;L&quot;, &quot;R&quot;, &quot;R&quot;, &quot;L&quot;, &quot;L&quot;... ## $ single &lt;chr&gt; &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;R&quot;, &quot;L&quot;, &quot;L&quot;, &quot;R&quot;, &quot;R&quot;... ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female... ## $ include &lt;chr&gt; &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;incl&quot;, &quot;excl&quot;, &quot;incl... ## $ age &lt;dbl&gt; 24.5, 22.1, 26.8, 23.1, 24.8, 24.2, 20.5, 20.6, 37.4,... ## $ height &lt;dbl&gt; 170.0, 175.0, 184.0, 164.0, 176.5, 163.0, 158.0, 181.... ## $ weight &lt;dbl&gt; 66.5, 64.0, 85.0, 53.0, 68.5, 56.0, 60.5, 83.5, 65.0,... ## $ BMD.head &lt;dbl&gt; 2.477, 1.916, 2.306, 2.163, 2.108, 2.866, 1.849, 2.21... ## $ BMD.arms &lt;dbl&gt; 0.952, 0.815, 0.980, 0.876, 0.917, 0.973, 0.871, 0.91... ## $ BMD.legs &lt;dbl&gt; 1.430, 1.218, 1.598, 1.256, 1.402, 1.488, 1.372, 1.42... ## $ BMD.body &lt;dbl&gt; 1.044, 0.860, 1.060, 0.842, 0.925, 0.984, 0.923, 1.01... ## $ BMD.ribs &lt;dbl&gt; 0.770, 0.630, 0.765, 0.636, 0.721, 0.737, 0.648, 0.70... ## $ BMD.pelvis &lt;dbl&gt; 1.252, 1.078, 1.314, 1.044, 1.154, 1.221, 1.194, 1.32... ## $ BMD.spine &lt;dbl&gt; 1.316, 0.979, 1.293, 0.899, 1.047, 1.089, 1.006, 1.14... ## $ BMD.whole &lt;dbl&gt; 1.268, 1.082, 1.325, 1.119, 1.181, 1.350, 1.166, 1.24... ## $ fat.left_arm &lt;dbl&gt; 1168, 715, 871, 610, 788, 372, 932, 1312, 388, 668, 5... ## $ fat.left_leg &lt;dbl&gt; 4469, 4696, 3467, 3023, 3088, 2100, 4674, 5435, 1873,... ## $ fat.left_body &lt;dbl&gt; 6280, 4061, 7740, 3638, 6018, 2328, 4896, 9352, 2921,... ## $ fat.left_whole &lt;dbl&gt; 12365, 9846, 12518, 7565, 10259, 5048, 10736, 16499, ... ## $ fat.right_arm &lt;dbl&gt; 1205, 769, 871, 610, 741, 374, 940, 1292, 413, 716, 5... ## $ fat.right_leg &lt;dbl&gt; 4497, 4900, 3444, 3017, 3254, 2082, 4756, 5455, 1782,... ## $ fat.right_body &lt;dbl&gt; 6082, 3923, 8172, 3602, 5699, 2144, 4705, 8674, 2640,... ## $ fat.right_whole &lt;dbl&gt; 12102, 9862, 12856, 7479, 10020, 4821, 10806, 15876, ... ## $ fat.arms &lt;dbl&gt; 2373, 1484, 1742, 1220, 1529, 747, 1872, 2604, 802, 1... ## $ fat.legs &lt;dbl&gt; 8965, 9596, 6911, 6040, 6342, 4182, 9430, 10890, 3655... ## $ fat.body &lt;dbl&gt; 12362, 7984, 15912, 7239, 11717, 4472, 9601, 18026, 5... ## $ fat.android &lt;dbl&gt; 1880, 963, 2460, 1203, 1933, 527, 1663, 3183, 1240, 1... ## $ fat.gynoid &lt;dbl&gt; 5064, 5032, 4779, 3739, 4087, 2740, 5217, 6278, 2309,... ## $ fat.whole &lt;dbl&gt; 24467, 19708, 25374, 15044, 20278, 9869, 21542, 32375... ## $ lean.left_arm &lt;dbl&gt; 1987, 1931, 2884, 1753, 2652, 2425, 1913, 2266, 3066,... ## $ lean.left_leg &lt;dbl&gt; 7059, 7190, 10281, 6014, 8242, 7903, 6829, 8889, 9664... ## $ lean.left_body &lt;dbl&gt; 9516, 10693, 13847, 9736, 11387, 10573, 8954, 11482, ... ## $ lean.left_whole &lt;dbl&gt; 20305, 21778, 29332, 19143, 24185, 22946, 18809, 2431... ## $ lean.right_arm &lt;dbl&gt; 2049, 2081, 2888, 1754, 2487, 2439, 1930, 2236, 3253,... ## $ lean.right_leg &lt;dbl&gt; 7104, 7506, 10200, 6009, 8685, 7841, 6950, 8923, 9198... ## $ lean.right_body &lt;dbl&gt; 9199, 10304, 14593, 9636, 10779, 9733, 8602, 10672, 1... ## $ lean.right_whole &lt;dbl&gt; 19605, 21310, 29643, 18792, 23653, 21837, 19407, 2372... ## $ lean.arms &lt;dbl&gt; 4036, 4012, 5773, 3508, 5139, 4864, 3843, 4501, 6319,... ## $ lean.legs &lt;dbl&gt; 14163, 14696, 20482, 12023, 16928, 15744, 13779, 1781... ## $ lean.body &lt;dbl&gt; 18715, 20998, 28440, 19372, 22166, 20306, 17556, 2215... ## $ lean.android &lt;dbl&gt; 2669, 2782, 3810, 2455, 2904, 2656, 2297, 3094, 3344,... ## $ lean.gynoid &lt;dbl&gt; 6219, 7209, 10233, 5866, 7525, 5970, 5825, 8175, 7760... ## $ lean.whole &lt;dbl&gt; 39910, 43088, 58976, 37934, 47837, 44783, 38216, 4804... ## $ BMC.left_arm &lt;dbl&gt; 181, 138, 204, 144, 180, 173, 140, 173, 220, 226, 225... ## $ BMC.left_leg &lt;dbl&gt; 567, 508, 728, 441, 562, 574, 482, 631, 633, 630, 672... ## $ BMC.left_body &lt;dbl&gt; 622, 414, 696, 367, 526, 465, 370, 629, 473, 629, 509... ## $ BMC.left_whole &lt;dbl&gt; 1680, 1321, 1945, 1201, 1527, 1580, 1131, 1688, 1544,... ## $ BMC.right_arm &lt;dbl&gt; 198, 150, 210, 142, 176, 183, 140, 176, 224, 251, 226... ## $ BMC.right_leg &lt;dbl&gt; 574, 514, 739, 431, 552, 565, 491, 641, 622, 636, 690... ## $ BMC.right_body &lt;dbl&gt; 592, 428, 730, 351, 502, 409, 358, 582, 420, 616, 483... ## $ BMC.right_whole &lt;dbl&gt; 1582, 1288, 1958, 1130, 1451, 1466, 1229, 1668, 1478,... ## $ BMC.arms &lt;dbl&gt; 379, 288, 414, 285, 356, 357, 280, 348, 444, 478, 451... ## $ BMC.legs &lt;dbl&gt; 1142, 1022, 1467, 872, 1115, 1139, 974, 1272, 1255, 1... ## $ BMC.body &lt;dbl&gt; 1214, 842, 1426, 718, 1028, 874, 728, 1211, 893, 1245... ## $ BMC.android &lt;dbl&gt; 80, 57, 90, 44, 56, 54, 43, 77, 52, 72, 59, 60, 65, 5... ## $ BMC.gynoid &lt;dbl&gt; 314, 285, 427, 245, 299, 262, 241, 379, 335, 378, 332... ## $ BMC.whole &lt;dbl&gt; 3261, 2609, 3903, 2331, 2978, 3046, 2360, 3356, 3022,... We can see that we got 80 rows and 59 columns in the dataset. The columns of interest to us are: participant time sex include age height weight fat.whole For a full description of the data set, you can type ?dxadata in your console. The participant columns is good to have to keep track of the dataset in the beginning. You might want to have this information to perform sanity checks on some results. time is needed to remove some observation that are not needed. This pre-training table only sums up information from before the intervention starts. sex is a grouping variable together with include, Table 1 in (Hammarström et al. 2020) uses Sex and Inclusion in data analysis as grouping for descriptive data. The other variables are used to describe the data sample. 6.1.1 The pipe operator and select As mentioned above, we will start by selecting the variables we want to work further with. Using the select function from dplyr we can select columns that we need. In the code below I will use select as part of a pipe. Think of the pipe as doing operations in sequel. Each time you use the pipe operator (%&gt;%) you say then do. The code below translates to: Take the dataset dxadata, then do select the following variables, then do print print, is a function that outputs the results of the operations. In each new function of a pipe, the data that we take with us from the above line ends up as the first argument. A representation of this behavior can be expressed as: DATA %&gt;% FUNCTION(DATA, ARGUMENTS) %&gt;% FUNCTION(DATA, ARGUMENTS) %&gt;% FUNCTION(DATA) We do not need to type the data part, instead the pipe operator (%&gt;%) gathers the data from each step and puts it in the subsequent function. Copy the code below to your own script or Rmarkdown document and run it. If you use a Rmarkdown document (.Rmd), you might want to set Chunk output in console in the settings menu. In my experience, this makes developing code a bit faster. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole print() # print the output Notice that I have added short comments after each line to make it clear what I want to accomplish. We will build further on the above code, and this is a common work-flow. Using pipes, it is easy to extend the code by adding lines doing certain operations, one at the time. Notice also that the select function uses a list of variable names with include:weight being short for \"take all variables from include to weight. 6.1.2 Filter observations The next step will be to filter observations. We need to remove the observations that comes from the post-intervention tests. The time variable contains to values pre and post to remove post-values we need to tell R to remove these all observations (rows) containing post. We will use the filter function from dplyr. This will be our first experience with logical operators. Lets try out to alternatives, copy the code to your console to see the results. ## Alternative 1: ## dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter away all observation with &quot;post&quot; filter(time != &quot;post&quot;) %&gt;% print() # print the output ## Alternative 2: ## dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% print() # print the output The above code should give the same output. The operator != says not equal to, the operator == says equal to. Notice that R uses two equal signs to say equal to. A single equal sign is used as an assignment operator in R. 6.1.3 Create or change variables The next problem for us is that we need to manipulate or combine information from two variables in order to calculate body fat percentage. The formula that we will use is simply expressing body fat as a percentage of the body weight. \\[Body~fat~(\\%) = \\frac{Body~fat~(g)/1000}{Body~weight~(kg)} \\times 100\\] By using the mutate function we can add or manipulate existing variables in a pipe. Mutate takes as arguments a list of new variables: dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% print() # print the output In the code above, we replace the variable fat.whole with the re-calculated variable. 6.1.4 Grouped operations and summary statistics In a pipe, we can group the data set giving us opportunities to calculate summary statistics over one or several grouping variables. In Table 1 in (Hammarström et al. 2020), include and sex are the two grouping variables. Using the group_by() function from dplyr sets a grouping of the data frame. If we use functions that summarizes data, such summarizes will be per group. In Table 1 in (Hammarström et al. 2020) the number of participants in each group are specified. We can use the function n() to calculate the number of observations per group in a mutate call. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% print() # print the output The new variable n now contains the number of observations in each group. For now we can regard this as a new variable. Each participant belongs to a specified group, and this specific group has n number of members. We can now go further and use the summarise function. Instead of adding variables to the existing data set, summarize reduces the data set using some summarizing function, such as mean() or sd(). These estimates are what we are looking for in our data set. Example of other summarizing functions for descriptive data are min() and max() for the minimum and maximum. We can use the summarise() function to calculate the mean and standard deviation for the weight variable. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Summarise weight summarise(weight.m = mean(weight), weight.s = sd(weight)) %&gt;% print() # print the output Try out the code in your own script. The above example gives us what we want, however, it means that we need to type a lot. Instead of needing to make a summary for each variable, we can combine the variables in a long format. To get to the long format we will use the pivot_longer() function. This function gathers several variables into two columns, one with the variables names as values and a second column with each value from the original variables. In our case we want to gather the variables age, height, weight, fat.whole and n. I will call the new variables that we create variable and value. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% print() The cols = age:n part of pivot_longer specifies what columns to gather. The dataset is still grouped by include and sex. We may now proceed by summarizing over these groups, however, we need to add another group to specify that we want differnt numbers per variable. To do this we re-specify the grouping. After this we add the summarize function. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% print() If you run the above code you will notice that the the standard deviation of each variable is larger than zero except for n which has no variability. This is because we created it per group and simply calculated it as the sum of observations. Take a look at Table 1 in (Hammarström et al. 2020). The format of the descriptive statistics are mean (SD), this is a preferred way of reporting these estimates. In order to achieve this we need to manually convert the numbers. In the example below, I will start by making a new variable by simply pasting the numbers together. I will also add the brackets. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = paste0(m, &quot; (&quot;, s, &quot;)&quot;)) print() In mutate(ms = paste0(m, \" (\", s, \")\")), the paste0 function simply paste components together to form a string of text. First, the vector of means are being used, then we add a parenthesis, followed by the SD and finally a parenthesis. If you run the above code you will notice that you end up with numbers looking like this: 167.666666666667 (6.86851298231541) This is neither good or good looking. We have to take care of the decimal places. There are a number of ways to do this but in this case the function signif seems to make the situation better. signif rounds to significant digits. This means that we will get different rounding dependning on the size of the value. I find signif(m, 3) to be agood starting point. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)) %&gt;% print() Things are starting to look good. Run the code, what do you think. A problem with the above is that we do not want any dispersion/variability in the n variable. So if the variable is nwe do not want that kind of formatting. It is time to add a conditional statment. In dplyr there are easy-to-use if/else functions. The function if_else sets a condition, if this is met then we can decide what to do, and likewise decide what to do if it is not met. This looks something like this inside a dplyr pipe: ... %&gt;% if_else(IF_THIS_IS_TRUE, THE_DO_THIS, OTHERWISE_DO_THIS) %&gt;% print() If variable is n, then we only want to display m otherwise we want the full code as described above: paste0(signif(m, 3), \" (\", signif(s, 3), \")\"). We add this to the code: dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;))) %&gt;% print() The as.character part is needed because the output of if_else must be the same regardless of what the outcome of the test is. We are getting close to something! The next step is to remove variables that we do not longer need. The select function will help us with that. we can remove mand s by select(-m, -s), the minus sign tells are to remove them from the list of variables in the dataset. We can then combine the grouping variables into a include_sex variable. Similarly to what we did above, we can simply paste them together. Now we will use the paste (function instead of paste0). In paste we specify a separator, maybe _ is a nice alternative. Selecting away the individual variables from the new combined one leaves us with this code and data set. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) print() If ungroup is not used, we cannot select away variables since they are used to group the dataset. We will now perform the last operations befor we can make it a table. To make it formatted as in Table 1 in (Hammarström et al. 2020), we can make the present dataset wider. Each group has its own variable in addition to the variable name column. We will use the opposite function to pivoy_longer, namely pivot_wider18. pivot_wider takes a variable or key column and a values column and divide the values based on the key. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% print() A final step is to format the variable variable(!). The easiest is to make it a factor variable with specified levels and names. After we have added this information, we can use arrange to sort the dataset. Using select will help you sort the columns to match wath we want. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% arrange(variable) %&gt;% print() 6.1.5 Starting the table generator - The kable function The next step is to pipe everything into the table generator. kable from the knitr package creates the framework for the table we are going to create. Next we will use functions from the kableExtra package to customize the table. Use the help pages for kable to see what can be specified in the function. We will make use of only a few arguments. kable can be part of a pipe, the same as the following kableExtra functions. The kablefunction is well described in the Rmarkdown cookbook. For functions belonging to the kableExtra package, see the vignettes. We will add the kable function to the end of the pipe we have built so far. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;) An important aspect is that we need to work in a RMarkdown document (.Rmd). This allows for conversion of html code generated by kable to the output file, if the output file format is html. In kable, as you can see in the example above, we specify this with the argument format =. Possible common formats are html and latex. \\(\\LaTeX\\) is a more advanced syntax for document preparation, the output is often a pdf file. In this session we are working with html, so we will keep format = \"html\". When knitting your .Rmd-document, the code chunk where you keep your table code needs to have a setting specifying results = \"asis\". The top of the code chunk should look like this: ```{r my_table, results=&quot;asis&quot;} dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% ... ``` When you run the above code, you may get output in your console. This output will look something like this: &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;text-align:left;&quot;&gt; variable &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; excl_female &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; excl_male &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; incl_female &lt;/th&gt; &lt;th style=&quot;text-align:left;&quot;&gt; incl_male &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; N &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 4 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 3 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 18 &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 16 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Age (years) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 22.9 (1.57) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 24.3 (1.46) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 22 (1.25) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 23.6 (4.11) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Mass (kg) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 64.6 (9.71) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 88.2 (22.4) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 64.4 (10.4) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 75.8 (10.7) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Stature (cm) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 166 (7.59) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 189 (4.58) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 168 (6.87) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 183 (5.88) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;text-align:left;&quot;&gt; Body fat (%) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 28.8 (8.69) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 24.3 (15.3) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 34.1 (5.64) &lt;/td&gt; &lt;td style=&quot;text-align:left;&quot;&gt; 20.4 (5.99) &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; This is html code. For some reason, RStudio will not interpret it as html-code and preview it in the Viewer pane, yet. We will now add some information to our table. First we want to change the headings. In kable we do this by setting the col.names argument. We do not need to repeat ourselves adding Female and Male to columns 2 and 3, and 4 and 5, respectively. Instead we specify the Include/Exclude grouping. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;)) Notice that I added a final select function to arrange the columns of the table. The col.names= argeumnet is specified in the same order. But how to separate male from females. This is where kableExtra comes in. This package provides functions for customizing the simple kable table. Using the function add_header_above we can specify in a named character vector what headers we want over a certain column span. We do not want any information in the first column, but we want Female to span two columns followed by Male spanning two columns. The resulting code will look like this: add_header_above(c(\" \" = 1, \"Female\" = 2, \"Male\", = 2)). Adding this code to our pipe (for some reason at least in my RStudio) puts the table output in the viewer pane! dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;)) %&gt;% add_header_above(c(&quot; &quot;= 1, &quot;Female&quot; = 2, &quot;Male&quot; = 2)) The table is almost done! We need to add some last information. Maybe a caption is in place and a footnote describing what the data represents. The caption is added in the kable function using caption = \"Table 1. Participant characteristics\". The footnote is added using a function from kableExtra, `add dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% kable(format = &quot;html&quot;, col.names = c(&quot; &quot;, &quot;Included&quot;, &quot;Excluded&quot;, &quot;Included&quot;, &quot;Excluded&quot;), caption = &quot;Table 1. Participant characteristics.&quot;) %&gt;% add_header_above(c(&quot; &quot;= 1, &quot;Female&quot; = 2, &quot;Male&quot; = 2)) %&gt;% add_footnote(label = &quot;Data are means and standard deviations (SD).&quot;, notation = &quot;none&quot;) Success! The table is complete. As with alot of things in R, there are at least five ways of getting to this end result, and there still might be some things to tweak to make it look even more like Table 1 in (Hammarström et al. 2020). 6.2 Summary In the example above we have introduced functions from dplyr, from the dplyr documentation: mutate() adds new variables that are functions of existing variables select() picks variables based on their names. filter() picks cases based on their values. summarise() reduces multiple values down to a single summary. arrange() changes the ordering of the rows. We have also explored pivot_wider and pivot_longer from the tidyr package. We used the above functions to prepare a table. The table was then made pretty by using kable from the knitr package. Functions from kableExtra gave us ways of making the table more suited to what we wanted to show. The vignette describining the kableExtra package is highly recommended! If you want to extend to pdf-tables (which are really nice!). Use the \\(\\LaTeX\\) vignette found here. When creating pdf/\\(\\LaTeX\\) tables/reports some basic understandning of \\(\\LaTeX\\) is good. 6.3 Flextable Flextable comes with the option of specifying tables directly in word outputs. This does not need any further formatting compared to HTML formatting. Lets use the the same pipe as used above to specify the flextable. Flextable also works as part of a pipe. The basic function is flextable. library(flextable) dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() We can now add changes to the underlying table and add new header labels to produce a table similar to the one above. library(flextable) dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() %&gt;% set_header_labels(variable = &quot;&quot;, incl_female = &quot;Include&quot;, excl_female = &quot;Exclude&quot;, incl_male = &quot;Include&quot;, excl_male = &quot;Exclude&quot;) Note that flextable does not need results = \"asis\" to work in rmarkdown files. Similar to kable, flextable can give a preview by just running the code, meaning you do not have to knit the whole document to see what changes you have done. Lets finnish up by adding a header and a footnote. dxadata %&gt;% # take the dxadata data set select(participant, time, sex, include:weight, fat.whole) %&gt;% # select participant, time, sex, include to height and fat.whole # Filter to keep all observations with pre filter(time == &quot;pre&quot;) %&gt;% # Calculate body fat # fat.whole in grams, needs to be divided by 1000 to express as kg # Multiply by 100 to get percentage mutate(fat.whole = ((fat.whole/1000) / weight) * 100) %&gt;% # Group the data frame and add a variable specifying the number of observations per group group_by(include, sex) %&gt;% mutate(n = n()) %&gt;% # Collect all variables for convenient summarizing pivot_longer(names_to = &quot;variable&quot;, values_to = &quot;value&quot;, cols = age:n) %&gt;% # Create a new grouping, adding variable group_by(include, sex, variable) %&gt;% # Summarize in two new variables m for mean and s for SD summarise(m = mean(value), s = sd(value)) %&gt;% # Add descriptive statistics together for nice formatting mutate(ms = if_else(variable == &quot;n&quot;, # If the variable is n as.character(m), # the only display the mean, otherwise: paste0(signif(m, 3), # Use signif to round to significant numbers &quot; (&quot;, signif(s, 3), &quot;)&quot;)), # Doing a new grouping variable include_sex = paste(include, sex, sep = &quot;_&quot;)) %&gt;% # removing unnecessary variables after ungrouping ungroup() %&gt;% select(-sex, -include, -m, -s) %&gt;% # pivot wider to match the desired data pivot_wider(names_from = include_sex, values_from = ms) %&gt;% mutate(variable = factor(variable, levels = c(&quot;n&quot;, &quot;age&quot;, &quot;weight&quot;, &quot;height&quot;, &quot;fat.whole&quot;), labels = c(&quot;N&quot;, &quot;Age (years)&quot;, &quot;Mass (kg)&quot;, &quot;Stature (cm)&quot;, &quot;Body fat (%)&quot;))) %&gt;% select(variable, incl_female, excl_female, incl_male, excl_male) %&gt;% arrange(variable) %&gt;% flextable() %&gt;% set_header_labels(variable = &quot;&quot;, incl_female = &quot;Include&quot;, excl_female = &quot;Exclude&quot;, incl_male = &quot;Include&quot;, excl_male = &quot;Exclude&quot;) %&gt;% # Adds a header specified for all columns of the table add_header_row(values = c(&quot;&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;)) %&gt;% # Merge the new cells with the same data # part = &quot;header&quot; meands that we want to add a row in the &quot;header&quot; part of the table. # i = 1 means row 1 # j = 2:3 means column 2 to 3 merge_at(part = &quot;header&quot;, i = 1, j = 2:3) %&gt;% merge_at(part = &quot;header&quot;, i = 1, j = 4:5) %&gt;% # Add footnote add_footer_row(values = &quot;Values are mean and (SD)&quot;, colwidths = 5) %&gt;% # Make the columns widths match the content autofit() Nice! This table should also print if the code chunk is included in a R Markdown file and the output is choosen to be Knit to word. See flextable documentation for more customizations. 6.4 An exercise, reproduce Table 1 from (Haun et al. 2019) In total 30 college students performed a heavy resistance training protocol where training volume was constantly increased over six weeks. In (Haun et al. 2018), a part of the study, focusing on supplementation was reported. In (Haun et al. 2019), participants were devided into two clusters based on training responses and the authors aimed to answer the question what separates high- from low-responders to resistance training. In this exercise we want to reproduce a big part of Table 1 in (Haun et al. 2019). The Table as re-produced here can be seen below. See the orginal article for explenation of clusters. To select variables, see the data description in the exscidata package, the dataset is called hypertrophy. Table 6.1: Table 1. Baseline characteristics at PRE and back squat training volume between clusters Variable LOW (n = 10) HIGH (n = 10) Age (years) 20.9 (1.9) 21.5 (1) Training age (years) 5.5 (2.3) 5.5 (2) Body mass (kg) 78.8 (8) 83.1 (12.8) DXA LBM (kg) 62.2 (5.9) 65.1 (9.7) DXA FM (kg) 13.5 (4.9) 14.5 (4.9) Type II fiber (%) 59.4 (16.9) 50.2 (13.6) 3RM back squat (kg) 127 (23.3) 135.4 (14.1) Total back squat training volume (kg) 106610.2 (18679.4) 111820.8 (12962.5) Click for a possible solution # load the data data(hypertrophy) hypertrophy %&gt;% # Select the variables needed to reproduce the table dplyr::select(PARTICIPANT, GROUP, CLUSTER, AGE, BODYMASS_T1, TRAINING_AGE, PERCENT_TYPE_II_T1, SQUAT_3RM, DXA_LBM_T1, DXA_FM_T1, SQUAT_VOLUME) %&gt;% # Pivot longer to gather all variables pivot_longer(cols = AGE:SQUAT_VOLUME, names_to = &quot;variable&quot;, values_to = &quot;values&quot;) %&gt;% # Remove participants not belonging to a cluster filter(!is.na(CLUSTER)) %&gt;% # Create a grouping before summarizing group_by(CLUSTER, variable) %&gt;% summarise(m = mean(values), s = sd(values)) %&gt;% # For nice printing, paste mean and SD mutate(m.s = paste0(round(m,1), &quot; (&quot;, round(s,1), &quot;)&quot;)) %&gt;% # Select only variables needed for the table select(CLUSTER, variable, m.s) %&gt;% # Transform the data set to a wide format based on clusters pivot_wider(names_from = CLUSTER, values_from = m.s) %&gt;% # Re-arrange the &quot;variable&quot; variable, correct order with levels, and correct labels mutate(variable = factor(variable, levels = c(&quot;AGE&quot;, &quot;TRAINING_AGE&quot;, &quot;BODYMASS_T1&quot;, &quot;DXA_LBM_T1&quot;, &quot;DXA_FM_T1&quot;, &quot;PERCENT_TYPE_II_T1&quot;, &quot;SQUAT_3RM&quot;, &quot;SQUAT_VOLUME&quot;), labels = c(&quot;Age (years)&quot;, &quot;Training age (years)&quot;, &quot;Body mass (kg)&quot;, &quot;DXA LBM (kg)&quot;, &quot;DXA FM (kg)&quot;, &quot;Type II fiber (%)&quot;, &quot;3RM back squat (kg)&quot;, &quot;Total back squat training volume (kg)&quot;))) %&gt;% # Sort/order the dataset arrange(variable) %&gt;% # Use kable to output the table with appropriate caption and column names. kable(col.names = c(&quot;Variable&quot;, &quot;LOW (n = 10)&quot;, &quot;HIGH (n = 10)&quot;), caption = &quot;Table 1. Baseline characteristics at PRE and back squat training volume between clusters&quot;) See for example the description of the gt package that list a bunch of table generators. All this talk about pivot, take a break and watch this clip from the hit series Friends, its about pivot! "],["writing-your-first-reproducible-report.html", "Chapter 7 Writing your first reproducible report 7.1 RStudio projects and your reproducible report", " Chapter 7 Writing your first reproducible report As we have already discussed, the degree to which research is reproducible is to a degree determined by the availability of: The data Code to analyze the data To make these ingredients even more tasty, we might want to have them nicely stored together. A way of doing this using the tools we discuss in this course is to think of data analysis projects as self-contained projects with all necessary ingredients. In RStudio, projects can help you organize your data and code in one place. You can also link your project to an online repository for others to access. In this chapter we will discuss the reproducible report as belonging to such a project. The online repository/sharing part will be discussed in the next chapter. 7.1 RStudio projects and your reproducible report When you build an analysis in a R markdown file, R will use the folder that the file is in as the root directory. This directory (or folder) is the top directory in a file system. This means that R will look for data or other files used to generate the report in this folder structure. Think of this folder as ./ (confusing, I know! But bare with me!). Any sub-folders to the root directory can be called things like ./data/ (a folder where you keep data files), ./figures/ (a folder where you output figures from analyses). The R markdown file, being in the root directory will have the address ./my_rmarkdown_file.Rmd This has several advantages, as long as you stick to one rule: When doing an analysis, always use relative paths (addresses to files and folders). Never reference a folder or file by their absolute path. The absolute path for the file Im writing in now is C:/Users/706194/Dropbox/Undervisning%20och%20kurser%20HIL/IDR3002%20Course%20notes/IDR3002/markdown.Rmd. The relative path is ./markdown.Rmd. When working in a project you may move the folder containing your project to other locations, but relative paths will not break. If you want to share your analysis, all you need to do is share the folder with all content with your friend. If you use relative paths, everything will work on your friends computer. If you use absolute paths, nothing will work, unless your friends computer uses a very similar folder structure (highly unlikely). RStudio projects makes it easy to jump back and forth between projects. The project menu (top right corner in RStudio) contains all your recent projects. When starting a new project, R will create a .Rproj file that contains the settings for your project. If you start a project a click this file, a settings menu will appear where you can customize settings for your particular project. What does this have to do with my RMarkdown file? As mentioned above, the RMarkdown file is often written in a context where you have data and other files that help you create your desired output. By always working in a project makes it easy to keep every file in the right place. "],["version-control-and-collaborative-coding-introducing-git-and-github-com.html", "Chapter 8 Version control and collaborative coding  introducing git and github.com 8.1 Why version control 8.2 A simple/complicated start 8.3 Step by step instructions on setting up a version controlled R project. 8.4 Forking and pull requests 8.5 Additional great things about GitHub 8.6 When will this knowledge be handy? 8.7 Resources", " Chapter 8 Version control and collaborative coding  introducing git and github.com In the previous chapter we underlined the importance of the project as a way of keeping data, code (and text) in an organized manner. The project concept in RStudio can easily be extended to include version control and ways to collaborate on different kinds of projects. Most often, collaborating on writing a report, assignment or paper is hard. You send a file, get another one in return. Some files are on dropbox, some are lost. What if we had a system for collaboration that made it easy to follow the progress of a project. Connecting R projects to git and GitHub makes this possible. 8.1 Why version control Github is a platform for collaborative coding. As we have noted before, collaboration concerns both others and you, in the future! This means that having a formal system for keeping track of your projects is a good thing. Github also provides version control. Version control can help you track changes in your entire analysis or writing project. This is helpful when multiple files make up a complex project, including e.g. scripts, data and manuscript files. It is also helpful when multiple collaborators work together (e.g. writing a report). You will, by using version control, avoid overwriting other peoples work. With multiple changes made to the project, merging will create the latest up-to-date version. When you change a file in your analysis you will be required to describe the changes you have made. Git creates a record of your changes. This also means that we have backups of previous versions. 8.2 A simple/complicated start I use version control when working in R by setting up repositories on www.github.com and then cloning it to my local computer. A repository is a folder containing all the files in a specific project. Using projects in RStudio makes it easy to synchrionize local projects with the online version control system. When cloning a project, you download all files to your personal computer, you are then free to work on the project without interference from others. When you have created a new file you add the file to version control and commit the changes. This means that your change has got a unique identity in the history of your project. You may now push changes to the online repository which is the online version of your work. When collaborating with others, pull requests makes it easy for others to make changes to your repository that you have to accept or decline. This is somewhat equivalent to suggesting a change in a word document with track changes activated. 8.3 Step by step instructions on setting up a version controlled R project. 8.3.1 Step 1. Create a free github account: Go to www.github.com, press sign up and follow the instructions. 8.3.2 Step 2. Download git. Git is the software responsible for actually keeping track of all your files. We need to have this installed locally to make this work. Go to https://git-scm.com/downloads and download the version compatible with your system (windows or mac). Install git by following the instructions. 8.3.3 Step 3. Connect git to RStudio In RStudio, click Tools -&gt; Global options -&gt; Git/SVN. Click to Enable version control interface . Now you have to point RStudio to your git executable. On my computer after following the standard installation of git, the executable is found in: C:/Program Files/Git/bin/git.exe. Browse to find your copy. RStudio has to be restarted to make the changes work. 8.3.4 Step 4. Create a new online repository Go to www.github.com and sign in using your user name and password created in step 1. Click New to create a new repository. Give the repository a name. If you want you can add a description of the repository, this can be something like statistics report 5 in IDR4000. You can decide if you want the repository to be public or private. To share it with others without giving special permissions, it needs to be public. Under Initialize this repository with you can select if you want to start your repository with a README file. This is good if you want to describe what the repository contains. After selecting what you want, press Create repository. Copy the link on the resulting page under quick setup. 8.3.5 Step 5. Create a new project in RStudio We will now connect the online repository to a RStudio project. Click the projects button in the top right corner and select new project, select a version controlled project, select Git and paste the link from github (step 4) into the field for repository URL. 8.3.6 Step 6. Make changes to your project You now have an empty project. You can add for example a R Markdown file by writing one and saving it in your local repository. In the tab containing the Environment, History etc. you should have a tab that says Git. Here you can commit changes and push them. Let us say that you have created a file called report.Rmd. In this file you have written an analysis. You now want to add these changes to version control. You can do this by clicking the file under Staged and then commit. You will need to write a commit message, a short description of what you have done and the press commit. The changes are now saved and version controlled. To upload these changes to your online repository, press push. When you have a lot of changes RStudio is a bit slow if you use the interface. An alternative is to use the terminal. A simple setup is to use the git bash terminal. Under Tools, go to Terminal and terminal options. Select Git Bash in the list New terminals open with. Where you have the console, there is also a tab called Terminal, if not, start a new one with Tools -&gt; Terminal -&gt; New Terminal. The simplest commands are as follow: You have made changes and want to upload them, in the terminal, write: git add -A This adds all changes to your next commit, next, write: git commit -m \"A commit message\" The A commit message is the description of the changes you have made. And finally write: git push This will push your changes to your repository. Lets say that someone else have made changes, or you have made changes online to your repository. You then want to start by downloading the latest changes to your computer. If you are using the RStudio interface, under Git press pull. If you are using the terminal, write: git pull 8.4 Forking and pull requests Creating a fork in git means that you make a copy of someones repository. In the web interface you can press Fork in the upper right corner when you are visiting another repository. This will copy the content of the repository to your collection of repositories. You may now change the content of the repository and issue a Pull request. The Pull request are suggested changes to the project (or repository). The Pull request may then be scrutinized by the maintainer of the original repository, they may accept the changes and if so, you have contributed in the project. 8.5 Additional great things about GitHub GitHub has great capabilities for managing projects. You can for example: Post issues that are suggestions or questions regarding a repository. Issues can be categorized with labels and assigned. You can create to-do lists in the Projects tab (in the web interface). This could be a nice way of sharing and tracking the progress of a project. You can build a wiki. This is simply a collection of pages that can be used to document the repository or a project (in a wider sense) that you are working on. All of the above can be private and public. You can choose whom have access to your repository. This makes it easy to work on a project even if you need to keep things a secret. In this course I want you to contribute to the wiki pages of the course. The wiki is hosted at github.com/dhammarstrom/IDR4000-2021. To contribute you need to have created your own GitHub user account. 8.6 When will this knowledge be handy? When writing your master thesis, it will be extremely easy to share your code with your supervisor or other students, whit whom you collaborate. You can just invite someone to make changes in your repository and then download them. As noted several times before, your most frequent collaborator is you. Using git makes it easy to keep track of changes in your project and it keeps your most frequent collaborator from messing up your work. Version control workflows are part of almost all technology companies, and will most certainly be part of many more types of businesses, institutions and workplaces in the future as we need to collaborate on large, complex projects. Knowing about these systems is in that sense quite handy! 8.7 Resources There are of course more functions in git, here are some resources for deeper understanding. Extensive resources can be found on Happy Git and GitHub for the useR Karl Broman provides a minimal tutorial GitHub hosts resources for learning Git "],["reliability-in-the-physiology-lab.html", "Chapter 9 Reliability in the physiology lab 9.1 Smallest worthwhile change or effect 9.2 Video from seminar", " Chapter 9 Reliability in the physiology lab We will follow Hopkins definition of reliability in this section (Hopkins 2000), and specifically concern ourselves with reliability in test/measurements that produces continuous numbers as results. For this lab you have collected data in several tests, at least two times, from several individuals. You can now estimate the reliability of each test. The reliability, expressed as typical error is an estimate of what we may expect as the random variation around a measurement if repeated under similar circumstances in the same individual (Hopkins 2000). Hopkins (Hopkins 2000) details two measures of reliability for estimates of within-individual reliability that are closely related, the typical error and limits of agreement. Using the example data in (Hopkins 2000) we can calculate first the typical error. We will take advantage of the change scores between two duplicate measures and derive the estimate of the typical error from them. (To run the code, copy and paste into your own Rmarkdown file). library(tidyverse) example_data &lt;- data.frame(trial1 = c(62, 78, 81, 55, 66), trial2 = c(67, 76, 87, 55, 63)) # Calculate the typical error example_data %&gt;% mutate(diff = trial2 - trial1) %&gt;% # Change/difference score summarise(s = round(sd(diff), 1), # Summarize to calculate sd, and... te = round(s / sqrt(2), 1)) %&gt;% # the typical error. # Round is used to get less decimal places... print() The interpretation of the typical error is that this is the typical variation of the test. If you repeat the test, this is the random variation that can be expected. If expressed as a percentage of the mean, it can be compared to other tests. In the code below, we will add the mean to allow calculation of the coefficient of variation (CV, or as referred to in (Hopkins 2000), typical percentage error). # Calculate the typical error example_data %&gt;% mutate(diff = trial2 - trial1) %&gt;% # Change/difference score summarise(s = round(sd(diff), 1), # Summarize to calculate sd, and... m = mean(c(trial1, trial2)), # mean te = round(s / sqrt(2), 1), # the typical error. cv = round(100 * (te / m), 1)) %&gt;% # Calculate as a percentage of the mean # Round is used to get less decimal places... print() The limits of agreement has a nice interpretation. If the limits of agreement are created as the 95% limits of agreement, there is a 1 in 20 chance of finding a test score outside these limits. The limits of agreement are calculated using a t-distribution. We will add to the code chunk to calculate the limits of agreement. # Calculate the typical error example_data %&gt;% mutate(diff = trial2 - trial1) %&gt;% # Change/difference score summarise(s = sd(diff), # Summarize to calculate sd, and... m = mean(c(trial1, trial2)), # mean te = s / sqrt(2), 1, # the typical error. cv = 100 * (te / m), 1, L = qt(0.975, 4) * s) %&gt;% # Calculate as a percentage of the mean print() The part with qt(0.975, 4) is the R-code equivalent to \\(t_{0.975, 4}\\) (See equation 1 in (Hopkins 2000)) which is the t-distribution with 4 degrees of freedom. The degrees of freedom comes from the number of participants in the data set minus 1 (\\(n - 1\\)). As noted by Hopkins, we may use the CV in the calculation of limits of agreement. 9.1 Smallest worthwhile change or effect When we do not have any measures of reliability, nor clinically relevant thresholds for a test, the smallest worthwhile effect may provide an indication of important changes in a test. An arbitrary number of the smallest effect of interest in an average change score has been defined as 20% of the between participant standard deviation (\\(0.2\\times SD\\)). This tells you nothing about the reliability of your test, it simply gives a proportion of the expected population variation. 9.2 Video from seminar Part 1: Part 2: "],["assignment1.html", "Chapter 10 Assignment 1: Reliability and tools for reproducible data science 10.1 Elements of the report 10.2 The format of the report 10.3 How to hand in the report.", " Chapter 10 Assignment 1: Reliability and tools for reproducible data science The purpose of this assignment is to present estimates of reliability of measures collected in the physiology lab. A second purpose is to use tools for reproducible data science. The report that you are expected to hand in therefore has some strict requirements in its format (see below). The assignment is a group assignment and at least three students are expected to contribute to each report. 10.1 Elements of the report The report should describe one test that you have performed in the physiology-lab. Select the test that is most interesting to you. The test should be described with a detailed protocol, including preparations of the participant (that is being tested), standardization of the test, and post-test data preparation. Post-test data preparation refferes to steps needed to get data from e.g. equipment used during the test. This section should take into account and reference (Halperin, Pyne, and Martin 2015; Tanner and Gore 2012) The next section should contain descriptive data from the test. This could be measures of central tendency and variability in the measures you have collected. If possible, try to find similar estimates in the scientific literature. Finally, we are interested in reliability. Here you need to calculate an estimate of reliability of the test. Use (and reference) (Hopkins 2000). Try to be clear with what measure of reliability you are using and what it is telling you. 10.2 The format of the report The report should be uploaded to github with both a raw .Rmd file and a .html file as output. The github folder should also contain the datset being used in the calculations. Contributions from members of the group can be made directly to the github directory. Follow this checklist on how to get started: (A nice introduction to github with R can be found here) Create a github account. All members of the group should have their own account. Once signed in to github.com, create a new repository with an informative name. Make sure that the repository is public. One member of the group can create this repository in their own account. Be sure to download and install git on your computer. Start up RStudio, start a new project, select the Version Control option and copy the address to the repository. Add files to your project and upload to github. You need to add files, commit them and push using git. Use the command line in your terminal: git add -A git commit -m &quot;A short message to describe your changes&quot; git push When you want to download the latest version of your project, type in the terminal: git pull You may encounter conflicts if pushing changes that overwrites changes made by other group members. These may be tricky but can be resolved Have patience! 10.3 How to hand in the report. Copy the link to your github folder into canvas. "],["introduction-to-the-molecular-exercise-physiology-lab.html", "Chapter 11 Introduction to the molecular exercise physiology lab 11.1 Health and safety in the lab 11.2 Good laboratory practice 11.3 The laboratory journal 11.4 Keeping it digital - Using our electronic lab journal 11.5 Protocols and experiments in the course", " Chapter 11 Introduction to the molecular exercise physiology lab 11.1 Health and safety in the lab 11.2 Good laboratory practice 11.3 The laboratory journal Your laboratory journal fills several purposes. It helps you in keeping track of what you have done, why you did it, what were the results of your experiments and what were your conclusions. The journal also helps your collaborators (fellow students, supervisors, laboratory managers) to understand what you have done. In a broader perspective the laboratory journal can be viewed as a primary source of data for your prospective manuscript, and when the manuscript has been published, the journal is source data kept for records at your department. 11.4 Keeping it digital - Using our electronic lab journal After a couple of years running our lab in Lillehammer, we have arrived at the conclusion that we need to organize the molecular lab using a electronic laboratory journal system. This will solve several issues with organizing laboratory work like storage of data, collaborating, running multiple projects in parallel, having guests and students contributing to projects etc. Traditionally, laboratory journals are stored in a fire safe cabinets in the lab, in notebooks. In some labs, each page must be read and understood and subsequently signed by a lab administrator or principle investigator. People working in laboratories typically have strong feeling about their laboratory journal system, regardless of digital or analog alternatives. The above observations indicate that, as a new lab member you should take care to learn the laboratory journal system and culture of the lab. This will make your work more important to the lab, as it will contribute to the collective knowledge based on the groups work. Our electronic laboratory journal system can be found at elab.inn.no. As a student at inn you may log in using your student username and password. The system has link to the documentation of the software which can also be found through this web address: doc.elabftw.net/. As a student you will be added to the group related to this course. 11.4.1 What to write in the journal A laboratory journal should consist of all necessary information needed to replicate your experiments. The tricky thing is that you do not always know what information is needed to do a replication. This means that you will need to include all relevant information about your experiment, and a bit more, within reasonable limits. 11.4.2 Structure of an entry In the electronic journal (elab.inn.no), entries are recorded as experiments. An experiment starts on a specific date, it has a status, a title and tags can be added. This will make it easy to keep track of the entry. eLabFTW will keep a specific record of your entry and time stamp it. The actual entry has three parts, namely purpose, methods and results. These are suggested in the template when you start a new entry. The purpose of an experiments should give a clear description of why you are performing the experiment. This description could be To extract and analyze DNA for ACTN3 genotyping. Further descriptions could be necessary if you are performing more complex experiments. Maybe you found something out in previous experiment that needs to be validated in this experiemnt. If you spend some time to write this up, your subsequent report will be easier to write and your collaborators will understand what you did. The methods in an experiment need to include all relevant information about your experiment, and a bit more, within reasonable limits, as already mentioned above. A detailed protocol should be included. If you are using pre-written protocols, this entry should refer to this protocol and include any changes made to the protocol. You should also note batch numbers on chemicals and reagents etc. If an experiment fail, a complete record of reagents and machines etc. will make it easier to track potential methodological issues. The method describes the experiment, e.g. by a step-by-step approach. This section includes recipes of solutions, what samples were used, in what order different steps of the protocol was done, if any problems occurred and so on. Remember to write explicit, you might remember tomorrow, but in one week or one year, you have no idea. A method section (as the other sections) can refer back in the laboratory journal system. For example, maybe you prepared and validated the lysis buffer in a previous experiment, write this and refer to the experiment describing the buffer. A part of a method section can look something like this: Overview: Freeze dried muscle samples are homogenized in lysis buffer (Refer to experiment: Lysis buffer test) and protein concentrations are determined in the plate reader (raw data included below). Based on protein concentrations, supernatant is normalized to a common 2 g/l. Step-by-step: 1. Freeze dry muscle over-night (at least 20 h). 2. Dissect away fat, connective tissue and blood 3. Move ~2 mg to a new tube and add 80 l of ice-cold lysis buffer per mg of tissue. 4.  Finally, the results should be a description of the actual results, and a description of what this means. If you would pick up your experiment later this will help you understand the results. This also applies to collaborations. In eLabFTW you can include raw data files. This will be the primary way of storing raw data from machines and instruments. Importantly, eLabFTW will not give you a way of structuring large amount of data. This step could instead be done outside eLabFTW. 11.4.3 Relationship between the laboratory journal and your samples, solutions, tubes etc. When you do work in the laboratory, you will notice that you accumulate a lot of micro-centrifuge tubes, boxes full of intermediate sample preparations, solutions etc. To keep track of all this you need to connect the content of your laboratory journal entries to the place where you keep all your stuff. For example, when marking a new cryo-box, this should be done with information about what is the content of the box, the date when the box was started, your name, and the experiment. Your name, date and experiment can be tracked back to your laboratory journal entries. This means that when your collaborator finds a mystical Sample X1 2017 10 10, experiment Y, she can go back to your journal and find out what you did. In summary: label everything! 11.5 Protocols and experiments in the course Protocols used in the course are as for now available at trainome.github.io. "],["the-linear-model.html", "Chapter 12 The linear model 12.1 Resources 12.2 Straight lines 12.3 Fitting regression models in R 12.4 Check the results 12.5 A note about printing the regression table 12.6 Interpreting the results 12.7 Do problematic observations matter? 12.8 A more intepretable model 12.9 An exercise", " Chapter 12 The linear model 12.1 Resources Chapter 5 in Spiegelhalter serves as a very good introduction to regression. There are several texts on Regression, Navarro provides a nice introduction with references to R in Learning statistics with R, Chapter 15. 12.2 Straight lines A straight line can be used to describe a relationship between two variables. This relationship can also be described with a formula: \\[y = \\beta_0 + \\beta_1x\\] Where \\(y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope and \\(x\\) is the predictor. In the model shown above, \\(y\\) increases two units for every unit increase in \\(x\\). If we measure something in nature and find such a fit (every point on the line) we should check our calculations as perfect relationships are seldom found in measured variables. This because of measurement error and other non measured variables that affect the relationship. We quantify these unmeasured sources of variation in an additional term in the formula: \\[y = \\beta_0 + \\beta_1x + \\epsilon\\] \\(\\epsilon\\) is the error-term. Here we quantify the distance from the best fit line for every observation. The best fit line is the line that minimizes the squared residuals (\\(e_i^2\\), in this case \\(e\\) is used to signify the error as it is observed data). A more realistic regression model contains some error. In the above figure, the error or residual is the distance (red lines) from the predicted (blue points) to the observed (black points). A two variable relationship can be positive (increase in \\(y\\) as \\(x\\) increases) or negative (\\(y\\) decreases as \\(x\\) increases). 12.3 Fitting regression models in R The data above can be fitted to a regression model in R using the lm() function. We will get far by specifying a formula and data where the variables used in the formula are stored. We can store a model as an object and inspect the results by using the summary() function. df &lt;- data.frame(x = c(5.461851, 6.110910, 6.952707, 5.321775, 5.951849), y = c(9.168992, 8.273749, 5.926797, 10.745583, 7.999151)) fit &lt;- lm(y ~ x, data = df) summary(fit) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## 1 2 3 4 5 ## -0.5561 0.2460 0.1005 0.6542 -0.4445 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.0085 2.6873 8.934 0.00296 ** ## x -2.6151 0.4488 -5.827 0.01007 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5789 on 3 degrees of freedom ## Multiple R-squared: 0.9188, Adjusted R-squared: 0.8917 ## F-statistic: 33.95 on 1 and 3 DF, p-value: 0.01007 The summary that you get from the model above will show the value of the coefficient (estimates, standard error, t-value and a p-value), we will get some information about the spread of the residuals and values that tells us the overall fit of the model. The estimates from a summary in a two variable situation tells the value of \\(y\\) when x is zero (the intercept) and the increase in \\(y\\) for every unit increase in \\(x\\) (the slope). Can you identify these from the output above? Two-variable regression (univariate regression) is closely related to the correlation. Try out the code cor.test(df$x, df$y) and see what similarities you find between the outputs. In the model we use in the example, the intercept is quite far away from the rest of the data (see figure below). This is, as noted above, because the intercept is the value of \\(y\\) when \\(x\\) is set to zero. Lets fit some real data. We might wonder if there are some characteristic that is related to VO2max. For example, do taller individuals have greater VO2max? It is always a good idea to start with a plot before we do the modeling. library(tidyverse) library(exscidata) data(&quot;cyclingstudy&quot;) cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height.T1) %&gt;% ggplot(aes(height.T1, VO2.max)) + geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + labs(x = &quot;Height (cm)&quot;, y = expression(&quot;VO&quot;[&quot;2max&quot;]~(ml^-1~min^-1))) + theme_minimal() There might be a positive relationship, what do you think? You might get a clearer picture if you use geom_smooth(method = \"lm\") in your ggplot command, try it out! To quantify the relationship between Height (height.T1) and VO2max (VO2.max) we can fit a linear model. Below I store the model in an object called m1. Before we look at the results of the regression model, we should think about the data and inspect the fit to see if it matches with our assumptions. Assumptions that generally needs to be met in order to get a valid regression model are: Independent observations. This is an assumption about the design of the study and the data at hand. If we have observations that are related, the ordinary linear model will give us biased conclusions. As an example, if we collect data from the same participants over time we will not have independent observations and this will lead to pseudo-replication, lower standard errors and biased confidence intervals. Another way to see it is that non-independent observations will give non-independence of the residuals which is the mechanism that creates bad inference (as the residuals are used to estimate the sampling distribution of parameters). Linear relationship. In the basic case, we expect a linear trend that can be described with a straight line. If the relationship is curve-linear, we may adjust the fit using e.g. polynomials. Normal residuals. This condition might be violated when there is an outlier. Constant variance. This assumption says that we want to be equally wrong all along the explanatory variable. If we predict \\(y\\) with greater error at large \\(x\\) we have heteroscedasticity (unequal variance), if we are equally wrong we have homoscedasticity (equal variance). 12.3.0.1 Code for fitting a preliminary model cyc_select &lt;- cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height.T1) m1 &lt;- lm(VO2.max ~ height.T1, data = cyc_select) 12.3.1 Linear relationship The plot can be used to see if the relationship is generally linear. We do not have that many data points, but a curve-linear relationship is not evident. 12.3.2 Constant variance This assumption can be checked by creating a residual plot. We will do it here by hand to see how it works. The model is fitted and stored in the object m1. From this object we can use the residuals() function to get every residual. We can add this data to the data set by creating a new variable called resid. It is common practice to plot the residuals against the fitted values. We can get the fitted values using the fitted(), these are the predicted values from the model. cyc_select$resid &lt;- residuals(m1) cyc_select$fitted &lt;- fitted(m1) We will now plot the fitted values against the residuals. If the model is equally wrong all along the fitted values (or the predictor values), we have homoscedasticity. The residual plot should not show any obvious patterns. library(tidyverse) cyc_select %&gt;% ggplot(aes(fitted, resid)) + geom_hline(yintercept = 0) + geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + theme_minimal() Sometimes you will see standardized residuals. This is the residual divided by the standard deviation of the residual. We can create this standardization like this: cyc_select %&gt;% mutate(st.resid = resid/sd(resid)) %&gt;% ggplot(aes(fitted, st.resid)) + geom_hline(yintercept = 0) + geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + theme_minimal() Looking at the plot tells us that observation with the largest error is about 2.5 standard deviation away from its predicted value. We are suffering a bit from having a small amount of data here. But the residual plot does not invalidate the regression model. 12.3.3 Normal residuals To check if the residuals are normal, we can create a plot that plot every observed residual against its theoretical position in a normal distribution. This is a quantile-quantile plot. To show the concept we may sample data from a normal distribution and plot it against the theoretical quantile. set.seed(1) ggplot(data.frame(y = rnorm(100, 0, 1)), aes(sample = y)) + stat_qq(size = 3, fill = &quot;lightblue&quot;, shape = 21) + stat_qq_line() + theme_minimal() The code above samples 100 observations. They are plotted against their theoretical values. If the values (points) follows the straight line, we have data that follows a normal distribution. The same can be assessed from our fitted model. cyc_select %&gt;% mutate(st.resid = resid/sd(resid)) %&gt;% ggplot(aes(sample = st.resid)) + stat_qq(size = 3, fill = &quot;lightblue&quot;, shape = 21) + stat_qq_line() + theme_minimal() The resulting plot looks nice. Except from one or two observation, the residuals follows the normal distribution 12.3.4 Independent observations This is an assumption about the data and design of the study. We created the model based on values only from the pre-training test. If we would have used all observations (all time-points) we would have violated the assumption. 12.4 Check the results To examine the results of the analysis we can use the summary() function. summary(m1) The output will show you the following things: Residuals which contains the minimum, maximum, median and quartiles of the residuals. The tails should be approximately similar above and below the median. Coefficients contains the estimates and their standard errors. As we have fitted a univariate model, we only see the increase in VO2max with every unit increase of height.T1 and the intercept. R-squared shows the general fit of the model, this is a value between 0 and 1 where 1 shows if the data fits perfectly. 12.5 A note about printing the regression table We might want to print the regression table in our reports. To do this in a nice way we might want to format the output a bit. This can be done using a package called broom. broom is not part of the tidyverse so you might need to install it. The package has a function called tidy that takes model objects and formats it into nice data frames that are more easy to work with. Together with the knitr package we can create tables for use in the report. The knitr package contains the function kable that makes nice tables with some arguments to format the table. library(knitr); library(broom) tidy(m1) %&gt;% print() To get a table out of this for the report we need to set a chunk option, results = 'asis'. The kable() function is used to create the table. It can take several arguments that we can use to customize the table. Below I set the following settings col.names = c(\"\", \"Estimate\", \"SE\", \"t-statistic\", \"p-value\") which names the column of the table, digits = c(NA, 1, 1, 2, 3)which sets the number of decimals of numeric variables. library(knitr); library(broom) tidy(m1) %&gt;% kable(col.names = c(&quot;&quot;, &quot;Estimate&quot;, &quot;SE&quot;, &quot;t-statistic&quot;, &quot;p-value&quot;), digits = c(NA, 1, 1, 2, 3)) Estimate SE t-statistic p-value (Intercept) -2596.3 2936.5 -0.88 0.388 height.T1 41.1 16.4 2.51 0.022 The kable() function is easily extended using another package called kableExtra(). See the vignettes for more details. 12.6 Interpreting the results From our model we can predict that a participant with a height of 175 cm will have a VO2max of 4597 ml min-1. We can do this prediction by combining the intercept and slope term multiplied with 175 as x-value. Remember the equation: \\[Y = \\beta_0 + \\beta_1X\\] \\[VO_{2max} = -2596.3 + 41.1 \\times 175 \\\\ VO_{2max} = 4597\\] As we have estimated this relationship we get the \\(\\beta\\)s from the regression table and can input 175 instead of X. Actual values from the regression table can be accessed from a tidy table created with broom. But we can also use coef() to get the coefficients. Using confint() we will get confidence intervals for all parameters in a linear model. # Coefficients coef(m1) # Confidence intervals confint(m1) We will talk more about confidence intervals, t-values and p-values in later chapters. For now, a small introduction may be enough. The confidence interval can be used for hypothesis testing, so can also p-values from the summary table. The p-values tests against the null-hypothesis that the intercept and slope are 0. What does that mean in the case of the intercept in our model? The estimated intercept is -2596 meaning that when height is 0 the VO2max is -2596. We are very uncertain about this estimate as the confidence interval goes from -8766 to 3573. We cannot reject the null. Think a minute about what information this test may give in this situation. The slope estimate has a confidence interval that goes from round(confint(m1)[2], 1) to 75.5 which means that we may reject the null-hypothesis at the 5% level. The test of the slope similarly test against the null-hypothesis that VO2max does not increase with height. Since our best guess (the confidence interval) does not contain zero, we can reject the null hypothesis. 12.7 Do problematic observations matter? In the residual plot we could identify at least one potentially problematic observation. We can label observations in the residual plot to find out what observation is problematic. cyc_select %&gt;% mutate(st.resid = resid/sd(resid)) %&gt;% ggplot(aes(fitted, st.resid, label = subject)) + geom_hline(yintercept = 0) + geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + geom_label(nudge_x = 25, nudge_y = 0) + theme_minimal() The plot shows that participant 5 has the largest residual. If we would fit the model without the potentially problematic observation we can see if this changes the conclusion of the analysis. cyclingStudy_reduced &lt;- cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;, subject != 5) %&gt;% select(subject, group, VO2.max, height.T1) m1_reduced &lt;- lm(VO2.max ~ height.T1, data = cyclingStudy_reduced) delta_beta &lt;- 100 * (coef(m1_reduced)[2]/coef(m1)[2] - 1) The delta beta above calculates the percentage change in the slope as a consequence of removing the observation with the greatest residual. The slope changes 13% when we remove the potentially problematic observation. This might be of importance in your analysis. Another way to look for potential influential data points would be to check the scatter plot. cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height.T1) %&gt;% ggplot(aes(height.T1, VO2.max, label = subject)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_point(size = 3, fill = &quot;lightblue&quot;, shape = 21) + labs(x = &quot;Height (cm)&quot;, y = expression(&quot;VO&quot;[&quot;2max&quot;]~(ml^-1~min^-1))) + geom_label(nudge_x = 1, nudge_y = 0) + theme_minimal() The plot will show participant 5 has not got a lot of weight in the slope. If an equally big residual would have been present in the far end of the range of the height variable, removing it would have made more difference. Since the observation is in the middle of the xs, it wont be that influential. There are many ways of doing diagnostics for the ordinary linear model in R. The simplest way is to write plot(m1), this will produce four graphs. Residuals vs. Fitted shows the fitted (or predicted) values against the residuals. If we would have tried to fit a linear trend to curve linear data, we would have catch it here. We want equal spread all along the fitted values. We test the assumption of homoscedasticity and linear trend. Normal Q-Q shows residual theoretical quantiles against the observed quantile. The points should to a large degree be on, or close to the line. We test the assumption of normality in the residuals. Scale location similarly to the residual plot, we can assess assumptions of heteroscedasticity and if we find the trend in the data. We are looking for a straight, flat line and points equally scattered around it. Residual vs. Leverage is good to find influential data points. If a point is outside the dashed line it changes the conclusion of the regression to a large degree. Remember that we identified participant 5 as a potential problematic case. The Residual vs. leverage shows that number 5 has a large residual value but no leverage, meaning that it does not change the slope of the regression line. 12.8 A more intepretable model The intercept in model m1 is interpreted as the VO2max when height is zero. We do not have any participants with height zero nor will we ever have. A nice modification to the model would be if could get the intercept to tell us something useful. We could get the model to tell us the VO2max in the tallest or shortest participant by setting them to zero. Even more interesting would be to get the VO2max at the average height. We accomplish this by mean centering the height variable. We remove the mean from all observations, this will put the intercept at the mean of heights as the mean will be zero. cyc_select &lt;- cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height.T1) %&gt;% mutate(height.mc = height.T1 - mean(height.T1)) # mean centering the height variable m2 &lt;- lm(VO2.max ~ height.mc, data = cyc_select) Examine the fit, what happens to the coefficients? 12.9 An exercise We think that body dimensions influence physiological characteristics. To test if if the stature (height.T1) influence maximum ventilatory capacity (VE.max) fit a regression model, check model assumptions and interpret the results. Here is a possible solution ## Load data cyc_select &lt;- cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VE.max, height.T1) %&gt;% mutate(height.mc = height.T1 - mean(height.T1)) # mean centering the height variable # fitting the model m1_ve &lt;- lm(VE.max ~ height.mc, data = cyc_select) # Check assumptions plot(m1_ve) # Check coefficients summary(m1_ve) # Get confidence intervals confint(m1_ve) "],["predictions.html", "Chapter 13 Linear and curve-linear relationships, and predictions 13.1 Predicting from data 13.2 The workload-lactate relationship", " Chapter 13 Linear and curve-linear relationships, and predictions In the previous chapter we looked at linear relationships, we can imagine these as straight lines. Many things in life are not straight. In this chapter we will add curve-linear relationships to our repertoire. We will start by predicting values. 13.1 Predicting from data Because of the relationship between inner dimension and our height, we might expect to see a relationship between body height and VO2max.The idea is that we will build a model and use this model to make predictions of our outcome with new data. Later we will hopefully see that this is one of the meany benefits of the powerful regression model technique. As a first step, it is a good idea to get a visual representation of the prospective model. In the code chunk below we load the cyclingstudy data set from the exscidata package together with loading tidyverse. We then plot the relationship between height and VO2.max. In the ggplot call, a good starting point is to use geom_point together with geom_smooth which will produce a scatter plot with a best fit line. Notice that method = \"lm\" and se = FALSE are being used to make sure you get a straight line (method = \"lm\") and no confidence bands (se = FALSE). Copy the code into your own document and run the code. library(tidyverse) library(exscidata) data(&quot;cyclingstudy&quot;) # A simple plot of the associtaion cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height = height.T1) %&gt;% ggplot(aes(height, VO2.max)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) We will now construct the model. The lm function (for linear model) take a formula and a data set in its simplest form. We have to save the output of the model in an object to be able to work with it down the line. In the code below I suggest storing the model object as m1. # Store the data set needed in the model fitting dat &lt;- cyclingstudy %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% select(subject, group, VO2.max, height = height.T1) %&gt;% print() # Fit the model m1 &lt;- lm(VO2.max ~ height, data = dat) The above code will have stored an object in your environment. Using this object we may now make predictions. The manual way of making prediction would be to get the coefficients from the model an use them with new data. In the code chunk below I retrieve coefficients from the model representing the intercept and slope of the model. Remembering the basic mathematics of this simple model tells us that we can predict VO2max using the estimates from the model. These estimates can be retrieved using coef(). The intercept will be the first coefficient (coef(m1)[1]) and the slope will be the second (coef(m1)[2]). Adding the together an multiplying the slope with our new data will get us the prediction. new_height &lt;- 185 prediction &lt;- coef(m1)[1] + coef(m1)[2] * new_height R has some built in functions for these kind of operations. The predict function can be used to calculate what each observation would look like if it were on the regression line. Using predict on the model without any new data will give you the same values as the fitted function, try it out! # Store output pred &lt;- predict(m1) fit &lt;- fitted(m1) # Plot the values data.frame(pred, fit) %&gt;% ggplot(aes(pred, fit)) + geom_point() predict has an argument called newdata, here we can use a new data frame with the same predictors as in the data set used to fit the model. Using this new data set we may do several predictions from our model. # New Data Frame containing data we want to predict with ndf &lt;- data.frame(height = c(160, 170, 180, 190)) predictions &lt;- predict(m1, newdata = ndf) Unsurprisingly, an increased height gives us higher predictions of VO2max. What would be your VO2max given this model. 13.2 The workload-lactate relationship In the cyclingstudy data set, data from lactate threshold tests are recorded for each participant. We need to wrangle the data set a bit to get the data in a format more suitable for analysis. In the code below I will first select columns needed for the analyses and the filter to retain one participant and one time-point. These data are then converted from a wide to long (tidy) format using the pivot_longer function. Notice that each of the lactate columns starts with lac., this information can be used in pivot_longer when rearranging the data. In pivot_longer we also convert the values to numeric values using as.numeric. Finally, we plot the data. The resulting plot (also shown below), shows a point for every lactate measurement. We have also connected the dots with geom_line which draws straight lines between each point. The straight line can be used to interpolate values between the observed lactate values. This is a common technique to calculate a lactate threshold, often defined as the intensity at 4 mmol L-1. library(tidyverse) library(exscidata) data(&quot;cyclingstudy&quot;) cyclingstudy %&gt;% # Select columns needed for analysis select(subject, group, timepoint, lac.225:lac.375) %&gt;% # Only one participant and time-point filter(timepoint == &quot;pre&quot;, subject == 10) %&gt;% # Pivot to long format data using the lactate columns pivot_longer(names_to = &quot;watt&quot;, values_to = &quot;lactate&quot;, names_prefix = &quot;lac.&quot;, names_transform = list(watt = as.numeric), cols = lac.225:lac.375) %&gt;% # Plot the data, group = subject needed to connect the points ggplot(aes(watt, lactate, group = subject)) + geom_line(lty = 2) + geom_point(shape = 21, fill = &quot;lightblue&quot;, size = 2.5) Figure 13.1: A workload and lactate relationship Next figure shows the value of x (watt, intensity) when y (lactate) is set to 4. The lines are added by eyeballing19 the expected value. This is all fine, we have an approximate lactate threshold. cyclingstudy %&gt;% # Select columns needed for analysis select(subject, group, timepoint, lac.225:lac.375) %&gt;% # Only one participant and time-point filter(timepoint == &quot;pre&quot;, subject == 10) %&gt;% # Pivot to long format data using the lactate columns pivot_longer(names_to = &quot;watt&quot;, values_to = &quot;lactate&quot;, names_prefix = &quot;lac.&quot;, names_transform = list(watt = as.numeric), cols = lac.225:lac.375) %&gt;% # Plot the data, group = subject needed to connect the points ggplot(aes(watt, lactate, group = subject)) + geom_line(lty = 2) + geom_point(shape = 21, fill = &quot;lightblue&quot;, size = 2.5) + # Adding straight lines at specific values geom_hline(yintercept = 4, color = &quot;red&quot;) + geom_vline(xintercept = 341.5, color = &quot;blue&quot;) Figure 13.2: Interpolation to estimate the lactate threshold, exercise intensity at 4 mmol L-1 To get a better approximation, we could make use of the curve-linear relationship between exercise intensity and lactate accumulation. The curvier the relationship, the more wrong the above approximation would be (as Yoda say, would)20. We can add a curve-linear model on top of our plot using the geom_smooth function in our ggplot call. In the code below, we will actually use several polynomial models together with a straight line to assess their fit. cyclingstudy %&gt;% # Select columns needed for analysis select(subject, group, timepoint, lac.225:lac.375) %&gt;% # Only one participant and time-point filter(timepoint == &quot;pre&quot;, subject == 10) %&gt;% # Pivot to long format data using the lactate columns pivot_longer(names_to = &quot;watt&quot;, values_to = &quot;lactate&quot;, names_prefix = &quot;lac.&quot;, names_transform = list(watt = as.numeric), cols = lac.225:lac.375) %&gt;% # Plot the data, group = subject needed to connect the points ggplot(aes(watt, lactate, group = subject)) + geom_line(lty = 2) + geom_point(shape = 21, fill = &quot;lightblue&quot;, size = 2.5) + geom_hline(yintercept = 4, color = &quot;red&quot;) + geom_vline(xintercept = 341.5, color = &quot;blue&quot;) + # Adding a straight line from a linear model geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x, color = &quot;#e41a1c&quot;) + # Adding a polynomial linear model to the plot # poly(x, 2) add a second degree polynomial model. geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ poly(x, 2), color = &quot;#377eb8&quot;) + # poly(x, 3) add a third degree polynomial model. geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ poly(x, 3), color = &quot;#4daf4a&quot;) + # poly(x, 4) add a forth degree polynomial model. geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ poly(x, 4), color = &quot;#ff7f00&quot;) # #fee090 represent color codes in th HEX format, palettes for different color can be found # here: https://colorbrewer2.org/ As we can see in the resulting plot, the different models are not that different around the 4 mmol L-1 mark. However, the linear model is just wrong at around 300 watts, the second degree polynomial model is wrong at 275 watts. The third and forth degree polynomial model does not differ much from observed values or each other. We may fit these models formally using lm and check their residuals. First we will store the data set in an object called lactate and the use this data set in several lm calls. The same formula can be used as in geom_smooth, but we must use the actual variable names. lactate &lt;- cyclingstudy %&gt;% # Select columns needed for analysis select(subject, group, timepoint, lac.225:lac.375) %&gt;% # Only one participant and time-point filter(timepoint == &quot;pre&quot;, subject == 10) %&gt;% # Pivot to long format data using the lactate columns pivot_longer(names_to = &quot;watt&quot;, values_to = &quot;lactate&quot;, names_prefix = &quot;lac.&quot;, names_transform = list(watt = as.numeric), cols = lac.225:lac.375) %&gt;% # Remove NA (missing) values to avoid warning/error messages. filter(!is.na(lactate)) # fit &quot;straight line&quot; model m1 &lt;- lm(lactate ~ watt, data = lactate) # fit second degree polynomial m2 &lt;- lm(lactate ~ poly(watt, 2, raw = TRUE), data = lactate) # fit third degree polynomial m3 &lt;- lm(lactate ~ poly(watt, 3, raw = TRUE), data = lactate) # fit forth degree polynomial m4 &lt;- lm(lactate ~ poly(watt, 4, raw = TRUE), data = lactate) # Store all residuals as new variables lactate$resid.m1 &lt;- resid(m1) lactate$resid.m2 &lt;- resid(m2) lactate$resid.m3 &lt;- resid(m3) lactate$resid.m4 &lt;- resid(m4) lactate %&gt;% # gather all the data from the models pivot_longer(names_to = &quot;model&quot;, values_to = &quot;residual&quot;, names_prefix = &quot;resid.&quot;, names_transform = list(residual = as.numeric), cols = resid.m1:resid.m4) %&gt;% # Plot values with the observed watt on x axis and residual values at the y ggplot(aes(watt, residual, fill = model)) + geom_point(shape = 21, size = 3) + # To set the same colors/fills as above we use scale fill manual scale_fill_manual(values = c(&quot;#e41a1c&quot;, &quot;#377eb8&quot;, &quot;#4daf4a&quot;, &quot;#ff7f00&quot;)) Figure 13.3: Assessing the fit f different linear models on a exercise intensity to lactate accumulation relationship. The forth degree polynomial model finds the observed values best, followed by the third degree. This is not strange as the polynomial model with increased degrees has more flexibility to fit to the data. The problem with polynomial models is that you cannot fit a fort degree polynomial model with only four data points. You may also encount some over-fitting to, for example a bad measurement. Lets settle for the third degree model. The next step will be to predict x from y. Remember that we have modeled the effct of x on y, i.e. the effect of exercise intensity on lactate. Using predict we may easily predict a lactate value for a specific value of watt. Since we want the inverse prediction we have to use some tricks in our prediction. The code below creates a data set of intensity values watt using the seq function which basically creates a vector of number with a specific distance between them. We can tehn use this vector of numbers to predict lactate values and find the value closest to 4 mmol L-1. # new data data frame ndf &lt;- data.frame(watt = seq(from = 225, to = 350, by = 0.1)) # high resolution, we can find the nearest10:th a watt ndf$predictions &lt;- predict(m3, newdata = ndf) # Which value of the predictions comes closest to our value of 4 mmol L-1? # abs finds the absolute value, makes all values positive, # predictions - 4 givs an exact prediction of 4 mmol the value zero # filter the row which has the prediction - 4 equal to the minimal absolut difference between prediction and 4 mmol lactate_threshold &lt;- ndf %&gt;% filter(abs(predictions - 4) == min(abs(predictions - 4))) Our best estimate of the lacatate threshold is 343. We have approximated the exercise intensity at a specific value of lactate. There a several ways of doing such calculations, and many other concepts of lactate thresholds exists. Newell (Newell et al. 2007) has developed R code for calculating several of these concepts. The code is a bit old but can be found here. Other have implemented R code in applications to calculate lactate thresholds, for example lactate dashboard. Most techniques and concepts rely on an underlying regression model. the act of eyeballing something is to measure or weigh something without any tools If you follow this link, take a cup of coffe before continuing with the chapter. "],["categorical-predictors-and-multiple-regression.html", "Chapter 14 Categorical predictors and multiple regression 14.1 Linear models can be used instead of t-tests 14.2 Dummy variables 14.3 Multiple regression", " Chapter 14 Categorical predictors and multiple regression We have up to now used a single continuous predictor to predict a dependent variables. We will now show that the ordinary regression models can be the same as other statistical tests, they can be extended and modified. This will show that the ordinary regression model is very flexible. 14.1 Linear models can be used instead of t-tests t-test are designed to compare means. A question you might want to answer using a t-test is how unlikely the results from your test is if there were no true differences between values from two groups (independent t-test), two samples from the same individuals (paired sample t-test) or to a set value (one sample t-test). Another way of describing these tests are; tests of differences from zero in a one-sample case or differences between groups with paired or unpaired observations. Using the cyclingstudy data set we can perform t-tests. In the code below we will select the variable squat jump and filter it from two time-points. pivot_wider is used to put squat jump performance from the two time-points in separate columns. A change score is then calculated. library(tidyverse) library(exscidata) data(&quot;cyclingstudy&quot;) cyc_select &lt;- cyclingstudy %&gt;% # select a subset of variables, squat jump max is the outcome of interest. select(subject, timepoint, sj.max) %&gt;% # time-points of interest filter(timepoint %in% c(&quot;pre&quot;, &quot;meso3&quot;)) %&gt;% # spread the data based on time-points pivot_wider(names_from = timepoint, values_from = sj.max) %&gt;% # create a change score mutate(change = meso3 - pre) The data above may be used to perform the paired sample t-test and a one sample t-test. These are basically equivalent. In the first case we use both vectors of numbers and test if the difference between them are different from zero. In the other case we calculate the differences first and then test if this change-score is different from zero. In the paired sample t-test, the argument paired = TRUE must be added to the t.testcall to make sure you do a paired comparison. In the one-sample case we have to set the mean we want to campare to, in this case zero (mu = 0). paired &lt;- t.test(cyc_select$meso3, cyc_select$pre, paired = TRUE) one_sample &lt;- t.test(cyc_select$change, mu = 0) These tests are equal as we can see from the comparison below. Next code chunk produces the table comparing the results. If we use a linear model we can perform the same test. When fitting the change variable to test against the same hypothesis (0), the same results will appear. We can add this information in the same table. linear_model &lt;- lm(change ~ 1, data = cyc_select) Table 14.1: Comparison of paired sample t-test, one-sample t-test of change scores and a linear model Test t-value p-value Estimate Lower CI Upper CI Standard error Paired sample t-test -1.788 0.091 -0.891 -1.938 0.156 0.498 One sample t-test -1.788 0.091 -0.891 -1.938 0.156 0.498 Linear model -1.788 0.091 -0.891 -1.938 0.156 0.498 By using the syntax change ~ 1 in the formula we specify that we want to estimate the intercept of the model. The intercept is tested against the null-hypothesis that it is 0. We can also test if there is a true difference in VO2.max percentage change between group INCRand DECR using a t-test. Run the code in your own session to explore the results. cyc_select &lt;- cyclingstudy %&gt;% # Select appropriate variables and filter time-points select(subject,group, timepoint, VO2.max) %&gt;% filter(timepoint %in% c(&quot;pre&quot;, &quot;meso3&quot;), group != &quot;MIX&quot;) %&gt;% # make the data set wider and calculate a change score (%-change). pivot_wider(names_from = timepoint, values_from = VO2.max) %&gt;% mutate(change = 100 * (meso3-pre)/pre) %&gt;% print() # Perform an unpaired sample t-test (or two-sample t-test) unpaired &lt;- t.test(change ~ group, data = cyc_select, var.equal = TRUE) Above we use the formula method to specify the t-test (see ?t.test). var.equal = TRUE tells the t.test function to assume equal variances between groups. The same result will appear when we instead fit the data to a linear model. The summary function is a generic function, meaning that many type of R objects has summary methods. From summary we get a regression table of estimates. The first row is the intercept, we can interpret this as the mean change in one of the groups (DECR). This rows has all the statistics associated with this estimate including the average (Estimate), standard error, t-value and a p-value. The second row is the difference between groups. The INCR group has a change score that is 3.016 units larger than the DECR group. The associated statistics can be used to assess if this difference is large enough to declare surprisingly large if the null hypothesis is actually true. lin_mod &lt;- lm(change ~ group, data = cyc_select) summary(lin_mod) If you compare the two tests, do they tell you the same? Using var.equal = TRUE in the unpaired t-test we assumed that the variation was similar in both groups. This might not be the case, and R uses the Welch two-sample t-test by default which does not assum equal variance between groups. Even the Welch two sample t-test can be replicated using a linear model. However, we have to specify it in a slightly different framework using the gls() function from the nlme package. library(nlme) welch_twosample &lt;- t.test(change ~ group, data = cyc_select, var.equal = FALSE) lin_mod_var &lt;- gls(change ~ group, data = cyc_select, weights = varIdent(form = ~1|group), na.action = na.exclude, method = &quot;ML&quot;) welch_twosample summary(lin_mod_var) You are not required to master gls at this time. It however shows that the linear model frame work is very flexible as it in this case also can be adjusted to take care of heteroscedasticity. 14.2 Dummy variables The group variable that we used in the code above introduces something new to the linear model, namely dummy variables. When we put a categorical variable in the lm command, R will code it as a dummy variable. This variable will be zero if the group corresponds to the first level of the categorical variable (coded as a factor variable) and it will be 1 if it is the second level. In the simplest case (as above) we will get a linear model looking like this: \\[Y = \\beta_0 + \\beta_1X\\] Where the \\(X\\) is the grouping variable, remember, 0 if first (reference) group and 1 if the second level group. The coefficient \\(\\beta_1\\) only kicks in if the group is 1. Meaning that when group = 0 we have only the intercept. If group = 1 we have the intercept + the slope. The slope represents the difference between the intercept (group = 0) and group = 1. If the grouping variable would have more groups more dummy-variables would have been added. Using all groups in the data set, fit a model and interpret the results. Here is a possible solution cyc_subset &lt;- cyclingstudy %&gt;% select(subject,group, timepoint, VO2.max) %&gt;% filter(timepoint %in% c(&quot;pre&quot;, &quot;meso3&quot;)) %&gt;% pivot_wider(names_from = timepoint, values_from = VO2.max) %&gt;% mutate(change = 100 * (meso3-pre)/pre) %&gt;% print() mod &lt;- lm(change ~ group, data = cyc_subset) summary(mod) The DECR group is the reference group, the intercept shows the mean of this group. Each parameter shows the difference from the reference. The same assumptions are made with these kinds of models and they can be checked with the same methods as described above. 14.3 Multiple regression Contrary to the t-tests used above, the linear model can be extended by adding predicting variables (independent variables). In a situation where multiple independent variables are included in the model, we control for their relationship to the dependent variable when we evaluate the other variables. Similarly with univariate regression we can examine each individual parameter from the summary. In a previous example we used height.T1 to predict VO2.max. We might want to add information to the model. We might wonder if the age (age) of participants have a relationship with VO2max. To fit this model, use the code below. cycling_data &lt;- cyclingstudy %&gt;% # select(subject, timepoint, VO2.max, weight.T1, height.T1) %&gt;% filter(timepoint == &quot;pre&quot;) %&gt;% print() mod1 &lt;- lm(VO2.max ~ height.T1 + age, data = cycling_data) summary(mod1) From the output we can see that there is a negative relationship, when age increases VO2max decrease. We can compare this model to the simpler model by looking at the \\(R^2\\) value. We fit the simpler model. mod0 &lt;- lm(VO2.max ~ height.T1, data = cycling_data) summary(mod0) We can interpret \\(R^2\\) as the percentage of the variation explained by the model. When we added more variables to the model we add information that collectively explain a larger portion of the observed data. When adding variables we face the risk of over-fitting our model. With enough variables the model will explain the observed data with less and less uncertainty, however, new data will probably not validate the model. The same assumptions apply to the multiple regression model as with the univariate regression model. We have to take care that we have homoscedasticity, independent observations and normally distributed errors. "],["correlations.html", "Chapter 15 Correlations 15.1 Always plot the data! 15.2 Extending the correlation to the regression 15.3 Correlation comes in many forms 15.4 Statistical and subject-matter interpretations 15.5 Summary", " Chapter 15 Correlations We are already familiar with the regression model. We will now take a step back, to the correlation. A correlation is a unit-less measure of the relationship between two variables. The strength of the relationship is expressed between -1 and 1 where values closer to 0 means weaker relationship. Using the data set provided by Haun et al. 2019 we will see how the correlation works. First we will load the data and select the variables SQUAT_VOLUME and DXA_LBM_T1. A reasonable interpretation of these variables are that SQUAT_VOLUME is the pre-intervention training volume and DXA_LBM_T1 is the percentage lean body mass before the intervention. To make the data more readable we will scale the squat volume from kg to tons library(tidyverse) library(exscidata) data(&quot;hypertrophy&quot;) dat &lt;- hypertrophy %&gt;% select(PARTICIPANT, SQUAT_VOLUME, BODYMASS_T1, DXA_LBM_T1) %&gt;% mutate(SQUAT_VOLUME = (SQUAT_VOLUME/1000)) %&gt;% print() A basic question given these kind of data is, do participants with more previous training volume have more muscle mass (lean mass)? We can test this by the doing a correlation analysis. The cor function gives the correlation coefficient from two variables: cor(dat$SQUAT_VOLUME, dat$DXA_LBM_T1) The value is quite high, around 0.5. Remember that a perfect correlation is either 1 or -1 and 0 indicates no correlation between variables. The correlation coefficient is not sensitive to the order of variables: cor(dat$DXA_LBM_T1, dat$SQUAT_VOLUME) We can use the correlation coefficients to draw inference. A test against the null hypothesis of no correlation, \\(H_0: r=0\\), can be done in R using the cor.test function. cor.test(dat$DXA_LBM_T1, dat$SQUAT_VOLUME) From this function we get a confidence interval, does the confidence interval contain the \\(H_0\\)? 15.1 Always plot the data! When doing a correlation analysis you are at risk of drawing conclusions based on wonky data. A single data point can for example create a correlation in a small data set. Lets look at the data we are using now. dat %&gt;% ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal() The plot displays no apparent relationship, there are no obvious outliers, both variables are evenly distributed (normally distributed). These are assumptions regarding the correlation analysis. Other assumptions are that the relationship is linear (a straight line). 15.2 Extending the correlation to the regression All good! We have a test that tells us about the relationship between two variables. However, the test does not tell us more about the correlation. Moving to a regression analysis gives us more information. First some similarities. Notice that the p-value for the regression coefficient for squat volume is (almost) precisely the same as the p-value for the correlation analysis! # Store the correlation analysis in an object c &lt;- cor.test(dat$DXA_LBM_T1, dat$SQUAT_VOLUME) # store the regression model m &lt;- lm(DXA_LBM_T1 ~ SQUAT_VOLUME, data = dat) # Display the p-value for the regression coefficient coef(summary(rm))[2, 4] # Display the p-value for the correlation coefficient c$p.value Also notice that the \\(R^2\\) value in the regression model is the same as the squared correlation coefficient. Remember that the \\(R^2\\) in the regression model is the degree to which the model account for the data (Navarro 2020), also see here. summary(m)$r.squared c$estimate^2 These similarities comes from the fact that they are the same analysis. The degree to which the two variables co-varies. The additional benefit of using a regression analysis comes from the interpretation of the regression coefficient estimates. In our example we can see that the increasing the weekly volume with one ton increases percentage lean mass by 0.283%-points. The confidence interval is given on the same scale and can be retrieved by using the code below: confint(m) This shows that the true value could be as low as 0.104 and as high as 0.463. Something that again indicates that the two variables do not vary together. 15.3 Correlation comes in many forms If you look at the help pages for cor (?cor) you will see that you may specify the type of correlation used for analysis. Commonly used are Pearsons (default) and Spearmans correlation coefficient. The difference between these two is that the Spearmans correlation coefficient does not assume normally distributed data. This is basically a correlation of ranks. The highest number in a series of numbers will have the highest rank and the smallest will be given the lowest ( = 1). We can prove this! The rank function gives a ranking to each number. We first plot the data as continuous values and then as ranks: dat %&gt;% ggplot(aes(SQUAT_VOLUME, DXA_LBM_T1)) + geom_point() + theme_minimal() dat %&gt;% ggplot(aes(rank(SQUAT_VOLUME), rank(DXA_LBM_T1))) + geom_point() + theme_minimal() We can see in the plot that the relationship persist after rank transformation. To use the Spearmans correlation coefficient we specify \"spearman\" in the cor.test function. cor.test(dat$SQUAT_VOLUME, dat$DXA_LBM_T1, method = &quot;spearman&quot;) To see that this is similar to using Pearsons correlation coefficient with ranked data, we do just that! cor.test(rank(dat$SQUAT_VOLUME), rank(dat$DXA_LBM_T1), method = &quot;pearson&quot;) Success! Another statistical mystery unlocked! In this case the interpretation of tests using ranked data and un-transformed data are very similar. When do we use the rank based correlation? In cases when assumptions are not met, a rank based correlation will protect us from making bad decisions when for example a single data point drives a correlation. The the rank-based correlation (Spearmans) will be more conservative. 15.4 Statistical and subject-matter interpretations It is now very important to stop and think about the estimates that we arrived to above. We have concluded that lean body mass correlate quite well with squat volume calculated as the amount of weight lifted per week. Does this mean that individuals that exercise with higher volumes have more muscle? Perhaps, but could it also mean that individuals that are taller and heavier work out with greater resistance? Perhaps. In any case, the correlation (and regression) analysis of snap-shot observational data may always be have underlying patterns that better exoplain mathematical relationships other that the causation that first comes to mind. We have to thread carefully when interpreting association (Spiegelhalter 2019, Chapter 2, 4 and 5), this is were you subject-matter knowledge is handy. 15.5 Summary The correlation coefficient has many similarities with a univariate regression model. Correlations measures strength of association, but the regression model comes with benefits in terms of interpretation. The correlation only takes two variables but we can extend the regression model. When we think that data do not match our assumptions we can do correlation analysis using Spearmans rank correlation to avoid biased estimates of estimation. "],["assignment2.html", "Chapter 16 Assignment 2: Regression models, predicting from data 16.1 Part 1: Lactate thresholds 16.2 Part 2: Predicting sizes of DNA fragments 16.3 Part 3: Intepreting a regression table 16.4 How to hand in the report", " Chapter 16 Assignment 2: Regression models, predicting from data 16.1 Part 1: Lactate thresholds There are several suggestions on how to best capture the physiological essense of the lactate threshold test (See Tanner and Gore 2012, Chapter 6). A simple, and very common way to analyze the relationship between exercise intensity and blood lactate is to determine exercise intensity at fixed blood lactate values. This can be done by fitting a regression model that captures the relationship and then inverse predict the exercise intensity value. An example of such inverse prediction can be found in Chapter 13. Your report should use data from the reliability project in the lab. Calculate at least two lactate thresholds (e.g. exercise intensity at 2 and 4 mmol L-1) and compare the reliability (typical error as a percentage of the mean) between the two thresholds. If you want to complicate things further you may want to implement other lactate threshold concepts (described in Tanner and Gore 2012; Newell et al. 2007). 16.2 Part 2: Predicting sizes of DNA fragments In the molecular laboratory you have been tasked to extract and analyze DNA. In this process we have to determine the size of resulting PCR (polymerase chain reaction) amplified DNA fragments. A tutorial using Image J and R can be found here. In your report you should show how you arrived to your predicted sizes by including the code chunk in your report. 16.3 Part 3: Intepreting a regression table Using the hypertrophy data set, state a question that concerns a linear relationship between two variables in the data set. These variables might be related to muscle size and strength, or two molecular markers or any other variables you are interested in. Include a regression table from your analysis in the report and interpret its components in plain language (e.g. for every unit increase in the independent variable the dependent variable decreseas by x units). The interpretation should also include a description and explanation of the standard error, the t-value and the p-value. Valuable guidance on how to interpret the table may be found in for example (Navarro 2020) and in (Spiegelhalter 2019, Chapter 5). Special attention should be made concerning the p-value. How do you define and interpret the p-value in your regression table. What does it mean?. The p-value debate on the 22:nd of September might give you an idea about that! 16.4 How to hand in the report The report is a group assignment, it is not to be included in the portfolio (mappeeksamen). However, it is required in order to pass the course (arbeidskrav). Create a new project on github and collaborate with your group there. The repository with all data and coded needed to create the report, and the report itselfe (in html, word or pdf format) should be reported on canvas as a link to the repository, no later than the first of October. Each member of the group hand in the link in canvas. The repository should be the same for all group memebers. "],["study-questions.html", "Chapter 17 Study questions 17.1 Chapter 2: Introduction to data science (with relevant literature) 17.2 Chapter 3: Storing data in spreadsheets (with relevant literature) 17.3 Chapter 4: Installing and starting up R (and R Studio) 17.4 Chapter 5: Creating your first graph 17.5 Chapter 6: Wrangling data to create your first table 17.6 Chapter 11: Introduction to the molecular exercise physiology lab", " Chapter 17 Study questions Updates: Chapters 2-6 updated on 2021-08-31 17.1 Chapter 2: Introduction to data science (with relevant literature) Define the PPDAC cycle. What are the components of this cycle? What is the difference between a reprodicible study and a replicated study? What defines reproducible science? What do we mean by scripted code vs. point-and-click. 17.2 Chapter 3: Storing data in spreadsheets (with relevant literature) Make a short list of suggestions on how to use spreadsheet software based on the reccomendations from Broamn and Woo. Define the following concepts in a spreadsheet: Cell, formula, function, attributes. Define the Anna Kerenina principle. How does it apply to data science and data sets? Define tidy data. How would you structure a spreadsheet in order to adhere to tidy data principles? What does it mean to be lazy when recording data? What is data validation, and how do you turn it on in your spreadsheet software? What are the differences between .csv and .xls(x) files? 17.3 Chapter 4: Installing and starting up R (and R Studio) Make sure you have working installations of R and RStudio. How would you define, the source, the environment, the console. When typing the code shown below in your console, what happends to the environment, what does this mean? obj &lt;- c(1, 2, 3, 56) How would you define obj in the code above? What is the c(1, 2, 3, 56) part doing in the code? What is &lt;-? What is a R package? What is the difference between reproducible and non-reproducible computing? What is a script? How does a simple computer program work? Why should you never save the workspace? 17.4 Chapter 5: Creating your first graph Define the following: aesthetics, geoms, mapping, theme. Think about situations when it would be appropriate to use the following: geom_point(), geom_boxplot(), geom_line(), geom_jitter(). 17.5 Chapter 6: Wrangling data to create your first table What do we mean with data wrangling? What is a pipe? Define (in plain language) the %&gt;% operator. When using %&gt;%, what makes a function pipable? Why do you put print() in the end of every pipe (when developing it)? Functions in the package dplyr can be useful when wrangling data, define the folowing functions, when do you use them? filter() select() mutate() Define grouped operations using group_by(). What will be the difference of using group_by() followed by summarise() as opposed to mutate(). Define the functions pivot_wider() and pivot_longer(). What is needed to make a table using kable() try to define (with plain language) how you need to set up your software. 17.6 Chapter 11: Introduction to the molecular exercise physiology lab Discuss: What are the most important points concerning health and safety in the laboratory? Your answer could involve before, during and after the experiment. Why do we use a laboratory notebook? Convert 1 gram to milligrams Convert 1 milligram to micrograms Convert 1 litre to millilitres Convert 500 millilitres to litres Convert 10 microlitres to millilitres You are to make a 1 Molar Tris solution (Tris MW = 121.14 g/mol). How much Tris should you add to your solution? In the same solution, we want to add 0.5 M of NaCl (NaCl MW = 58 g/mol), how much NaCl should go int your solution? To create a reagent (Reagent B), we will add EDTA from a Stock solution with a concentration of 0.5 M. The final concentration should be 0.06 M in 1 litre. What volume stock solution should be added to your reagent? Use the formula below to answer the question: \\[C_1 \\times V_1 = C_2 \\times V_2\\] One unknown can be calculated from the formula by rearranging: \\[V_2 = \\frac{C_1 \\times V_1}{C_2}\\] \\(V\\) for volume and \\(C\\) for concentration. "],["references.html", "Chapter 18 References", " Chapter 18 References Broman, Karl W., and Kara H. Woo. 2018. Data Organization in Spreadsheets. Journal Article. The American Statistician 72 (1): 210. https://doi.org/10.1080/00031305.2017.1375989. Ellefsen, S., D. Hammarstrom, T. A. Strand, E. Zacharoff, J. E. Whist, I. Rauk, H. Nygaard, et al. 2015. Blood flow-restricted strength training displays high functional and biological efficacy in women: a within-subject comparison with high-load strength training. Am. J. Physiol. Regul. Integr. Comp. Physiol. 309 (7): R767779. Halperin, I., D. B. Pyne, and D. T. Martin. 2015. Threats to Internal Validity in Exercise Science: A Review of Overlooked Confounding Variables. Journal Article. Int J Sports Physiol Perform 10 (7): 82329. https://doi.org/10.1123/ijspp.2014-0566. Hammarström, Daniel, Sjur Øfsteng, Lise Koll, Marita Hanestadhaugen, Ivana Hollan, William Apró, Jon Elling Whist, Eva Blomstrand, Bent R. Rønnestad, and Stian Ellefsen. 2020. Benefits of Higher Resistance-Training Volume Are Related to Ribosome Biogenesis. Journal Article. The Journal of Physiology 598 (3): 54365. https://doi.org/10.1113/JP278455. Haun, C. T., C. G. Vann, C. B. Mobley, P. A. Roberson, S. C. Osburn, H. M. Holmes, P. M. Mumford, et al. 2018. Effects of Graded Whey Supplementation During Extreme-Volume Resistance Training. Journal Article. Front Nutr 5: 84. https://doi.org/10.3389/fnut.2018.00084. Haun, C. T., C G. Vann, C. Brooks Mobley, Shelby C. Osburn, Petey W. Mumford, Paul A. Roberson, Matthew A. Romero, et al. 2019. Pre-Training Skeletal Muscle Fiber Size and Predominant Fiber Type Best Predict Hypertrophic Responses to 6 Weeks of Resistance Training in Previously Trained Young Men. Journal Article. Frontiers in Physiology 10 (297). https://doi.org/10.3389/fphys.2019.00297. Hopkins, W. G. 2000. Measures of Reliability in Sports Medicine and Science. Journal Article. Sports Med 30 (1): 115. http://www.ncbi.nlm.nih.gov/pubmed/10907753. Ioannidis, John P. A. 2005. Why Most Published Research Findings Are False. Journal Article. PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124. Leek, J. T., and R. D. Peng. 2015. Statistics: P Values Are Just the Tip of the Iceberg. Journal Article. Nature 520 (7549): 612. https://doi.org/10.1038/520612a. Navarro, D. 2020. Learning Statistics with r. Newell, J., D. Higgins, N. Madden, J. Cruickshank, J. Einbeck, K. McMillan, and R. McDonald. 2007. Software for Calculating Blood Lactate Endurance Markers. Journal Article. Journal of Sports Sciences 25 (12): 14039. https://doi.org/10.1080/02640410601128922. Peng, R. D., F. Dominici, and S. L. Zeger. 2006. Reproducible Epidemiologic Research. Journal Article. Am J Epidemiol 163 (9): 78389. https://doi.org/10.1093/aje/kwj093. Spiegelhalter, D. J. 2019. The Art of Statistics : How to Learn from Data. Book. First US edition. New York: Basic Books. Stephen, G. Powell, R. Baker Kenneth, and Lawson Barry. 2009. Errors in Operational Spreadsheets. Journal Article. Journal of Organizational and End User Computing (JOEUC) 21 (3): 2436. https://doi.org/10.4018/joeuc.2009070102. Tanner, R. K., and C. J. Gore. 2012. Physiological Tests for Elite Athletes 2nd Edition. Book. Human Kinetics. https://books.google.no/books?id=0OPIiMks58MC. Wickham, Hadley. 2014. Tidy Data. Journal Article. Journal of Statistical Software; Vol 1, Issue 10 (2014). https://www.jstatsoft.org/v059/i10. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; OReilly Media. http://r4ds.had.co.nz/. Ziemann, Mark, Yotam Eren, and Assam El-Osta. 2016. Gene Name Errors Are Widespread in the Scientific Literature. Journal Article. Genome Biology 17 (1): 177. https://doi.org/10.1186/s13059-016-1044-7. "]]
